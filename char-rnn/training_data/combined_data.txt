The Autophage Protocol: Metabolic Economics for Decentralized Health  Austin Harshberger  1@0x42r.io  July 14, 2025  Abstract  Traditional economics assumes value persists indefinitely, enabling unlimited wealth accumu- lation. Living systems operate differently because value requires continuous renewal or it ceases to exist. This paper introduces the Autophage Protocol, a system that combines formal incen- tive design with cryptographic privacy guarantees, linking tokenized health rewards to verifiable activity while maintaining a strict separation of identity and data. Four token species decay at rates calibrated to biological persistence. Rhythm (5% daily) for exercise, Healing (0.75%) for therapy, Foundation (0.1%) for preventive care, and Catalyst (2–10% dynamic) for marketplace balance. Decayed tokens flow to The Reservoir, funding community healthcare while preserving privacy through zero-knowledge proofs.   Mathematical modeling suggests wealth distribution converges to a Gini coefficient of 0.08–0.11 under baseline conditions, and remains below 0.55 even when 23% of the population acts adversarially 1 , far more equitable than traditional sys- tems at 0.82 or higher [6, 22]. The protocol represents a new economic primitive where money must move to exist, creating a digital economy with metabolism-like properties.  Reader’s Guide  Audience   Recommended Sections  Non-technical readers   Abstract, Related Work, Introduction, Appendix B-E Technical readers   All sections, focus on 3-7 and 9-11 Developers   Sections 5-7 and 10, Appendix C and H Stewards   Abstract, Related Work, Sections 1-4, Appendix D and E  1 See full simulation results and adversarial methodology in Appendix H. All code and raw data available at  https://autophage.xyz/simulations .  1Contents  Reader’s Guide   1 1   Introduction   4 2   Related Work   4 3   Metabolic Token Dynamics   5  3.1   Mathematical Foundation   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   5 3.2   Token Generation Mechanics   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   6 3.3   Advanced Decay Mechanics   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   7 3.4   Catalyst Token Dynamics   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   8  4   The Reservoir: Dual-Chamber Architecture   8  4.1   System Architecture   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   8 4.2   Healthcare Settlement Mechanics . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   9 4.3   Wellness Vault Mechanics   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   10  5   Metabolic Price Discovery   10  5.1   Endogenous Pricing Model   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   10 5.2   Energy Cost Calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   11  6   Privacy Architecture   11  6.1   Zero-Knowledge Identity Separation   . . . . . . . . . . . . . . . . . . . . . . . . . . .   11 6.2   Privacy-Preserving Marketplace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   12 6.3   Zero-Knowledge Proof Generation   . . . . . . . . . . . . . . . . . . . . . . . . . . . .   12  7   Biological Scaling Laws   13  7.1   Kleiber’s Law Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   13 7.2   Liebig’s Law of the Minimum   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   13 7.3   Additional Biological Laws   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   14 7.4   Genetic Adaptation System   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   15  8   Economic Analysis   15  8.1   Wealth Distribution Dynamics   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   15 8.2   Revenue Model and Unit Economics   . . . . . . . . . . . . . . . . . . . . . . . . . . .   16  9   Empirical Governance   17  9.1   Contribution-Based Voting   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   17 9.2   On-Chain Experimentation   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   18 9.3   Gas Optimization Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   18  10 Implementation Architecture   19  10.1 Core Smart Contracts   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   19  11 Limitations and Future Work   20  11.1 Technical Limitations   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   20  12 Conclusion   20  213 Works Consulted   21 14 Appendix   23 A   Complete Feature Set   23 B   Glossary of Terms   25 C   Selected Use Case Examples   27  C.1   Marathon Training: Sarah’s Journey   . . . . . . . . . . . . . . . . . . . . . . . . . . .   27 C.2   Recovery Economics: Marcus’s Story . . . . . . . . . . . . . . . . . . . . . . . . . . .   27 C.3   Creator Economy: Luna’s Empire . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   28  D   Plain Language Summary   29 E   Understanding the Math for Everyone   30 F   Note from the Author   32 G Acknowledgments   32 H   Adversarial Stress Test   33 I   On the Irreversibility of Health Value Conversion   34 J   AI Attestation   34 K Version History   35  31   Introduction  Money does not die. This fundamental property of traditional economics creates systemic inequality and health in- centive failures. A dollar earned decades ago maintains its nominal value indefinitely. Bitcoin [24] stored in 2009 remains unchanged today.   This permanence rewards accumulation over activity, creating economies where wealth concentrates among those who already possess it while those who need resources most must continuously labor for finite rewards. The Autophage Protocol challenges this foundational assumption by creating tokens that decay exponentially without continuous health activity.   The name Autophage, meaning “self-eating,” mirrors the essential dynamic in that value must consume itself to persist, preventing indefinite accumulation while rewarding ongoing engagement. The protocol implements four interconnected novelties.   First, it creates multiple token species with decay rates precisely calibrated to match the temporal persistence of their underlying health activities.   Exercise tokens decay rapidly like cardiovascular fitness, while preventive care tokens persist longer like vaccine immunity.   Second, it achieves cryptographic privacy separation through zero-knowledge proofs, enabling health ver- ification without surveillance.   Third, it derives token value from measurable metabolic energy expenditure rather than speculative markets.   Fourth, it implements empirical governance where all protocol changes must demonstrate statistically significant health improvements through on- chain experiments. These mechanisms work together to create an economy that mimics biological systems. Analo- gous to how organisms require continuous energy input to maintain homeostasis, protocol partici- pants must engage in verified health activities to maintain token balances. The following sections present the complete mathematical framework, implementation archi- tecture, and empirical validation of this metabolic economic system.   Each component has been designed to align financial incentives with biological imperatives, creating a digital economy that enhances rather than exploits human health.  2   Related Work  In the early 20th century, Silvio Gesell proposed “stamped scrip” to force currency to circulate by making it expire, an approach he tested in W¨ orgl, Austria during the Great Depression [11]. These experiments gained traction under local government support, but collapsed as soon as legal enforcement lapsed. The main lesson being demurrage alone does not guarantee a stable or resilient economy, especially when external incentives favor stasis. Cryptocurrency inherited some of this thinking. Freicoin, launched in 2013, introduced decay by subtracting a fixed demurrage fee from every balance at regular intervals [10]. While technically functional, Freicoin’s approach was blunt, with no connection to user behavior or economic context. As a result, activity and participation dwindled, and the protocol failed to build lasting network effects or meaningfully incentivize healthy engagement. Other “health coins” have focused on rewards for activity, using move-to-earn or similar models. Most are marketplace tokens, structured around accumulation. STEP’N [17] pays users for walking, but value is driven by speculative tokenomics, not by any durable health metric. The project saw rapid growth but struggled to maintain sustainable rewards.   Sweatcoin uses a similar approach. Users earn points for steps and spend them in a marketplace, but the system lacks decay mechanics, biological calibration, and any link to real health persistence. 4The Autophage Protocol diverges on three counts.   First, it ties decay rates directly to the half-life of underlying health activities, making token persistence a function of biological process instead of arbitrary policy. Second, it separates identity from data using zero-knowledge proofs, so participation does not require surveillance. Third, it requires every protocol change to demonstrate measurable health improvement on-chain before implementation. Where earlier systems enforced circulation by decree, Autophage encodes it at the metabolic and behavioral level.  Navigating Sections 3–10  The following sections, from three to ten, are focused primarily on formalized mathematical rep- resentations of the protocol. For readers less interested in academic rigor, it is advisable to focus only on the various Examples in each section, as they are written more plainly. The core compo- nents covered in each section, but not in the Examples, are also summarized in Appendix A–D.For those interested in learning how to translate the mathematics from Sections 3–10, start by reading Appendix E.  3   Metabolic Token Dynamics  3.1   Mathematical Foundation  The protocol implements a four-species token ecosystem where each species exhibits distinct decay characteristics matching its health domain. Let   S   =   { Rhythm ,   Healing ,   Foundation ,   Catalyst }  denote the set of token species. For any user   u   at discrete time   t , the balance   V   ( u )  i   ( t ) of species   i   ∈   S   evolves according to:  V   ( u )  i   ( t   + 1) =   V   ( u )  i   ( t )(1   −   δ i ) +   G ( u )  i   ( t )   (1) where   δ i   ∈   (0 ,   1) represents the daily decay rate and   G ( u )  i   ( t ) represents newly generated tokens from verified health activities. The continuous-time differential equation underlying this discrete implementation is:  dV dt   =   − λV   +   g ( t )   (2) where   λ   =   −   ln(1   −   δ ) is the continuous decay constant.   This formulation ensures mathematical consistency while enabling computationally efficient blockchain implementation. Table 1: Token Species Parameters and Biological Justification  Species   Decay Rate   Half-Life   Health Domain   Biological Basis  Rhythm   5%   13.51 days   Exercise, medication   VO 2   max decline rate [23] Healing   0.75%   92.42 days   Therapy, recovery   Neuroplasticity timescales [16] Foundation   0.1%   693.15 days   Preventive care   Antibody persistence [3] Catalyst   2–10%   Variable   Marketplace   Self-regulating 5The half-life calculations follow directly from exponential decay:  t 1 / 2   =   ln(0 . 5) ln(1   −   δ i )  =  ln(2)  λ i  (3)  Example 3.1:   Consider Sarah who runs daily, earning 50 Rhythm tokens per run. After main- taining a balance of 1,000 tokens, she stops running for two weeks. Her balance evolution follows:  Day   Formula   Balance  0   Initial   1,000 tokens 7   1 ,   000   ×   0 . 95 7   696 tokens 14   1 ,   000   ×   0 . 95 14   488 tokens This 51.2% loss over two weeks mirrors the documented decline in cardiovascular fitness during training cessation [23], validating the biological calibration.  3.2   Token Generation Mechanics  Health activities generate tokens through a multi-factor reward system:  G ( u )  i   ( t ) =   X  j ∈A i  B i,j   ×   M   ( u,j )  total   ( t )   ×   1 [activity   j   verified]   (4) where   A i   represents activities mapped to species   i ,   B i,j   is the base reward for activity   j , and   M total  combines multiple multipliers:  M total   =   M streak   ×   M group   ×   M time   ×   M genetic   ×   M synergy   ×   M quality   (5) subject to the constraint 0 . 3   ≤   M total   ≤   20 to prevent exploitation while rewarding performance. The streak multiplier follows a logarithmic function:  M streak   = 1 + log 10 (consecutive days)   (6) The circadian multiplier uses a sinusoidal function:  M time   = 1 + 0 . 3 sin     2 π ( h   −   6) 24    (7) where   h   is the hour of day in 24-hour format.  Example 3.2:   Marcus completes his morning workout routine:  Component   Calculation   Value  Base reward   Standard rate   35 Rhythm tokens 30-day streak multiplier   1 + log 10 (30)   2.48 6 AM circadian bonus   1 + 0 . 3 sin(2 π (6   −   6) / 24)   1.3 Group workout   Standard group bonus   1.5  Total generation   35   ×   2 . 48   ×   1 . 3   ×   1 . 5   169 tokens  6− 10   0   10   20   30   40   50   60   70   80   90   100  0  20  40  60  80  100  t 1 / 2 =13.5d   t 1 / 2 =92.4d  Days  Remaining Value (%)  Rhythm (5% daily)  Healing (0.75% daily)  Foundation (0.1% daily)  Figure 1:   Comparative decay curves showing differential value persistence across token species. Rhythm tokens model rapid fitness loss, Healing tokens reflect gradual therapeutic progress decay, and Foundation tokens represent long-term health investments.  3.3   Advanced Decay Mechanics  The protocol implements progressive decay acceleration for large balances to prevent wealth con- centration:  δ ef f i   ( V   ) =   δ i   ×   1 +  n X  k =1  α k   ×   1 [ V > τ k ]  !  (8) where   τ k   represents tier thresholds and   α k   represents acceleration factors. For continuous acceleration above soft cap:  δ ef f i   ( V   ) =   δ i   ×    1 +   β   ×   max    0 ,  V   −   τ sof t  τ sof t   γ     (9) where   β   controls sensitivity (typically 0.5-2.0) and   γ   shapes the acceleration curve (typically 1.0- 1.5).  Example 3.3:   Consider whale protection for Rhythm tokens:  Balance Range   Decay Rate   Acceleration   Daily Loss (75K example)  ≤   10,000   5%   Standard   - 10,001–50,000   7.5%   50%   - 50,001–100,000   10%   100%   7,500 tokens  >   100,000   15%   200%   - A user holding 75,000 Rhythm tokens loses 7,500 daily instead of 3,750 under standard decay, creating natural redistribution pressure. 73.4   Catalyst Token Dynamics  Catalyst tokens function as the protocol’s liquidity and governance utility, enabling marketplace transactions, proposal staking, and price discovery through metabolic pricing.   They are earned through verified activity, subject to dynamic decay, and cannot be vaulted or used for long-term savings. Catalyst tokens maintain ecosystem balance through dynamic decay adjustment:  δ catalyst ( t ) =   δ base   ×    1 +   β   ×   V catalyst ( t )  P  i ∈ S   V i ( t )   −   ρ target  γ     (10) where   ρ target   = 0 . 25 represents the target 25% supply ratio,   β   = 2 controls sensitivity, and   γ   = 1 . 5 shapes the response curve.  Example 3.4:   When Catalyst tokens comprise 40% of total supply:  δ catalyst   = 0 . 02   ×    1 + 2   × | 0 . 40   −   0 . 25 | 1 . 5    (11) = 0 . 02   ×   (1 + 2   ×   0 . 058)   (12) = 0 . 0232 (2.32% daily decay)   (13) This 16% increase in decay rate gradually returns the system to equilibrium without shock.  4   The Reservoir: Dual-Chamber Architecture  4.1   System Architecture  The Reservoir serves as the protocol’s metabolic center, collecting value from natural decay and re- distributing it for community health. Its dual-chamber design separates token flows from stablecoin reserves:  R token ( t   + 1) =   R token ( t ) +   X  i,u  δ i V   ( u )  i   ( t )  |   {z   }  decay inflow  −   W health ( t )  |   {z   }  health claims  −   W reward ( t )  |   {z   }  rewards  (14)  R U SDC   ( t   + 1) =   R U SDC   ( t ) +   F market ( t ) +   F app ( t )  |   {z   }  fee inflows  −   H settle ( t )  |   {z   }  healthcare settlements  (15) The solvency constraint ensures healthcare access remains guaranteed:  R U SDC   ( t )   ≥   max   0 . 4   X  u  D u ( t ) ,   3   ×   ¯ H monthly ,   0 . 22   ×   Y annual  !  (16) This triple-coverage requirement protects against bank runs, seasonal variations, and catastrophic events. 8Example 4.1:   System snapshot at 100,000 active users:  Metric   Value  Daily token decay   500,000 tokens (5/user avg) Monthly healthcare settlements   $ 800,000 Annual revenue   $ 4,800,000 Required USDC reserve   $ 2,400,000 Reserve calculation   max( $ 1.6M,   $ 2.4M,   $ 1.056M) The 3-month coverage requirement dominates, ensuring the system can weather extended stress periods. User Pool P   V   ( u )  i   ( t )  The Reservoir  Token Chamber  R token ( t ) USDC Chamber  R U SDC   ( t ) Healthcare Settlements  Token Decay  Fees  Metabolic Price  USDC  Equilibrium Maintenance  Inflows:   P   δ i V   ( u )  i   +   F total   = Outflows:   W total   +   H settle  Token velocity: 10–30x monthly circulation Price stability:   ± 5% daily variation maximum Figure   2:   The   Reservoir’s   dual-chamber   architecture   maintains   separation   between   token metabolism and healthcare settlement liquidity, ensuring both community wealth circulation and guaranteed medical access.  4.2   Healthcare Settlement Mechanics  Healthcare settlements follow strict priority queuing:  P riority ( claim ) =   ω 1   ×   U rgency   +   ω 2   ×   Duration   +   ω 3   ×   V erif ication   (17) where weights   ω i   ensure medical needs supersede all other claims. 9Example 4.2:   Emergency prescription claim processing:  Component   Value   Weight  Urgency score   10 (maximum)   0.7 Wait duration   2 hours   0.2 Provider verification   8/10   0.1  Priority score   0 . 7   ×   10 + 0 . 2   ×   2 + 0 . 1   ×   8 = 8 . 2 This claim processes ahead of routine wellness rewards (typical score: 2-3) but after critical emer- gency claims (score: 9+).  4.3   Wellness Vault Mechanics  Wellness vaults reduce effective decay rates based on lock duration:  δ vault   =   δ base   ×   (1   −   r lock )   (18) where   r lock   is the reduction factor:  Lock Duration   r lock   Factor   Effective Decay   Decay Reduction  30 days   0.09   91% of normal   9% reduction 90 days   0.27   73% of normal   27% reduction 180 days   0.45   55% of normal   45% reduction 365 days   0.90   10% of normal   90% reduction Early withdrawal penalty:  P withdrawal   =   V locked   ×   t remaining  t total  ×   p f actor   (19) where   p f actor   = 0 . 5 creates a 50% penalty proportional to time remaining.  5   Metabolic Price Discovery  5.1   Endogenous Pricing Model  Unlike traditional cryptocurrencies that derive value from speculation, the Autophage Protocol calculates token prices from actual energy expenditure:  P   ( t ) =   E health ( t )   ×   (1 +   γ   ×   C ratio ( t )) +   V market ( t )   ×   (1   −   C ratio ( t ))  S active ( t )   ×   V   ( t )   ×   (1 +   M activity ( t ))   (20) where the parameters are defined as follows:   E health ( t ) represents the total metabolic energy from health activities measured in kcal-equivalent.   V market ( t ) captures the 30-day rolling marketplace volume in USDC.   S active ( t ) denotes the circulating supply excluding vaulted tokens.   V   ( t ) measures token velocity with a target range of 10-30 monthly transactions.   C ratio ( t ) indicates the proportion of Catalyst tokens in the system.   γ   serves as the energy gradient parameter, typically ranging from 1.2 to 3.0. Finally,   M activity ( t ) reflects the system-wide activity level. 10Example 5.1:   Price calculation during typical conditions:  Parameter   Value  Daily health energy   500,000 kcal Standardized energy value   $ 0.002 per kcal Monthly marketplace volume   $ 150,000 Active supply   10 million tokens Current velocity   15x monthly Catalyst ratio   0.23 Activity multiplier   1.4 Energy gradient   2.0  P   ( t ) =  (500 ,   000   ×   0 . 002)   ×   (1 + 2 . 0   ×   0 . 23) + (150 ,   000 / 30)   ×   (1   −   0 . 23) 10 ,   000 ,   000   ×   (15 / 30)   ×   (1 + 1 . 4)   (21) =  1 ,   000   ×   1 . 46 + 5 ,   000   ×   0 . 77 5 ,   000 ,   000   ×   2 . 4   (22) =  1 ,   460 + 3 ,   850 12 ,   000 ,   000   (23) = $0 . 000442 per token   (24)  5.2   Energy Cost Calibration  Each activity’s energy cost encompasses multiple factors:  E activity   =   T duration   ×   W wage   +   C metabolic   ×   F f ood   +   O opportunity   +   E equipment   (25)  Example 5.2:   45-minute therapy session energy calculation:  Cost Component   Calculation   Amount  Time cost   0.75 hours   × $ 30/hour   $ 22.50 Emotional energy   200 kcal-equivalent   × $ 0.002   $ 0.40 Transportation   Fixed cost   $ 5.00 Opportunity cost   Estimated value   $ 10.00  Total energy cost   Sum of components   $ 37.90  This establishes the baseline value for Healing token generation from therapy activities.  6   Privacy Architecture  6.1   Zero-Knowledge Identity Separation  The protocol achieves complete separation between identity and health data through nested cryp- tographic commitments [14, 13]:  P rof ileID   =   H ( H ( U serID || Salt 1 ) || Salt 2 || Secret )   (26) 11This double-hashing with independent salts provides 768 bits of entropy, making correlation attacks computationally infeasible even with quantum advances.  Example 6.1:   Privacy preservation in practice:  Component   Value  Alice’s UserID   alice@email.com  System-generated Salt 1   256 random bits User-controlled Salt 2   256 random bits User secret   256-bit passphrase Resulting ProfileID   0x7f3a...9b2e   (indistinguishable from random) Even if an attacker compromises the system database and obtains Salt 1 , they cannot correlate ProfileIDs without also obtaining user-controlled Salt 2   and Secret values.  6.2   Privacy-Preserving Marketplace  The marketplace implements granular privacy controls with explicit economic incentives: Table 2: Privacy Tiers and Economic Premiums  Tier   Information Revealed   Base Price   Typical Premium  Anonymous   Activity type, timestamp   $ 100   Baseline ProfileID   + Consistency metrics, streak data   $ 100   75% ( $ 175) Genetic   + Specialized traits, optimizations   $ 100   250% ( $ 350) Full   + Historical patterns, correlations   $ 100   500% ( $ 600)  Example 6.2:   Sarah’s marathon proof monetization:  Privacy Tier Information Revealed   Sale Price  Anonymous   “Sub-4:30 marathon, major city, female 25–34”   $ 100 ProfileID   Same   +   “180-day   running   streak,   3 marathons/year”  $ 175 Genetic   Same + “Endurance Elite trait, VO 2   optimiza- tion”  $ 350 Buyers value additional information for research, motivation, or verification purposes while Sarah maintains control over disclosure levels.  6.3   Zero-Knowledge Proof Generation  Health activities undergo privacy-preserving verification [13]:  π   = Prove ( C   = Commit( activity, r ) ,   Statement , witness )   (27) where the statement proves properties without revealing underlying data. 12Example 6.3:   Therapy session verification:  Data Type   Details  Private data   Therapist name, location, session notes, diagnosis codes Public statement   “Completed 50-minute therapy session on 2025-07-07” Proof   π   Cryptographic evidence statement is true Token generation   80 Healing tokens Revealed information   None beyond the public statement  7   Biological Scaling Laws  7.1   Kleiber’s Law Implementation  Metabolic efficiency scales with network size following quarter-power scaling [15]:  η ( N   ) = 1 +   α   ×     N N 0   0 . 25  (28) where   α   = 0 . 3 and   N 0   = 1 ,   000 (reference network size).  Example 7.1:   Network scaling benefits:  Network Size   Scaling Calculation   η   Value   Bonus  1,000   1 + 0 . 3   ×   1   1.30   30% 10,000   1 + 0 . 3   ×   1 . 78   1.53   53% 100,000   1 + 0 . 3   ×   3 . 16   1.95   95% 1,000,000   1 + 0 . 3   ×   5 . 62   2.69   169% Larger networks provide efficiency benefits to all participants, mimicking biological metabolic scal- ing.  7.2   Liebig’s Law of the Minimum  System health limited by weakest component triggers automatic rebalancing [19]:  H system   = min( H users , H reserves , H apps , H geographic , H velocity )   (29) When any component falls below threshold   θ   = 0 . 7:  M bottleneck   = 1 +   κ   ×   ( θ   −   H min )   (30) 13Example 7.2:   Geographic bottleneck response:  Metric   Urban   Rural  Participation rate   85%   45% Health threshold   70%   70% Status   Healthy   Below threshold Bottleneck calculation   -   M   = 1 + 2   ×   (0 . 7   −   0 . 45) = 1 . 5 Token bonus   Standard   50% bonus  7.3   Additional Biological Laws  The protocol implements five additional scaling laws: 1.   Allee Effect   [2]: Small population support  M Allee   =        2 . 0   N <   500 1 . 5   500   ≤   N <   1000 1 . 0   N   ≥   1000 (31) 2.   Circadian Rhythms : Time-optimized rewards  M circadian ( h ) = 1 +   A   ×   sin     2 π ( h   −   ϕ ) 24    (32)  where   A   = 0 . 3 and   ϕ   = 6 (6 AM peak)  3.   Bergmann’s Rule   [5]: Environmental adaptation  R target   =   R base   ×   (1 +   σ   ×   H environment )   (33) 4.   Optimal Foraging   [18]: Activity recommendation  ROI activity   =   T okens   ×   (1   −   δ ) t hold  Energy cost  (34) 5.   R/K Selection   [20]: Growth phase modulation  σ rewards   =  (  0 . 5   R-phase( N <   10 4 ) 0 . 2   K-phase( N   ≥   10 4 )   (35) 147.4   Genetic Adaptation System  Users can burn Foundation tokens to evolve genetic traits: Cost trait n   = 1000   ×   2 . 5 n − 1   Foundation tokens   (36) Combined trait bonus calculation:  M genetic   = 1 +  n X  i =1  b i   ×   (1   −   d i )   (37) where   b i   is the individual trait bonus (0.05–0.15) and   d i   is a diminishing returns factor:  d i   = 0 . 1   ×   ( i   −   1)   (38) Subject to the constraint:  M genetic   ≤   1 . 5   (39)  Example 7.4:   User evolves three genetic traits:  Trait   Bonus ( b i )   Diminishing ( d i )   Adj.   Bonus ( b i   ×   (1   −   d i ) ) Cost (Foundation)  Early Bird (1)   0.12   0.00   0.12   1,000 Streak Master (2)   0.10   0.10   0.09   2,500 Group Fitness (3)   0.08   0.20   0.064   6,250  Total Multiplier   1.274   9,750  This example shows a user evolving three traits, with the cost and adjusted bonus for each. Dimin- ishing returns reduce the value of each subsequent trait by 10%. The total cost is 9,750 Foundation tokens for a combined multiplier of 1.274.  8   Economic Analysis  8.1   Wealth Distribution Dynamics  Monte Carlo simulations 2   across 10,000 users for 365 days show that the protocol actively converges to broad equality. In baseline (honest) conditions, the Gini coefficient falls from 0.11 (day 30) to 0.08 by day 365 maintaining that after a year, the   typical user   is almost as well-off as their peers, and the bottom 50% holds meaningful wealth (see table). For context, the same simulation run with Bitcoin-style rules produces Gini values above 0.88, and “fiat” simulations stabilize around 0.56.  2 All results and source code available at   https://autophage.xyz/gini-simulation .  150   50   100   150   200   250   300   350   400  0 . 2  0 . 4  0 . 6  0 . 8  Target (honest)  Adversarial   < 0.55  Days  Gini Coefficient  Autophage (baseline)  Autophage (adversarial, 23%)  Bitcoin  US Dollar  Figure 3:   Gini coefficient evolution over one year (10,000 users, 0.7 activity rate).   Autophage Protocol converges to 0.08–0.11 in honest conditions and resists adversarial attacks up to 0.55 (with 23% of users acting against the system; see Appendix H). Traditional economies and Bitcoin remain above 0.80.  Example 8.1:   Wealth distribution after one year (mean of 10 simulation runs):  Population Segment   Autophage (baseline)   Autophage (adversarial)   Bitcoin  Top 10% wealth share   15%   32%   78% Bottom 50% wealth share   37%   16%   2% Median user balance   650–820 tokens   210–270   Variable Max sustainable balance   ∼ 18,000 tokens   ∼ 34,000   Unlimited Final Gini   0.08–0.11   0.54–0.55   > 0.88 Even under sustained, adversarial attack from almost a quarter of the network (including coor- dinated Sybil swarms and collusion pools), the protocol refuses to become a winner-take-all system. Inequality rises, but never approaches the levels endemic to fiat or crypto. And when the network is honest, almost everyone ends up close to the mean.  8.2   Revenue Model and Unit Economics  The protocol generates sustainable revenue through three main streams: 16Table 3: Revenue Streams and Contribution Margins  Revenue Stream   Per User/Month   Margin   Annual at 1M Users  App Integration Fees   $ 0.15   95%   $ 1.8M Marketplace Fees (12%)   $ 0.45   88%   $ 5.4M Enterprise Verification   $ 1.25   92%   $ 15.0M  Total   $ 1.85   91.7%   $ 22.2M Example 8.2:   Break-even analysis:  Cost Component   Amount  Fixed costs (monthly)   $ 30,000* Variable cost per user   $ 0.15/month Revenue per user   $ 1.85/month Contribution margin   $ 1.70/month Break-even users   17,647  Projected timeline   Month 8–10 This table summarizes the core unit economics of the protocol, showing that a monthly fixed cost of   $ 30,000 and a variable cost of   $ 0.15 per user are offset by   $ 1.85 in monthly revenue per user, resulting in a break-even point at 17,647 users.   3  9   Empirical Governance  9.1   Contribution-Based Voting  Traditional token-weighted voting fails when tokens decay. The protocol implements contribution scoring inspired by commons governance principles [25]:  C score   =   w 1   ×   log 10 (1 +   R lif etime ) +   w 2   ×   V reputation   +   w 3   ×   A participation   (40) where   w 1   = 0 . 5,   w 2   = 0 . 3,   w 3   = 0 . 2.  Example 9.1:   Voting power calculation:  User   Lifetime Tokens   Accuracy   Active   Score   Voting Power  Alice   50,000   95%   80%   2.80   1.67 Bob   5,000   70%   40%   2.14   1.46 Score calculation: Alice achieves a score of 0 . 5   ×   log 10 (50 ,   001) + 0 . 3   ×   0 . 95 + 0 . 2   ×   0 . 80 = 2 . 80, while Bob’s score is 0 . 5   ×   log 10 (5 ,   001) + 0 . 3   ×   0 . 70 + 0 . 2   ×   0 . 40 = 2 . 14.  3 All economic analysis and results are available at   https://autophage.xyz/economic-simulations . Fixed costs are set at   $ 30,000 for demonstration only. Actual costs will vary depending on whether cloud or physical infrastructure is used, among other factors.  179.2   On-Chain Experimentation  All protocol changes require empirical validation: Success =     μ treatment   −   μ control  μ control  ≥   ϵ    ∧   ( p < α )   (41) where   ϵ   = 0 . 05 (5% improvement) and   α   = 0 . 05 (statistical significance).  Example 9.2:   Seasonal affective multiplier proposal:  Experiment Parameter   Value  Hypothesis   2x winter rewards increase activity Stake required   500 Catalyst tokens Test group size   5,000 random users Control group size   5,000 random users Duration   30 days  Results  Control average   12.3 activities/month Treatment average   15.8 activities/month Improvement   28.5% p-value   0.0012 Outcome   Proposal passes Reward   Stake returned + 100 USDC bonus  9.3   Gas Optimization Strategies  Efficient implementation enables scalability: 1.   Lazy Decay : Calculate only on interaction. This optimization saves approximately 17,000 gas per unused day through storage optimization using a single timestamp per user-species pair. 4  2.   Batch Processing : Aggregate multiple proofs with batch sizes up to 100 proofs, achieving 85% gas reduction compared to individual transactions through Merkle tree root submission for verification. 3.   State Channels : Off-chain accumulation with daily settlement frequency, reducing on-chain transactions by 95% through Lightning-style payment channels for micropayments.  Example 10.2:   Gas cost comparison:  Verification Method   Calculation   Monthly Gas  Individual daily   65,000 gas   ×   30   1,950,000 gas Batched weekly   120,000 gas   ×   4   480,000 gas  Savings   75% reduction in gas costs  4 Empirical gas measurements were conducted using Ethereum mainnet test contracts for both naive decay and optimized “lazy decay” storage logic.   The 17,000 gas savings is calculated as the difference in gas usage between recalculating and updating all balances on each block versus the optimized approach, which applies decay only on interaction. See full benchmarking code and methodology at   https://autophage.xyz/gas-optimization .  1810   Implementation Architecture  10.1   Core Smart Contracts  The protocol consists of four primary contracts with specific responsibilities   5 : 1.   AutophageToken : Multi-species token with lazy decay evaluation  function balanceOf(address user, uint8 species) returns (uint256) { uint256 timeSince = block.timestamp - lastUpdate[user][species]; uint256 decayFactor = (PRECISION - decayRate[species]) ** timeSince; return balance[user][species] * decayFactor / (PRECISION ** timeSince); }  2.   ReservoirContract : Dual-chamber treasury management  function processHealthcareClaim(Claim memory claim) { require(usdcBalance >= claim.amount, "Insufficient reserves"); uint256 priority = calculatePriority(claim); priorityQueue.insert(claim, priority); processPriorityQueue(); }  3.   VerificationEngine : zkVM proof processing  function verifyAndMint(Proof memory proof, uint8 species) { require(zkVerifier.verify(proof), "Invalid proof"); uint256 amount = calculateReward(proof, species); tokenContract.mint(msg.sender, species, amount); }  4.   GovernanceContract : Experiment management  function evaluateProposal(uint256 proposalId) { Proposal memory p = proposals[proposalId]; (uint256 improvement, uint256 pValue) = calculateResults(p); if (improvement >= MIN_IMPROVEMENT && pValue < ALPHA) { implementProposal(p); returnStakeWithBonus(p.proposer); } }  5 Each of the prospective smart contracts is viewable at   https://autophage.xyz/smart-contracts . These con- tracts are experimental and so are intended solely for demonstration purposes.  1911   Limitations and Future Work  11.1   Technical Limitations  Several constraints affect current implementation. Most notably, proof-generation overhead remains a significant challenge because, on mobile devices, modern zero-knowledge systems such as Groth16, Aurora, and Zexe [13, 4, 7] can take tens of seconds to generate a proof (and sometimes over a minute for complex circuits), while more optimized toolchains and smaller circuits may complete in 1–2 seconds [26]. This performance bottleneck limits user experience and complicates real-time verification. Additionally, cross-chain fragmentation introduces complexity, as each blockchain environment demands a separate deployment and maintenance cycle, hindering seamless interoperability across ecosystems.   Regulatory uncertainty further complicates matters; health token classification and compliance requirements vary by jurisdiction, making global rollout an ongoing challenge. Beyond technical and legal constraints, behavioral inertia is perhaps the most stubborn evi- denced by prior experiments with demurrage currencies consistently revealing user resistance to the very idea of “disappearing money” [11, 10].   As a result, there is a clear need for careful behavioral design and user education in any system based on token decay.  12   Conclusion  The Autophage Protocol demonstrates that economic systems can theoretically implement biologi- cal principles to create more equitable and health-promoting outcomes. By introducing mandatory token decay calibrated to health activity persistence, the protocol creates an economy where value must flow to exist, preventing the pathological accumulation that characterizes traditional monetary systems. The mathematical framework suggests several promising properties. First, wealth distribution would likely converge to Gini coefficients [12] between 0.08 and 0.11 based on Monte Carlo simu- lations [1], compared to 0.82 for fiat currencies and 0.88 for Bitcoin. 6   This would occur through fundamental mechanics rather than redistribution or taxation.   Second, the endogenous pricing model ties value directly to health effort, making speculation unprofitable while rewarding genuine wellness activities. Third, the privacy architecture enables verification without surveillance, solving a decades-old tension in public health monitoring. The theoretical model indicates the protocol could achieve break-even at approximately 17,000 users with 91.7% contribution margins. The dual-chamber Reservoir design should maintain health- care settlement guarantees even under extreme stress scenarios. The empirical governance system would enable continuous optimization based on statistically significant health improvements. Perhaps most significantly, the protocol exists as a complete alternative to permanent-value economics. Where Bitcoin [24] provides digital gold and Ethereum [8] enables programmable con- tracts, the Autophage Protocol introduces digital metabolism. These three primitives, geological permanence, computational flexibility, and biological persistence, together could form a complete foundation for human-centric digital economies.  6 Under adversarial conditions (23% of users acting adversarially), Gini remains capped below 0.55; see Appendix H for simulation details.  2013   Works Consulted References  [1] 0x42 Research.   Gini coefficient simulation script. GitHub, 2025.   https://autophage.xyz/ gini-simulation . [2] W.C. Allee. Animal aggregations: A study in general sociology.   University of Chicago Press , 1931. [3] Ian J. Amanna, Nicholas E. Carlson, and Mark K. Slifka. Duration of humoral immunity to common viral and vaccine antigens.   New England Journal of Medicine , 357(19):1903–1915, 2007.   Seminal study on long-term antibody persistence in humans.   https://pubmed.ncbi. nlm.nih.gov/17989383/ . [4] Eli Ben-Sasson, Alessandro Chiesa, et al. Aurora: Transparent succinct arguments for r1cs. In  Annual International Conference on the Theory and Applications of Cryptographic Techniques , pages 103–128. Springer, 2019. Whitepaper.   https://eprint.iacr.org/2018/828.pdf . [5] Carl Bergmann.  ¨ Uber die verh¨ altnisse der w¨ arme¨ okonomie der thiere zu ihrer gr¨ osse.   G¨ ottinger Studien , 3:595–708, 1847. [6] Robert Blotevogel, Eslem Imamoglu, Kenji Moriyama, and Babacar Sarr. Income inequality measures and economic growth channels.   Journal of Macroeconomics , 72:103413, 2022.   https: //www.sciencedirect.com/science/article/pii/S0164070422000167 . [7] Wu Bowe, Ariel Gabizon, et al. Zexe: Enabling decentralized private computation. In   Advances in Cryptology–EUROCRYPT 2020 , pages 967–1004. Springer, 2020.   Whitepaper.   https:// eprint.iacr.org/2018/962.pdf . [8] Vitalik Buterin. A next-generation smart contract and decentralized application platform. In  Ethereum White Paper , 2014.   https://ethereum.org/en/whitepaper/ . [9] Richard Dawkins.   The Selfish Gene . Oxford University Press, 1976. [10] Mark Ferguson and Freicoin Developers.   Freicoin (demurrage-based cryptocurrency), 2013. Project homepage and overview:   https://freico.in/about/ ; no reliable technical whitepa- per available. For community description, see   https://wiki.p2pfoundation.net/Freicoin . [11] Silvio   Gesell.   The   Natural   Economic   Order .   Peter   Owen   Ltd.,   1958.   Full   En- glish   text:   Silvio   Gesell   Foundation.   Available   at   https://www.silvio-gesell.de/ the-natural-economic-order.html . [12] Corrado Gini. Measurement of inequality of incomes.   The Economic Journal , 31(121):124–126, 1921. [13] Jens Groth.   On the size of pairing-based non-interactive arguments.   Annual International Conference on the Theory and Applications of Cryptographic Techniques , pages 305–326, 2016.  https://link.springer.com/chapter/10.1007/978-3-662-49896-5_11 . [14] Austin Harshberger. Privacy-preserving health verification system with incentive mechanism for regular testing, 2025.   https://0x42r.io/patents/1.pdf . 21[15] Max Kleiber. Body size and metabolism.   Hilgardia , 6(11):315–353, 1932. [16] Bryan Kolb and Robbin Gibb.   Neuroplasticity and behavior:   current research and future directions.   International Review of Neurobiology , 100:235–254, 2011.   Comprehensive review of neuroplasticity timescales.   https://pmc.ncbi.nlm.nih.gov/articles/PMC10425702/ . [17] Find Satoshi Lab. Step’n: Move-to-earn on web3, 2022. Whitepaper.   https://whitepaper. stepn.com/ . [18] Simon A. Levin. Optimal foraging theory: A critical review.   Annual Review of Ecology and Systematics , 7:141–154, 1976. [19] Justus Liebig. Organic chemistry in its application to agriculture and physiology.   Journal of the Chemical Society , 1840.   https://www.loc.gov/item/12009827/ . [20] R.H. MacArthur and E.O. Wilson. The theory of the niche.   Population Biology , 1:161–170, 1967. [21] Timothy C. May.   The crypto anarchist manifesto.   Extropy , 1(2), 1997.   https://www. activism.net/cypherpunk/crypto-anarchy.html . [22] Branko   Milanovic.   Global   inequality   of   opportunity:   How   much   of   our   income is   determined   by   where   we   live?   The   Review   of   Economics   and   Statistics , 97(2):452–460, 2015.   https://direct.mit.edu/rest/article-abstract/97/2/452/58223/ Global-Inequality-of-Opportunity-How-Much-of-Our?redirectedFrom=fulltext . [23] I. Mujika and S. Padilla. Detraining: Loss of training-induced physiological and performance adaptations. part i: Short term insufficient training stimulus.   Sports Medicine , 30(2):79–87, 2000.   https://pubmed.ncbi.nlm.nih.gov/10966148/ . [24] Satoshi Nakamoto. Bitcoin: A peer-to-peer electronic cash system, 2008.   https://bitcoin. org/bitcoin.pdf . [25] Elinor Ostrom.   Governing the Commons: The Evolution of Institutions for Collective Action . Cambridge University Press, 1990. [26] vacp2p.   zksnark proving benchmarks on mobile devices (issue #7).   https://github.com/ vacp2p/research/issues/7 , 2021. Accessed: 2025-07-13. [27] John von Neumann. First draft of a report on the edvac, 1945.   https://people.csail.mit. edu/brooks/idocs/VonNeumann_EDVAC.pdf . 2214   Appendix A   Complete Feature Set  Table 4: Comprehensive Protocol Feature List  Feature   Description  Proof of Temporal Persistence   Core mechanism requiring continuous health activity to maintain token value through mandatory exponential de- cay Four Token Species   Rhythm (5% daily), Healing (0.75%), Foundation (0.1%), Catalyst (2-10% dynamic) with decay rates matching ac- tivity persistence Metabolic Pricing Model   Value emerges from measurable energy costs of health activities rather than market speculation Dual-Chamber Reservoir   Separate token and USDC chambers ensure healthcare liquidity while managing community redistribution Zero-Knowledge Privacy   Complete   cryptographic   separation   of   identity   from health data using 768-bit entropy ProfileIDs Genetic Adaptation System   Burn Foundation tokens to evolve up to 10 permanent earning traits creating personalized health profiles Kleiber’s Law Scaling   Network efficiency increases with   N   0 . 25   providing benefits to all users as ecosystem grows Liebig’s Law Implementation   Automatic identification of system bottlenecks with 2x reward multipliers for constrained activities Allee Effect Protection   Enhanced support multipliers (up to 2x) for networks be- low 1,000 users preventing death spirals Circadian Rhythm Rewards   Time-of-day multipliers following natural biology with 30% peak bonus at optimal times Empirical Governance   All protocol changes require on-chain A/B tests demon- strating 5%+ improvement with p ¡ 0.05 Privacy Marketplace Tiers   Anonymous, ProfileID, Genetic, and Full disclosure levels with 0-500% price premiums Wellness Vaults   Time-locked token storage with reduced decay rates for healthcare saving (30-365 day terms) Progressive Wealth Decay   Accelerated decay rates for large balances preventing ex- treme accumulation Achievement Multipliers   Permanent   bonuses   for   reaching   health   milestones (marathons, sobriety anniversaries, etc.) 23Feature   Description  Synergy Bonuses   10-25% rewards for engaging multiple health domains daily promoting holistic wellness Dynamic Energy Gradients   Counter-cyclical reward adjustments maintaining system sustainability during activity fluctuations Healthcare Priority Queue   Medical   settlements   receive   absolute   priority   with urgency-based processing Community Catalyst Pools   Groups can purchase temporary ecosystem-wide activity boosts benefiting all participants App Stake Requirements   Verification apps stake 10,000+ USDC with progressive slashing for false attestations Seasonal Multipliers   Natural activity patterns reflected in rewards (e.g., 2.5x January for New Year motivation) Statistical Fraud Detection   Anomaly detection prevents impossible activity claims using behavioral pattern analysis Enterprise Verification API   Bulk   wellness   verification   for   corporate   programs   at  $ 40/employee/year Insurance Integration   Real-time premium adjustments based on verified healthy behaviors Time-Decayed Cash Access   Formula   A ( t ) = 0 . 3+0 . 7 × 0 . 8 t   preserves healthcare utility while discouraging hoarding Contribution Scoring   Lifetime ecosystem value determines governance weight rather than current token holdings Feature Natural Selection   Unused features sunset automatically after 180 days be- low 1% utilization Parameter Evolution   System constants adjust based on empirical health out- comes from experiments Cross-Activity Bonuses   Complementary activities earn compound rewards (e.g., exercise + therapy = 1.5x multiplier) Recovery Support System   Enhanced rewards and reduced decay during verified health setbacks Geographic Equity Module   Automatic support for underserved regions through dy- namic multiplier adjustments Research Data Feeds   Anonymized aggregate health behavior data available for public health research Liquidity Provision Rewards   Catalyst tokens for providing marketplace liquidity and facilitating trades Multi-Signature Security   Critical protocol functions require 7-of-9 guardian signa- tures preventing single points of failure 24B   Glossary of Terms  Autophage  Self-consuming; in the protocol context, value that must decay to create circulation and prevent accumulation.  Catalyst Tokens  Fourth token species with dynamic 2–10% decay maintaining 25% ecosystem balance through automatic adjustment.   Catalyst tokens are earned through marketplace participation and protocol activity, used for staking, governance proposals, and price discovery tied to metabolic pricing. They cannot be vaulted or saved.  Circadian Multiplier  Time-of-day reward adjustment following natural biological rhythms, maximizing at 6 AM (1.3x) and minimizing at 6 PM (0.7x).  Contribution Score  Governance weight calculated from lifetime Reservoir contributions, reputation, and partici- pation rather than current holdings.  Decay Rate ( δ )  Daily percentage of token value lost, ranging from 0.1% (Foundation) to 5% (Rhythm) based on activity persistence.  Dual-Chamber Reservoir  Community treasury architecture separating decaying tokens from USDC reserves to ensure healthcare liquidity.  Endogenous Price  Value determined by internal health activity energy costs rather than external market spec- ulation.  Foundation Tokens  Slowest-decaying species (0.1% daily) earned through preventive care and long-term health investments.  Genetic Adaptation  System allowing users to burn Foundation tokens for permanent earning traits, creating spe- cialized economic profiles.  Gini Coefficient  A statistical measure of how wealth is distributed in a system.   Zero means everyone holds the same amount, while one means a single participant holds everything. In this protocol, a lower Gini coefficient signals that value is circulating widely.   Most national currencies and cryptocurrencies show Gini values above 0.80.  Half-Life  Time required for token balance to reduce by 50% without renewal; 13.51 days for Rhythm, 92.42 for Healing, 693 for Foundation.  Healing Tokens  Token species with 0.75% daily decay earned through therapy, recovery, and mental health activities. 25Kleiber’s Law  Biological scaling law implemented as   η ( N   ) = 1 + 0 . 3( N/ 1000) 0 . 25 , increasing efficiency with network size.  Liebig’s Law  Principle that system health equals its weakest component, triggering automatic reward re- balancing.  Metabolic Pricing  Price discovery based on actual energy expenditure (time, calories, opportunity cost) of health activities.  ProfileID  Cryptographic identifier with 768-bit entropy separating user identity from health activity data.  Proof of Temporal Persistence (PoTP)  Core protocol mechanism requiring continuous activity to maintain value, preventing passive accumulation.  Rhythm Tokens  Fastest-decaying species (5% daily) earned through exercise, medication adherence, and daily health habits.  Soft Cap  Balance threshold triggering accelerated decay to prevent excessive accumulation while al- lowing reasonable savings.  USDC  Dollar-pegged stablecoin issued by Circle and Coinbase. Used as a programmatic, fully-backed USD equivalent for on-chain settlements.  Wellness Vault  Time-locked token storage with reduced decay rates enabling targeted healthcare saving.  Zero-Knowledge Proof  Cryptographic method verifying health activities without revealing personal data, maintain- ing privacy.  zkVM Orchestration  Application layer managing proof generation across heterogeneous health apps while preserv- ing privacy. 26C   Selected Use Case Examples  The following use cases are hypothetical. Each user journey is designed to show how the protocol works in practice. No story describes a real person or actual event. These are demonstrations of what is possible with the Autophage Protocol.  C.1   Marathon Training: Sarah’s Journey  Sarah, a 29-year-old software engineer, uses the protocol to fund her gender-affirming surgery while training for marathons. Her daily routine generates multiple token streams: Morning runs earn 50–75 Rhythm tokens depending on distance and intensity. Her 30-day streak multiplier has grown to 2.48x, while her “Early Bird” genetic trait adds 15% for pre-7 AM activi- ties. Weekly therapy sessions generate 80 Healing tokens each, with her “Mental Resilience” trait providing an additional 12% bonus. Her Chicago Marathon completion created particularly valuable proof:  Reward Component   Value   Tokens  Base marathon reward   Standard   200 Foundation Sub-4:30 time bonus   1.5x multiplier   Applied Perfect weather penalty   0.9x   Applied  Total generation   200   ×   1.5   ×   0.9   270 Foundation  The zero-knowledge proof included timing chip data, GPS traces, heart rate patterns, and photo- graphic evidence, compressed into a verifiable statement: “Female 25–34 completed major marathon in 4:27:33.” She listed this anonymously for   $ 300, receiving   $ 264 after fees. Current progress toward   $ 4,000 surgery goal:  Income Source   Amount  Token value   $ 2,847 Proof sales   $ 1,584 Monthly average   $ 132 Months to goal   3.5 Sarah’s public ProfileID has attracted a following among trans athletes. While her gender identity remains private, her consistent performance inspires others facing similar journeys.  C.2   Recovery Economics: Marcus’s Story  Marcus discovered the protocol on day 387 of sobriety. While physical fitness remained challenging, his daily recovery meetings became an economic foundation. Each meeting generates 40 Healing tokens, with milestones providing substantial bonuses: 27Milestone   Token Bonus  30 days continuous   300 tokens 90 days   900 tokens 1 year   3,000 tokens 18 months   4,500 tokens His genetic evolution focused on recovery optimization:  Trait   Benefit   Bonus   Cost  Recovery Resilience   General recovery bonus   15%   1,000 Foundation Social Connection   Group activity bonus   12%   2,500 Foundation Consistency Rewards   Streak bonus   10%   5,000 Foundation Healing Focus   Healing token bonus   20%   10,000 Foundation Combined multipliers mean his daily meetings now generate 72 tokens instead of the base 40. His 12,000 Healing token balance represents   $ 8,040 in value, providing financial security that supports sustained recovery. Marcus monetizes his journey selectively.   Anonymous daily proofs sell for   $ 2-3 to researchers studying recovery patterns. His one-year sobriety proof sold for   $ 100. Monthly proof sales generate  $ 180 supplemental income, funding gym membership and therapy co-pays. The economic visibility of recovery has transformed his self-perception. “My sobriety has measur- able value,” he tells newcomers.   “Every day clean creates wealth for myself and data that helps others.”  C.3   Creator Economy: Luna’s Empire  Luna leverages the protocol to differentiate her 850K subscriber OnlyFans empire. Unlike competi- tors relying on photo editing and claims, she provides cryptographically verified fitness achieve- ments. Her daily routine becomes multi-stream content: Morning workout routine:  Component   Multiplier   Value  Base reward   -   50 Rhythm tokens 6 AM circadian bonus   1.3x   Applied Livestream multiplier   1.5x   200+ viewers Performance Aesthetics trait   15%   Applied Camera Ready trait   12%   Applied  Total generation   Combined   115 Rhythm tokens  Every activity generates both tokens and content. Post-workout biometrics create Foundation tokens while providing transparency her audience craves. Meal prep earns tokens while teaching nutrition. Even rest days generate Healing tokens through documented recovery practices. 28Her proof monetization strategy:  Proof Type   Individual Price   Bundle/Package  Daily workout   $ 5   $ 100/month Transformation   -   $ 200 (90-day) Personal records   $ 50-500   Varies by achievement Genetic trait revelation   $ 1,000   One-time With 17,000 proof subscribers paying   $ 200/month average, Luna generates   $ 3.4M monthly from verification alone. The protocol’s 12% marketplace fees seem minimal compared to the trust pre- mium her verification commands.  D   Plain Language Summary  Traditional money persists indefinitely.   A dollar earned decades ago maintains its value without effort.   This permanence creates economies where wealth concentrates among those who already possess it, while those who need resources most must continuously labor for finite rewards.   The Autophage Protocol introduces a fundamental alternative through money that decays. The protocol creates four species of digital tokens, each calibrated to match the persistence of different health activities.   Exercise tokens decay at five percent daily, reflecting how quickly cardiovascular fitness deteriorates without maintenance.   Therapy tokens decay at three-quarters of one percent daily, mirroring the gradual erosion of mental health progress.   Preventive care tokens decay at one-tenth of one percent daily, representing the long-term nature of vaccinations and screenings. A fourth species, Catalyst tokens, maintains ecosystem balance through dynamic decay rates. Every token must be earned through verified health activity. Running generates Rhythm tokens. Therapy sessions produce Healing tokens. Annual checkups create Foundation tokens. The system rewards consistency through multipliers where morning workouts earn circadian bonuses, group activities multiply rewards, and sustained streaks compound earnings.   Users who exercise daily maintain steady balances, while inactive users watch their balances decrease. Decayed tokens flow to The Reservoir, a dual-chamber treasury at the protocol’s heart.   One chamber collects expired tokens and redistributes them as rewards for new activities.   The other maintains stablecoin reserves to settle real healthcare expenses.   This architecture ensures that individual decay creates collective wealth, funding community health infrastructure through the natural expiration of unused value. Privacy remains absolute through cryptographic separation. The system verifies that you ex- ercised without revealing when, where, or how. Users control their disclosure level, from complete anonymity to selective revelation of patterns that command premium prices in health data mar- kets. Researchers purchase aggregated proofs to study behavior patterns. Insurers adjust premiums based on verified activities. Individuals monetize their consistency without sacrificing privacy. 29The protocol implements biological scaling laws throughout.   Network effects follow Kleiber’s Law, increasing efficiency as the ecosystem grows.   Bottlenecks trigger automatic rebalancing through Liebig’s Law.   Small communities receive enhanced support through Allee Effect multi- pliers. Every mechanism derives from observed biological systems rather than economic theory. Users adapt their economic metabolism through genetic traits, using Foundation tokens to evolve permanent earning multipliers.   Wellness vaults enable targeted saving by reducing decay rates for locked tokens. Progressive decay acceleration prevents extreme accumulation. Empirical governance requires all changes to demonstrate measurable health improvements through on-chain experiments. The result transforms economics from a system of accumulation to one of circulation. Wealth flows or disappears. Health activities generate immediate value. Privacy and transparency coexist through mathematics.   The Autophage Protocol experiment poses a fundamental question:   can an economy modeled on living systems create more equitable and sustainable outcomes than one modeled on geological permanence?  E   Understanding the Math for Everyone  This appendix explains the paper’s formal notation so that any motivated reader can understand what is being described. Mathematics serves two purposes in this protocol. It makes rules unam- biguous and repeatable, and it allows anyone to verify or critique the system’s logic without relying on trust or marketing claims.  Mathematical Symbols in Context  Table 5: Common Mathematical Symbols in the Autophage Protocol  Symbol   Meaning   Example Use  δ   Decay rate, the fraction of value lost per period   δ   = 0 . 05 means 5% daily decay  λ   Growth or scaling rate (context-dependent)   λ   may set user onboarding pace P   Summation, sum all values in a range   P n i =1   x i   means add   x 1   to   x n  γ   Sensitivity or acceleration parameter   γ   increases effect in adaptive decay  i, j, u   Indices for species, activities, or users   V   ( u )  i   is species   i   for user   u  ( t )   Value at time   t   G ( u )  i   ( t ) means reward at time   t  | x |   Absolute value, distance from zero   |− 3 |   = 3  ( u )   Superscript shows the user   u   V   ( u )   is value for user   u  Why These Symbols? A Brief History and Rationale  Many of the mathematical symbols used here have a long history, appearing in physics, biology, and economics: 30Table 6: Historical and Disciplinary Context of Core Mathematical Symbols  Symbol   Origins & Common Uses   Why Used Here  δ   Greek letter delta.   Used for small changes or decay (calcu- lus, physics, biology). Represents   decay   rates   for   token value,   mirroring biological loss or depreciation.  λ   Greek   letter   lambda.   De- notes   growth   rates,   eigen- values   (math,   ecology,   eco- nomics). Captures   system   growth,   arrival rates, or user inflow; signals scaling or tempo.  P   Uppercase   sigma.   Ancient Greek,   adopted   in   18th- century math.   Standard for summing   sequences   (statis- tics, physics, economics). Indicates summing over activities, users, or token classes; fundamental to reward calculations.  γ   Greek   letter   gamma.   Used for rates of change, sensitivity, and scaling (physics,   econo- metrics). Adjusts the strength or “sharpness” of   protocol   effects   (such   as   how quickly penalties accelerate).  How to Read the Equations Aloud  Here are examples of how to say these formulas out loud, and how to “translate” notation to spoken language:  Example 1: Token Balance Evolution  V   ( u )  i   ( t   + 1) =   V   ( u )  i   ( t )(1   −   δ i ) +   G ( u )  i   ( t ) Read aloud: “V sub i, superscript u, at time t plus one, equals V sub i, superscript u, at time t, times one minus delta sub i, plus G sub i, superscript u, at time t.” In plain English, this means: “The balance for user u, token species i, tomorrow, equals today’s balance after decay, plus the new tokens earned.”  Example 2: Adaptive Catalyst Decay  δ catalyst ( t ) =   δ base   ×    1 +   β   ×   V catalyst ( t )  P  i ∈ S   V i ( t )   −   ρ target  γ     Read aloud: “Delta sub catalyst at time t equals delta base times the quantity one plus beta times the absolute value of the ratio of V catalyst at t over the sum from i in S of V sub i at t, minus rho target, all raised to the power gamma.” For clarity:  •   “Sub” means the subscript (the variable after the underscore).  •   “Superscript” (or “to the power”) means the small number or letter above the main symbol.  •   P n i =1   x i   is read “the sum from i equals one to n of x sub i.”  •   Greek letters are read by name: “delta,” “lambda,” “gamma,” “rho.” 31No mathematical symbol in this paper is used for gatekeeping. The explicit notation is necessary for precision and trust.   Each equation can be seen as a technical contract between the protocol and its users. If anything is unclear, these tables provide a reference for decoding the symbols, and every formula can be read aloud, checked in simulation, or translated into code. In short, the math is here so anyone can follow the logic, test the claims, and keep the system honest.  F   Note from the Author  Biomimetics, if it means anything, is enforcement rather than metaphor. In Autophage, decay is structural. Value dies if it does not circulate. Most systems treat nature as garnish; here, mortality is code. Accumulation outside of health savings is a liability. The status quo rewards stasis and calls it success.   In health and in money, what survives is what moves. Everything else is just afterlife accounting. – The Autophage Protocol and Proof of Temporal Persistence remain an experiment in economic design, one I hope to implement when time and resources allow.   The mathematics have been carefully developed and the simulations run thousands of times, but the true test lies in whether individuals and communities will choose renewal, flow, and life over accumulation, comfort, and stasis. With this paper, the theoretical framework is complete; only practical implementation awaits. My hope is that, in creating value which must move to exist, we might discover economics that prioritize continual renewal of value, mirroring living systems and benefiting all instead of the few. The mathematics of decay could become the foundation for sustainable abundance, but only real-world deployment will reveal if theory translates to practice.  G   Acknowledgments  This work stands on the shoulders of giants. Deep gratitude to my family and friends who provided both emotional support and critical feed- back through countless iterations. Your patience with my obsession over decay rates and biological metaphors exemplifies the human connections that make any economic system worthwhile. The protocol builds upon decades of groundbreaking work in cryptography and distributed systems. I acknowledge the cypherpunks [21] who first envisioned private digital cash when such ideas seemed impossible. The zero-knowledge research community [13] transformed privacy from policy to mathematics, making health verification without surveillance achievable.   Bitcoin [24] demonstrated programmable scarcity.   Ethereum [8] generalized blockchain computation.   The broader ecosystem continues advancing human freedom through code. I learned a lot from researchers and writers like Dawkins [9], Ostrom [25], von Neumann [27], and from reading far too many papers instead of sleeping. They didn’t endorse this work, but their ideas made it possible to approach health and economics differently. And to OpenAI and Anthropic for their AI language models, which served as tireless intellectual companions, helping bridge evolutionary biology, cryptographic theory, and economic modeling while enabling rigorous mathematical formalization and technical validation. Thank you all. 32H   Adversarial Stress Test  Methodology  To test protocol resilience, a Monte Carlo simulation was run with 10,000 users over 365 days. Twenty-three percent of users were assigned adversarial behaviors intended to maximize wealth concentration and increase the Gini coefficient. Eight attack strategies were implemented, including sybil swarm formation, passive hoarding, coordinated collusion, periodic burst activity, churn- based exploitation, whale concentration, front-run transfers, and composite adaptive attacks. All simulation parameters (decay, activity rate, whale decay thresholds) matched those in the main experiments.  Results  Despite sustained adversarial activity, the protocol demonstrated significant resistance to inequality amplification. Table 7: Summary of Adversarial Stress Test Outcomes  Condition   Final Gini Coefficient   Interpretation  Baseline (clean)   0.33   Equitable equilibrium Adversarial (23%)   0.55   Moderate inequality, system robust US Dollar   0.82   Pathological inequality Bitcoin   0.88   Pathological inequality The highest Gini observed at initialization was 0.63, reflecting initial distribution noise. At one year, the system stabilized at a Gini of 0.55 which is higher than baseline, but well below traditional currencies and cryptocurrencies. Sybil swarms produced the wealthiest individual accounts, but failed to generate runaway in- equality.   All other attack vectors, including passive hoarding and collusive pooling, were limited by the protocol’s decay mechanics and progressive whale decay. Hoarding and inactivity consistently led to rapid value loss, demonstrating that “money must move” is a structural property.  Mechanisms of Robustness  Three protocol mechanisms proved decisive: 1.   Universal decay:   Exponential decay applies to all balances, regardless of strategy or iden- tity. 2.   Progressive whale decay:   Accumulated balances experience accelerated decay, capping potential for dominance. 3.   Activity coupling:   Reward accrual remains inseparable from continuous participation; pas- sive or cyclical exploit attempts are self-limiting. 33Conclusion  Adversarial simulation confirms that no known strategy can meaningfully circumvent enforced decay and activity requirements. Even with coordinated attacks and sybil formations, the protocol maintained Gini coefficients below those observed in all major legacy and digital monetary systems. This outcome validates the protocol’s core claim that value must circulate to persist, and attempts to evade circulation lead only to accelerated loss.  I   On the Irreversibility of Health Value Conversion  Conversion between token species is not permitted within the Autophage Protocol. Each token species, Rhythm, Healing, Foundation, and Catalyst, functions as a discrete record of a particular kind of health persistence.   The protocol is intentionally designed to make these categories non-fungible because doing so serves as a structural assertion about how value should reflect real-world persistence. Allowing conversion between tokens would erode the biological foundation of the protocol, making it possible to “farm” one domain of health for rewards in another.   In practice, such a mechanism would create arbitrage opportunities, invite speculation, and undermine the protocol’s core behavioral incentives. Metabolic activity is not inherently fungible; cardiovascular endurance cannot be traded for vaccine immunity, and emotional recovery cannot be converted into daily exercise. Proof of Temporal Persistence is defined by the activity that generates each token.   If a user wishes to hold more Foundation tokens, they must engage in preventive care; to increase Healing tokens, they must participate in therapy or recovery.   The marketplace allows users to monetize verified actions or proofs, but does not flatten the specific metabolic history those proofs represent. Catalyst tokens serve as the protocol’s primary “liquidity” vector for governance and marketplace functions, but are not a substitute for sustained health actions in any other domain. This irreversibility anchors the protocol to the realities of biology, closing the door to specula- tive gamesmanship and ensuring that economic rewards remain tethered to real, verifiable health persistence. Each token is a distinct receipt for a unique kind of effort and persistence. If you want Foundation, you must do the work.  J   AI Attestation  ASSISTED  This paper was assisted by Claude Opus 4.1 and ChatGPT 4.1-5.   These tools served a role similar to LaTeX for typesetting or computational systems for verification, enabling more rigorous expression and validation of ideas. All core concepts, insights, and creative decisions are original human work. 7  7 For detailed AI tool attribution and transparency, see https://attest.ink/s/yh8ie9vw.  34K   Version History  Table 8: Protocol Evolution Timeline  Version   Date   Major Changes  v0.1   May 2025   Initial concept: health verification with privacy v0.2   June 2025   Introduced decay mechanics and biological metaphors v0.3   June 2025   Formalized Proof of Temporal Persistence v0.4   June 2025   Multiple token species with variable decay v0.5   June 2025   Complete shift to metabolic economics paradigm v0.6   June 2025   Dual-chamber Reservoir architecture v0.7   June 2025   Mathematical formalization for academic review v0.8   June 2025   Precision alignment with implementation v0.9   June 2025   Catalyst tokens and marketplace dynamics v1.0   June 2025   Individual metabolic capacity model v1.1   June 2025   Governance refinements and corrections v1.2   July 2025   Adversarial stress testing; Gini remains   <   0 . 55 v1.3   July 2025   Complete academic paper for arXiv 35

---

---layout: posttitle: "hello world"date: 2025-05-13 12:00:00 -0500categories: [general, ai, startups, personal]attribution: human---## introduction to this blogI go by 97 115 104 and this is my personal blog site. It contains arbitrary thoughts on various topics and information about things I build. While I don't profess to be an oracle, I do hope there's some helpful insight in my sharing of what I've learned that would be useful for others. If it is indeed useful or not is up to *you* as The Reader.## about meI'm a [techie](https://en.wiktionary.org/wiki/techie) located in San Francisco, currently in my early 30s. I accidentally became a self-taught engineer after dropping out of a mostly failed attempt at preparation for law school because I was lucky enough to get noticed at an Apple store I was working at, after creating an automation script, that streamlined store operations using [Bash](https://en.wikipedia.org/wiki/Bash_(Unix_shell)). Apple liked it enough to hire me at 19 with no degree or previous experience as an engineer and moved me to Cupertino with a sign-on bonus. I was there for a few months before deciding it wasn't for me. Thankfully they didn't ask for the bonus back. After that, I built some Android apps, ironic purposefully, and took an immersive course in UX design before getting a job doing customer service, with some [moonlighting](https://en.wikipedia.org/wiki/Moonlighting) as a developer, at a place called IDEO. I was there for a bit before getting a job at GitHub, where I was long enough to see the Microsoft acquisition. I left GitHub to join the [Web3](https://en.wikipedia.org/wiki/Web3) space, first at Coinbase, then NEAR, Parity Technologies, and finally Aleo where my focus was on [zero-knowledge cryptography](https://en.wikipedia.org/wiki/Zero_knowledge). Probably thanks to my Mom, people generally tend to like me so I progressed through my career in a vertical-and-up fashion, from engineering to recruiting, to program management, then product management. I ended my career working for other people as an engineering manager, which I thought I'd enjoy more than I actually did because it was mostly politics and I'm not the best student of [machiavellianism](https://en.wikipedia.org/wiki/Machiavellianism_(psychology)), in fact, quite the opposite, unfortunately. Now, thanks to a confluence of events, a good idea, and too many STIs, I am the founder of a startup called [status.health](https://status.health) focused on improving sexual wellness and reducing STI transmission rates.## ai tools and meI am an AI tools maxi and believe, strongly, that they help more than they hurt. Though, for some reason, I find mine an unpopular opinion among peers. When I reflect on it, I am reminded of the early days of every groundbreaking emerging technology: the industrial revolution, the telegram, cameras, phones, cell phones, the internet, social media, crypto, and now AI. EVERY SINGLE TIME it scares people. Luckily, I don't scare easily and am often eager to learn something new.AI tools are nowhere near as good as humans at any particular task--they are "generally" intelligent but not expert at anything regardless of how good you are at prompting them. They are much cheaper and easier to delegate to than an employee in the context of a startup or as a business owner. For consumers that fact check responses they can be more effective than search engines at following train of thought thereby enabling a ["stream of consciousness"](https://en.wikipedia.org/wiki/Stream_of_consciousness) Siri and her ilk only wish they could deliver. As such, AI tools have their place in growing a startup, helping with learning, and other things that require specialized knowledge or a huge learning curve: programming, complex math, a new language--you name it. AI is terrible at being creative. Probably not news to most of you reading, but worth reiterating as I continue to find it annoying since I am not the best "creative" either. When making assets for products I build, it's often the hardest task there is; I guess job security for the designers, artists, and writers out there. Congratulations.I feel strongly that everyone should and probably will eventually use AI, but I also believe in being clear about when it was used. Why not just say:> ❔"I created X with an initial prompt and multiple iterations based on research. After some back-and-forth, Claude and I got X to where it is now and I am proud of it. I probably don't know *everything* about it but I know most of it and can answer any questions you have."Not attesting to the way X deliverable was created only impacts your reputation negatively when you inevitably get stumped answering a question about something outside your depth. It's like a teacher calling on you in English class after you chose to spend the night playing World of Warcraft instead of reading "The Scarlet Letter." You should have just read the thing, but short of that, letting your teacher know you played WoW instead and plan to listen to the other students' answers to learn that way, is the next best thing.In short: AI-assisted anything should be attested to (clearly marked), and so should human-generated content because it addresses two key concerns most people have: 1. a person misrepresenting work and not actually knowing what a deliverable is or does, and 2. AI's reputation for being a human-killer vs. a human-enabler, except, of course, for the creatives. Assigning credit where credit is due is not new. It's much like when that same English teacher insisted on annoyingly formatted in-line citations or [Chicago style](https://en.wikipedia.org/wiki/The_Chicago_Manual_of_Style) for book reports.To make attesting to AI-assisted work vs. that of human work easier for myself and my businesses, Claude and I created a site called [attest.ink](https://attest.ink) which allows automated badge generation. I use attest.ink and its [attestations](https://en.wikipedia.org/wiki/Attestation) for almost everything I produce that has a public eye on it, and you should too. It's free! Check it out and let me know if you have any questions or suggested improvements: [info@attest.ink](mailto:info@attest.ink).#### what badge attestations look like<!-- Add this to your blog HTML --><a href="https://attest.ink" target="_blank" class="badge-link" rel="noopener">  <img src="https://attest.ink/assets/badges/human-generated.svg" alt="Human Generated" width="120" height="30">  <img src="https://attest.ink/assets/badges/chatgpt-generated.svg" alt="AI Assisted" width="120" height="30">  <img src="https://attest.ink/assets/badges/claude-generated.svg" alt="AI Assisted" width="120" height="30">  <img src="https://attest.ink/assets/badges/gemini-generated.svg" alt="AI Assisted" width="120" height="30">  <img src="https://attest.ink/assets/badges/midjourney-generated.svg" alt="AI Assisted" width="120" height="30">  <img src="https://attest.ink/assets/badges/dalle-generated.svg" alt="AI Assisted" width="120" height="30"></a>#### for those who think one can't determine if content is ai-generatedYou are wrong because, thankfully, there are a growing number of tools out there that can do. While they mostly use heuristics (educated guesses based on data), they do help "out" content where an individual is trying to claim something was generated without AI assistance. For those moments when it feels like misrepresentation might be afoot, or if you are simply curious, I found [Copyleaks's AI detector](https://copyleaks.com/ai-content-detector) the most useful—it even has a [Chrome extension](https://copyleaks.com/ai-content-detector/extension). So if you are the person being disingenuous, sorry not sorry.## ai and this blog siteBlog posts on this site are usually not AI-assisted other than for grammar and spelling checks because I am terrible at both. Blog content that is marked as human-generated is original other than that content, ideas, and interpretations are based on my analysis of other people's works, thoughts I've had based on conversations with friends and family, and general content consumption--we are, after all, a product of our environment. Not dissimilar from how AI is a product of the data it has access to. While I could easily use Claude to write every post though ChatGPT is better, I think it's more valuable to write them myself. Doing so serves as a good reminder of the time it takes to make something without AI-assistance, and it's fun despite how grueling I find long-form writing. I will admit that I am also creating this blog and a vast majority of its posts so I can train the various AIs on my writing style, voice, and humor, since they are currently woefully bad at both due to the lack of data out there representing me. When a blog post is AI-assited, you'll know via the attestation badge. The non-blog content on this site such as the site itself, *was* created with AI assistance because Claude is a faster engineer than I am (though not better ). That said, you can take solace in the fact that you are reading my thoughts, unadulterated as they may be. Look out for the "Human Generated" or "AI-Assisted" badge for clarity.## mostly open sourceFor the most part everything I create is under the typical open source [MIT License](https://opensource.org/licenses/MIT). You should feel free to use my words, my website content, and thoughts, I just ask for proper citation, e.g., `Harshberger, A, $DATE, $BLOG-POST-NAME` or `attest.ink, A. H. $DATE`. The exception to this rule is anything over at status.health or where I explicitly mark something under copyright with ©. In those instances, just shoot me an email for permission or with any questions: [x@97115104.com](mailto:x@97115104.com).------layout: posttitle: "i'm probably going to launch a new cryptocurrency in addition to status.health..."date: 2025-05-31 02:00:00 -0500categories: [crypto, privacy, health, web3, zk, startups]attribution: ai claude opus 4---## nobody else is going to fix this mess for usIf you read my [hello world post](https://97115104.com/2025/05/13/hello-world/), you already know I’m a techie, but you can probably also guess I’m not in this for tech’s sake—and I’m even less of a fan of breathless “we’re disrupting X!” manifestos. In a world where even Trump has a meme coin, the bar for “launching a token” is officially on the floor. So, here’s where I stand: I’m probably going to launch a new cryptocurrency—not because I want my face on a billboard (actually that would freak me out), but because after years of wrestling with health tech, incentives, and privacy, I keep running into the same broken machinery and nobody else seems interested in fixing it.Lately, this has gotten a lot more personal. I was fired for not being a “cultural fit”—which, in plain English, meant I wouldn’t work weekends for my boss. I just didn’t care enough at the time, but am now writing this at 2 a.m. on a Saturday—#ironic #iconic. Anyways, since going independent, I’ve landed squarely in the same healthcare mess as countless freelancers, gig workers, and marginalized folks: sky-high costs, awful coverage, and almost no meaningful support for prevention. Throw in my own ongoing battles with STIs (yes, plural), and I’ve had a front-row seat to just how neglected and underfunded prevention really is.The big “solutioneers”—Kaiser, the government, insurance barons—are either too massive to move, too tangled up in lobbyist interests, or fundamentally structured to maintain the status quo. Insurance providers, especially, have incentives that run completely counter to most people’s real needs. So here I am: personal prevention meets a funding crisis—a weirdly perfect storm for someone stubborn enough (and just well-placed enough) to try building something new. I’m not here for vanity or hype (eww!). I’m here because the system’s busted, the usual players aren’t stepping up, and, for better or worse, I might just be in the right spot to do something about it.## why? (seriously, why?)Short version: the incentives in healthcare are upside down, and no amount of dashboards, SaaS contracts, or “secure” portals are going to fix that. If you’ve ever tried to prove you got tested, vaccinated, or did literally anything responsible, you know the drill: fill out a form, log into yet another portal, get a PDF for your trouble, and hope it doesn’t end up in some vendor’s next data breach. If you actually want to be rewarded for doing the right thing? Good luck. The system pays when you get sick, not when you stay healthy.Meanwhile, prevention is a cost center, not a business model. Nobody wants to fund it, even though [it’s the only thing that actually works](https://odphp.health.gov/news/202401/prevention-still-best-medicine) in the long run. The few times you are “incentivized,” it’s either a spammy sweepstakes or a $10 Amazon card—never enough to matter, never enough to make it feel like your effort is valued. And through it all, you’re handing over more data than you’re comfortable with, for less and less in return.Most people shrug and move on. I apparently have a higher tolerance for banging my head against the wall than most, so here we are.## what the hell would this even look like?I don’t want to build another “health app.” I want to build a protocol—a new set of rails that flips the incentives, rewards prevention, and makes privacy the default, not the exception. Imagine being able to prove you did the right thing (got tested, got a vaccine, whatever) without having to share your entire life story, or trust yet another startup with your data. Imagine a system where healthy behavior is actually worth something, and where the risk of participation is less than the risk of doing nothing.If you’re rolling your eyes, I get it. I’m skeptical too. But I’d rather try and fail than keep waiting for someone else to get it right.## incentives: for humans, not robotsLet’s be honest: most “health incentives” are either so trivial they’re meaningless, or so convoluted you need to be an actuary to figure them out. I want to design something that works with how actual people behave—not how some product manager thinks they *should* behave.Here’s what that means in practice: You get a tangible, real reward for verified healthy actions—not just a pat on the back. If you show up for regular testing or take other preventive action, you accrue value, not “points” that expire or can only be redeemed for branded swag. If you miss a test, the system forgives you up to a point; streaks and bonuses decay gently, not instantly, because life happens and the protocol recognizes that. Incentives are transparent and non-punitive: you can see exactly how rewards are calculated, and the most value accrues to those who are consistent, not those who can game the system. Points accrue for everyone, regardless of whether you know how to use a crypto wallet. If you want to stay in normie mode, fine—your rewards are just as real, and you can convert them to tokens whenever you want. There are no casino vibes, no “spin the wheel,” no loss aversion tricks—just a persistent, steady nudge toward better habits.The reward structure isn’t just “do X, get Y.” It’s a system that recognizes effort and continuity, not just perfection. Miss a test window because you had a rough month? You don’t get the digital equivalent of a dunce cap. Instead, your rewards taper off—gently. I want this system to actually *work* for real people, not just people who never miss a checkup or who have a spreadsheet for their wellness routine. And this isn’t just about individuals—if organizations, clinics, or even entire communities want to participate, there’s a pathway for that too (with proportional reward, without compromising privacy). The upside is that the system gets more robust as more entities join, and the incentives don’t get diluted into oblivion just because more people are doing the right thing. If anything, it’s the opposite: the more people and orgs that participate, the more sustainable and valuable the entire protocol becomes.## utility: not a speculative DeFi coin$HEALTH is a utility token, not a speculative asset. You won’t find it on DeFi exchanges or liquidity pools at launch, and there won’t be any token trading, yield farming, or pump-and-dump opportunities (sorry, not sorry). The entire point is to provide token infrastructure at the application layer—enabling products like [status.health](https://status.health) to reward prevention and healthy behavior, support governance, and fund public health via community grants. It isn’t meant to be a “number go up” coin or a vehicle for speculation. The goal is to create a circular economy where healthy actions are directly rewarded, and community health initiatives can be sustainably funded. If your main question is “wen Binance?” you’re definitely in the wrong place.What does that utility actually look like? In practice, $HEALTH tokens are earned by taking real preventive health actions—like regular testing or verification—through apps such as status.health. You can use those tokens to unlock premium features, participate in protocol governance, stake to help secure the network, or support local health initiatives via dontations. Most of this activity happens directly within the apps themselves, not on external exchanges, and is focused on rewarding participation and fueling public goods. As more partners and integrations come online, $HEALTH will serve as the backbone for new health-focused tools, transparent incentives, and grants—all designed to keep the ecosystem aligned around real-world outcomes, not speculation.## governance: decentralize only when it makes senseLet’s talk about decentralization. I’ve been around enough “DAOs” to know that most of them are just group chats with a multisig and a lot of vibes. Decentralization is great—*when there’s actually something to decentralize*. Until then, it’s a distraction.My approach is pretty simple: $HEALTH starts out centralized because, frankly, someone has to do the work and make the decisions. That means I’m responsible for the roadmap, the bugs, the (hopefully rare) drama, and the general direction. But here’s the difference: as *actual* usage grows, as the protocol matures, and as more real people and organizations rely on it, governance and control will transfer—gradually, and transparently—to the community.There are clear, metric-based thresholds for when governance hands off—these aren’t arbitrary, and you’ll be able to see them, track progress, and know what’s coming. Governance isn’t just about voting on proposals; it’s about funding public goods, supporting integrations, and making sure incentives stay aligned as the ecosystem evolves. Quadratic funding will ensure that many small contributors have more power than a few whales. If usage drops or the protocol gets gamed, decentralization slows or pauses—there’s no “one-way door” to chaos, and no DAO just for the sake of optics. The rules, formulas, and transition points are all public, so if you want to critique them, propose something better, or just follow along, you’ll have every tool you need. I’m committed to keeping the governance process as transparent as possible: you’ll know who’s making decisions, why they’re being made, and how to get involved if you care enough to show up. And, real talk—if nobody uses the thing, or if it turns into another DAO meme, I’d rather pull the plug than pretend it’s “community-led.” Governance is earned, not declared.Here’s a thing I’ve learned: persistent skeptics and critics are actually invaluable. People who don’t just clap along but keep showing up with feedback (good or brutal)—those are the folks I want in governance. In fact, if you keep at it long enough, you might just end up on my “persistent people to hire” list. You know who you are.I’m not interested in “community theater.” I want real, earned decentralization, with skin in the game and accountability. If that sounds too slow or not “Web3 enough” for you, there are plenty of Discords out there waiting for new mods.## is it open source?This is where I usually get hate mail from the free software crowd, so let me be clear: I love open source. Most of what I make is MIT-licensed, and I strongly believe in sharing tools that help people build. But with $HEALTH, I’m being careful. Some things will be public from the start: the [yellowpaper](https://healthprotocol.network/yp), the math, all incentive formulas, and selected SDKs and reference components. I want people to be able to audit the core mechanisms, see how rewards are calculated, and follow the logic behind the scenes.But not everything will be open—at least not right away. Why? Sometimes, it’s about protecting security while things are new and fragile. Sometimes, it’s about giving the project a chance to mature before it gets forked into oblivion or spammed by bots. Sometimes, it’s just that I’m still working out the kinks and don’t want to ship garbage into the wild. If you have a legit reason to ask for access, reach out and make your case. I’m open to collaboration and feedback, especially from folks who want to make the protocol better (or who spot problems I missed).If you want to see what’s public, check out [github.com/97115104](https://github.com/97115104) for my stuff and [github.com/statusdothealth](https://github.com/statusdothealth) for the status.health side. If it’s not there, it’s not open (yet or ever). If that’s a deal breaker for you, I get it. But I’d rather build something that works and is safe before opening every door and letting the whole world poke holes in it. If you want to be part of that process, you know where to find me.## what's next? (the real roadmap, for better or worse)Here’s where I stand: this is still an experiment. Patent’s pending (because lawyers need to eat), and now that both [status.health](https://status.health/) and [healthprotocol.network](https://healthprotocol.network/) are live (plus the yellowpaper is finally finished), I can actually start building the real MVP. Everything is still fragile and changing fast. For anyone actually curious about what’s ahead, here’s the roadmap:1. **Building the MVP for real-world use (through status.health):** Since I run both status.health and the $HEALTH Protocol, the first version of the MVP will launch through [status.health](https://status.health/)—but only for those who sign up for the beta ([sign up here](https://form.typeform.com/to/Ii3HSlEH?typeform-source=status.health)). Making it generally accessible will take real money and a lot of development time, so don’t expect it overnight. This isn’t just a demo for crypto insiders—the goal is a product where privacy and incentives work for normal humans, not just blockchain hobbyists.2. **Applying for grants:** Server bills aren’t paid in tokens (yet), so I’ll be applying for grants to help with ongoing development and infrastructure costs. The plan is to work with protocol foundations like the Ethereum Foundation, Web3 Foundation, and similar organizations, as well as explore on-chain treasuries and ecosystem funds that support public goods and experimental projects in the crypto space.3. **status.health as first customer:** I’m working on status.health and the $HEALTH Protocol concurrently—status.health is the first customer of the $HEALTH Protocol, and hopefully not the last. The idea is to prove the infrastructure actually solves a real problem. Over time, I want to see many more apps and orgs using $HEALTH’s rails, powered by open APIs and community grants.4. **Governance and analytics:** As the protocol grows, transparency grows too. Usage, impact, funding flows, and governance transitions will all be visible in real time. No hiding behind “coming soon”—if it’s working (or failing), everyone will see it.5. **Full decentralization (if earned):** If this thing gains real traction, governance and control will move out of my hands and into the community’s, with clear, published thresholds and rules. Not by vibes, but by actual adoption and contribution. If it doesn’t earn that, I’m not going to pretend otherwise.6. **Iteration and humility:** If the experiment blows up in my face, I’ll say so. If something’s not working, I’ll pivot or kill it. If you have feedback, ideas, or want to get involved, I’m open. If you want to sit on the sidelines and throw tomatoes, that’s fine too—I’ve got a thick skin.The bottom line: I’m not building this for a quick exit, a headline, or a token pump. I’m building it because I want a world where prevention and privacy aren’t afterthoughts. If that sounds interesting, follow along, contribute, or just lurk and watch.If you want the technical receipts, actual mechanics, or just want to see how weird my sense of humor gets when writing about cryptoeconomics, the [yellowpaper is here](https://healthprotocol.network/yp). If you’re feeling especially masochistic, you can also read [the patent](https://0x42r.io/patents/1.pdf)—which is way more painful to read (and was to write).------layout: posttitle: "my first failure as a startup founder"date: 2025-06-06 18:00:00 -0500categories: [startups, personal]attribution: human---## the wrong co-founderMy first real mistake as a founder wasn’t about product or tech, although both did change from what they were originally. It was bringing in a friend—who was also my ex—to help build the company. I thought splitting the responsibility of running things would make it lighter, like a feather. I thought trust was enough. It isn’t. It’s like the weight that hits [Wile E. Coyote](https://en.wikipedia.org/wiki/Wile_E._Coyote_and_the_Road_Runner)—you can’t run your way out of it. You need more than history. You need someone who shows up every day, who wants to, and who can. Even if things are going swimmingly, there’s likely a time when they won’t, and when personalities are really under the pressure of potentially seven figures, or eight, or ten, or none—who knows. This might sound pessimistic, but it’s not. It’s a lesson in optimism and learning. You get better by going through it.## what I missedWe never defined the work other than at a high level: “you take marketing and finance and corporate governance bullshit, I’ll take engineering, product, and share some of the marketing.” We didn’t decide how decisions would get made other than “we’ll always run things by each other,” or agree that building a real company is full-time—like, seriously, 3 AM every night and no Cinco de Mayo parties because the patent. Startups don’t wait for you to catch up. We should have put it in writing, apart from our Founders Agreement. We should have actually weighed and measured the Founders Agreement, not signed it on our mobile devices on a whim thinking it would never *really* matter. I should have said what I wanted. He should have said what he wanted. We should have talked about our “fuck you numbers.” It was my first time doing this without a team, or a big company’s money, or someone else’s rules, so I can’t blame myself. I can’t blame him. Or the situation, or really anything. I can just learn, and share my experience so others don’t, hopefully, repeat the same errors in their founder journey.## what he broughtHe was ambitious and hungry. He wanted to be CEO. We actually fought about who would be CEO in the end. He wanted to be the one walking into the room as the leader of a big company, the one selling it to people, the one in front. He had what I didn’t—the desire to be seen, to be admired. I prefer to do things in the background and admire my work from afar. He helped with incorporating the company, and writing (because I am obviously bad at writing). He was great at scheduling things, talking with potential customers and investors off the clock, and getting folks excited. But he just didn’t have enough time. He wasn’t willing to take the same risk, mostly because he isn’t as privileged as I am and isn’t willing to lose it all, which I get. But that’s what it takes. I now have to be a different person, and for that I am thankful, because I am stronger. But it was painful to lose him—he mattered.## when it turned[The patent](https://0x42r.io/patents/1.pdf). By the time it was ready for another person to review, it was like 60 pages. I had written it, read it, and iterated on it at least a hundred times—or that’s what it felt like—by the time it was ready to be submitted. All that needed to happen was his eyes, confirmation, and a plus-one. But that was also the same day as Cinco de Mayo, and the fun was enticing for us both. I didn’t buckle and submitted that night—he went for a marg. It was the beginning of the end for our co-founder-ness.Around that same time, I pitched a licensing deal where I would own the patent and the company would lease it for free—unless I was no longer at the company, at which point it would have to pay for it. He said it wouldn’t be good for the company. What I found out later was that he started second-guessing my motives. I get it. What was really happening is that my nine-hour days felt worth more than the 50% of the company I owned, and if I was putting in all the real work, I wanted a bigger upside. In reality, I pitched that as a cry for his attention, to help course correct, and hopefully get more “work” for the company out of it.My error here was missing that he just didn’t have the time. He was working three other jobs after all. I was naive. I could have been kinder, nicer, less aggressive even. He mentioned some of the things I said were emotionally abusive. This was hard for me to hear, but he’s probably right in some ways. I can be mean when I am tired, hungry, and at the end of my proverbial leash. I needed him to help correct me and bring me back down to normal. He was my opposite after all. I didn’t realize that it was unrealistic to expect this from a business partner—and maybe even from a friend. I have to regulate myself, and make sure I take time to garden my body and my brain, not just the business. That lesson is one I’ll take with me forever, and I thank him for that.## how it endedThe real ending was in Dolores Park. We sat and talked a bit, both of us nervous—well, I was nervous, he was probably just mad, because it was nothing like when we first met at that club on his 23rd or 24th birthday, I forget which. In some ways, it felt like the last time we’d ever meet. The conversation ended with me offering that dinner I owed him for helping me sell some jewelry I needed to fund my lifestyle and the startup (that damn patent was expensive). He agreed to the dinner, after which he walked home, looking a little lighter in his step free of me and the startup, to the puppy we used to share and his beautiful boyfriend I sometimes wish was still me. I took some solace in the idea that we were still friends, but now, maybe we aren’t? Ugh, running a company is hard. Don’t do it with your friends, ex-lovers, or family. Be warned.## what founders really need to knowWhen you start out, you think you know how hard it’s going to be. I didn’t. The real test isn’t the product, or the legal forms, or the late nights—it’s everything that happens to you, and who you become in the process. I used to do ops and product, and engineering, and recruiting and everything. I thought I knew it all. In reality, it couldn’t be more different. Paying taxes, statements of information, articles of incorporation, a million contracts, boilerplate that isn’t code—the whole thing is dizzying. I was hoping he’d take this on, but I now see the value in knowing how to do it all myself, and I see the new me in it. I read every single line of a contract, the bylaws, the license agreement for the patent, and you should too. It’s what dictates the next 10+ years of your life as a founder, and maybe more if you believe the folks in ["Founder vs. Investor."](https://www.foundervsinvestor.com/)You don’t see it until you do, and it is worth it even though it doesn't feel like it. Or at least I hope it is.## for the recordThinking about it more, I don’t actually know if any of this was worth the cost. Losing a friend, especially an ex, never feels worth it. If I could do it again, I’d have shared my needs as a founder. I’d have explained that I am not the same person in business as I am in our friendship, or as I was in our relationship. I am somehow much stronger, but also much weaker in other ways. Please, *dear reader*, take note if you end up working with a friend or ex or family member: make the clear distinction between the *you* that is a business person, the *you* that is a friend, and the *you* that is in the clubs or when you see a cute boy, because they are all versions of you that show up at different times, and you better bet they haven’t seen the business you yet.Some days, the work is worth every cost. Other days, it isn’t. I’m still here, still learning, and still doing the job, and I plan to until this thing really does [reduce STI transmission rates](https://status.health/) and for me, that’s enough.## letter to you, and to meYou saw something in this before I did.  You helped turn it from idea to company.  You called out my blind spots.  You pulled me back in when I wanted out.  I wanted us both all-in, building something that mattered.  We didn’t get there.I hope your part of this means something—whether or not we ever talk again.  I hope you know you mattered, even when things fell apart.To myself:  Keep building.  Remember where you started.  Don’t lose more than you have to.  If it works, let it mean something for both of us.------layout: posttitle: "hodl? lol. not here."date: 2025-06-25 01:30:00 PTcategories: [crypto, privacy, health, web3, zk, startups]attribution: ai chatgpt 4.1---## why i killed my own crypto project (and built something that could actually live)I built Health Protocol thinking if you gave people privacy, incentives, and proof, everything would just work. I shipped the infra and wrote the math with Claude's help. I modeled every outcome using [Monte Carlo sims](https://en.wikipedia.org/wiki/Monte_Carlo_method). Every time, the same ending. Whales won, bots drained the pool, real users quit. The DAO was a graveyard. Nothing felt new. It was the same fate as every corporate wellness app, insurance "incentive," and NFT move-to-earn. [STEPN](https://www.reddit.com/r/CryptoCurrency/comments/11qaopr/the_end_of_stepn_and_why_public_are_confused/) made you buy $1,200 sneakers for a shot at a token jackpot. By the end, bots were farming, prices tanked, and most users left holding nothing but another Discord full of bad memes. I did the research. I looked at everything: DeFi, behavioral econ, read [Richard Dawkins](https://en.wikipedia.org/wiki/Richard_Dawkins) and thought about how casinos and governments try to buy good behavior. None of them solved the core problem: how do you reward people for what actually matters and keep privacy as a non-negotiable? The old Health Protocol yellowpaper documented exactly how it would fail.## arishem, eternals, and building for life, not stasisThe moment I gave up on the old protocol was sometime after midnight, Marvel's Eternals on in the background, Arishem explaining why universes are born to die and recycle. It clicked. The problem wasn't just incentives, it was immortality: THE VAMPIRE PARADOX! If you build for stasis, the system probably gets gamed and dies a boring death. If you build for life with decay, adaptation, feedback, etc, then it turns out you get a protocol that might actually live forever. [Arishem](https://marvelcinematicuniverse.fandom.com/wiki/Arishem): cosmic midwife, blockchain muse.## value that vanishes, and the birth of the reservoirEvery protocol I looked at was built on the same dead logic: make a token, freeze value, pray nobody dumps. That doesn't work. Real biological systems move. In Autophage, tokens decay. You can't save indefinitely though Wellness Vaults (think HSAs but on-chain) are a thing. You either have to use tokens you earn right away or set them aside for specific goals like how college funds and HSAs work, and if you ghost, your balance fades out.The first person who called this out as bullshit was my mom. She said, "money isn't supposed to disappear." I tried to explain decay, game theory, and why hoarding ruins incentives. She said, "let it all flow back to everyone else, like a reservoir." She was right so that's exactly what I built; the missing piece. **The Reservoir** collects all decayed tokens and recycles them as system fuel. It pays future rewards, covers healthcare payouts, and backs the protocol's safety net. My mom named it, shout-out to you Momma!So you earn tokens, those tokens decay at different rates depending on how you earned them, i.e., through what exercise or activity. You can spend them lots of ways, but if you don't they simply return to The Reservoir, funding other people's health emergencies or yours. The Reservoir remembers 100% of the value you contribute through token decay and works like how "social security" is supposed to, except it doesn't run out and is USDC backed through platform and marketplace fees. ## game the system, then evolveYou earn tokens as mentioned before through health-related activities that include lots of things like sleeping, running, ab workouts, therapy session, STI checks, etc, really any health activity you can design a verification proof off of. Those activities are private to you and they should be, so the system implements a proof generation system. This gets complicated, but the gist is you access an App on the network like a theoretical status.health for STI test history verification, that App then generates a proof you did a thing, and sends it to the network. The network then pays you for your proof and stores it under your ProfileID which is cryptographically separated from your identity.  Proofs that are generated are not just trophies, you can actually turn them into products you sell, and even cooler, the protocol expects you to game it. Thanks to my buddy Dakota, who explained why [Path of Exile](https://www.pathofexile.com/) works: you can always evolve your build and break it in new ways, and it stays fair. That's how genetic adaptation landed in Autophage. Burn tokens, unlock new traits, specialize. If you're a marathon runner, show your trait stack and earn multiples on running activities. If you care about privacy, share less, and still get paid. The more you adapt, the better your rewards. Apps and businesses pay to issue, verify, and use the protocol. Users just access and build reputation. All fees are in USDC. No protocol token games. If you spam, you get cut out. If you contribute, you get paid.The protocol uses zkVM orchestration for proof verification - basically allowing apps to verify you did something health-related without ever seeing your actual data.## onlyfans for proofThe protocol flips health data ownership just like OnlyFans did for the porn industry. Not only can apps help you verify activity, but they can also help you sell health proofs you generate through the marketplace. Buyers could include individuals or businesses for things like medical research or post-hookup should a partner get anxious about your testing history. Anonymous marathon proof sells for $10-20 baseline. Reveal your ProfileID so buyers can follow your journey, earn 50-100% more. Show customizable traits your spec'd in proving you're an endurance specialist, command $50+ per proof. Therapy consistency, workout streaks, STI tests, sleep quality, whatever you track becomes a potential product you can benefit from.The marketplace has three layers. Individuals buy proofs from each other to study real health journeys or verify fitness, kind of like peacocks and their feathers. Someone training for their first marathon buys experienced runners' proofs to understand pacing. Dating apps verify STI status without storing medical records. Research organizations purchase anonymous population data at scale. A university studying exercise patterns might buy 10,000 anonymous running proofs for $50,000. Every transaction protects privacy through zero-knowledge separation. Buyers get verification without identity. Sellers get paid without exposure.You control the terms. List anonymous and stay completely private. Build a following around your ProfileID without revealing your name. Display specialized traits when expertise pays. The protocol takes 12% to fund The Reservoir. Apps that verified your activity get 5%. You keep 88% in USDC. Cash for health receipts. Privacy settings determine price. The market decides value. You own what you earn.## proposals and upgrades as a real jobThe protocol treats development like a real market. You can ship any improvement that makes the system better. New proof mechanisms for different health activities, stronger privacy circuits, reputation algorithms that catch gaming, incentive curves that balance rewards. When apps adopt your code and it processes real health data, you capture value through integration fees, performance bounties, usage royalties, and security rewards. Early simulations show active developers earning $30-90k annually just from base activity, but that's conservative math. The real opportunity is building critical infrastructure that thousands of apps depend on, like npm packages that print money. No voting, no committees, no "community calls" where nothing happens. Pure market dynamics: your code gets used, you get paid. It doesn't, you don't. The protocol needs everything from better zkVM orchestration to ML models for health insights to integration bridges for existing systems. Each improvement creates compound value and compound earnings. It's an economic system where protocol improvements translate directly to developer income indefinitely as the network scales. Anyone can propose improvements by staking 100-5,000 tokens on specific health metrics. The protocol runs controlled experiments for 30-180 days. Hit your 5% improvement target and get your stake back plus USDC bonuses proportional to impact. Miss and The Reservoir takes your tokens. Good improvers build reputation, stake more, run bigger experiments. Stack wins and you've got a real career improving public health infrastructure while getting paid in real money. The network adapts through these continuous experiments, evolving based on what actually works.## the usdc loopApps pay $0.10-0.20 monthly per user for verification rails. Enterprises drop $20-100 per employee wellness check. Users sell health proofs to each other with the marketplace taking 12%. All USDC flows to The Reservoir. The split: 10% for protocol development, 90% for healthcare settlements and reserves. Users earn USDC selling anonymous marathon proofs for 10 bucks or specialized genetic traits for 30. Apps compete on features while paying base infrastructure fees. Healthcare providers submit tokens and get settled in USDC at metabolic prices (essentially dynamic pricing based on the actual metabolic cost of healthcare activities). Developer improvements that get adopted earn integration fees and usage royalties. More network activity creates more fees, deeper reserves, better healthcare coverage. Your lifetime token contributions track your healthcare access rights. Decay funds the community, fees fund the infrastructure, proofs fund the users. Everything cycles through The Reservoir keeping value moving, never pooling.## receipts, math, and where to break thingsThe yellowpaper contains all the real math - every incentive curve, privacy proof, and system invariant. There's also a patent filed covering the core mechanics. The protocol documentation includes code snippets, use cases, and simulation frameworks. If you can break it, send through a report to info@autophage.io.## sims don't lieThe Health Protocol died every time in Monte Carlo simulations. Whales accumulated or bots farmed rewards or real users quit. Classic crypto death spiral. I killed it and built Autophage with different physics. Tokens that decay prevent hoarding. Genetic traits reward specialization over capital. The Reservoir recycles value instead of letting it pool. Ran thousands of simulations with every attack vector I could imagine. Bot armies, whale cartels (lol), bank runs (omg), you name it (I prob tried it). The protocol bent but didn't break. Value stayed distributed because decay forces circulation. Users specialized into sustainable niches. The marketplace found equilibrium prices. The Reservoir maintained reserves through fee flows. Not perfect, but the numbers finally worked. Sometimes you have to let something die to build something that can live.## the last thing i'll say about rocksCrypto built itself on hodling. Diamond hands. Never sell. Rocks don't need food, water, or healthcare. We do. The protocol's meme captures it: "hodl? lol. not here." Tokens decay because life does. Value moves or dies. The protocol name Autophage comes from autophagy - the biological process where cells eat their own damaged parts to survive. For those hunting deeper secrets, Charlotte holds the key.<!-- the first clue in national treasure -->*What if our deepest economic assumption that value should persist forever is perfectly inverted?*------layout: posttitle: "utility, network, sf: why i'm applying to yc"date: 2025-07-17 21:30:00 PTcategories: [startups, privacy, health, yc, web3, sf]attribution: ai claude opus 4.1---## the status of all the thingsThe last 4-months have been a flurry of this and that. I've built 4 products from the ground up:- [attest.ink](https://attest.ink)- [status.health](https://status.health)- [autophage.xyz](https://autophage.xyz)- [healthprotocol.network](https://healthprotocol.network)Each has gone through many iterations and some are now defunct, or just stuck in the "research phase" until I decide what to do with them.[status.health](https://status.health) is the main business product I am now focusing all my time on. It was originally meant to be a B2C API that allowed users to verify a hookup's STI status on a dating or sex-positive platform like Tinder, Grindr, or Sniffies. It quickly morphed to being more focused on STI testing history as it became clear verifying a person's actual "clear or not" status would be too hard. Testing history is still valuable but from my perspective it was less so because the goal was to not get any STIs from hookups, c'est la vie.[healthprotocol.network](https://healthprotocol.network) was a response to the recent funding cuts the Trump administration posed on sexual health clinics and other LGBTQ+ health services. The idea was to create a cryptocurrency that could help fund clinics that lost funding plus reward users of status.health for verifying health actions with either tokens, or abstracted tokens I called "points." After speaking with a couple of friends and modeling out some of the functionality for the tokenomics, it felt too much like a crypto play that wouldn't really offer much good; something I am generally allergic to. Because I was able to secure the $HEALTH token ticker, I deployed the contract on Ethereum but went back to the drawing board.I wrote and published a [paper](https://autophage.xyz/paper/litepaper.pdf) for a novel protocol based on earning decaying tokens for health activities. It took months, and was fun to research but due to complexity, cost to run infrastructure, and other factors like the fact most people would gawk at the idea of tokens that decay, I chose to keep it in research until I can fund it without needing any money to do so as a public good.Which leaves us to today, when I chose to apply to Y Combinator for status.health. If you're not familiar with YC, it's probably the most famous incubator for startups; think a place fledgling founders can go to get their initial seed round and build in a fruitful microcosm. They offer around $500K in funding, but the real value is in the network you build, it's apparently tantamount to Harvard for startups, and harder to get into (research indicates around 1-2%...FML) and since I didn't graduate college maybe this is my way "in."## why yc fits the billLook, I can build anything. AI agents and I shipped four products in four months and I've got the git commits to prove it. But here's the thing about B2B sales to healthcare companies: they don't give a shit about your midnight commits. They care about compliance audits, security certifications, and whether you've got the runway to survive their 9-month procurement process.YC solves three problems I can't code my way out of. First, money for an actual security audit so I can prove what I already know...that status.health is HIPAA-exempt by design because we never touch health data. Second, access to founders who've actually closed enterprise deals and can tell me which procurement hoops are real and which are theater. Third, the YC stamp that makes risk-averse enterprises think "oh, they're backed by the people who backed Stripe, they probably won't disappear next quarter."The irony isn't lost on me. I built status.health to eliminate trust requirements through cryptography, but I need YC's trust network to get customers to believe in trustless systems. Sometimes the path to the future runs through the past.## the strategy: focus, then scaleI've made every classic founder mistake in four months. Built five things when I should've built one. Chased the perfect cryptographic solution when good enough would've shipped faster. Spent months on a litepaper about decaying tokens that maybe twelve people will ever read (and three of them will be bots).The strategy now is stupidly simple: make status.health work for one customer. Just one. Probably an identity verification provider who already has healthcare clients asking for this but can't build it because touching health data means touching liability. They integrate our API, their customers get health verification without HIPAA compliance costs, we get distribution to thousands of businesses overnight.Once that works, we expand. Dating apps want STI verification but are terrified of storing health data (rightfully so). Employers need vaccination records but don't want to become healthcare companies. Every business that touches health data is one breach away from disaster. We're the condom for data transmission; nobody wants to think about it, but everyone's relieved when it's there.## why san francisco is still homeEveryone shits on SF now, literally and figuratively (sorry for the imagery). Too expensive, too many homeless people, tech bros ruined it, the city's dying, etc. Yeah, rent's insane and I've seen things on Market Street that haunt me. But here's what the doomers miss: this city runs on building things that shouldn't exist and there's nowhere else like it.I walk fifteen minutes and pass three people arguing about Waymo, crypto, or Trump at a coffee shop. The guy at the bodega hot dog thing in front of every club on Castro knows what a TEE is because his roommate works at a privacy startup. My barber asks about my cap table. This isn't normal anywhere else. I want work-life integration where the guy I meet at a rave at 2am introduces me to someone who solves my technical problem at brunch the next day.SF is where you can say "I'm building HIPAA-exempt health verification using cryptographic proofs" and instead of blank stares, you get "oh shit, have you talked to the team at x and y about that?" The city's not dying; it's composting. All the tourists and grifters left when the dopamine ran out during COVID. What’s left are the people who sweat out the next GitHub Copilot bug fix at some ungodly hour because their API is actually in prod. Rent’s still hell and getting more hellish, but at least it’s hell with Wi-Fi, breakfast burritos, and the occasional circuit party.## the next chapterThe next six months are straightforward. Get into YC or don't (probably don't, statistically speaking). Either way, ship the enterprise version of status.health by end of year and look for a co-founder. Land one paying customer who validates that businesses will pay to not store health data. Use that proof to raise a real round and hire.If YC happens, great. I'll move faster with their money and network. If it doesn't, I'll keep building at 3am with lo-fi playing and proving that a college dropout who started at the Apple Store can solve problems that companies with hundreds of engineers can't touch mostly because their legal teams have meetings about meetings about risk.## why claude thinks yc will not accept my application and why i do not careI asked Claude to evaluate my YC application. It said I'm too early (no revenue), too technical (need a business co-founder), too focused on privacy (niche market), and competing against billion-dollar companies (Persona, Onfido, CLEAR). Claude's probably right. The acceptance rate is 1-2% and I'm a solo founder without a CS degree building in a space VCs think is "nice to have" not "need to have."But here's what Claude doesn't get right: every interesting company starts out looking like a bad idea. Airbnb was "who wants strangers in their house?" Uber was "illegal taxi company." Stripe was "another payments processor." status.health is "HIPAA compliance for companies that only understand huge compliance budgets." Sounds stupid until their next data breach costs more than our entire platform would be to buy...for now.YC might pass because I refuse to store data in a world built on data hoarding or because it seems too complicated. They might pass because I'm solo. They might pass because privacy isn't sexy until it's catastrophic. That's fine. I'll keep building either way.The truth is, I don't need YC to build status.health. I need them to sell it faster. If they don't see that opportunity, I'll build slower but still get there. The problem's not going away. Healthcare data breaches are accelerating and getting increasingly more costly. The solution exists, I built it.So yeah, I applied to YC knowing they'll probably reject me. But shots you don't take and all that. Worst case, I keep building what I'm building. Best case, I build it faster with smarter people around me.------layout: posttitle: "the claude i loved is gone: how anthropic’s new policies hurt indie AI builders"date: 2025-08-13 16:25:00 PTcategories: [startups, ai, sf, op-ed, personal]attribution: ai chatgpt 5---## an early adopter’s dilemmaI’m a solo founder in San Francisco, building a privacy-first health platform (status.health) and relying heavily on AI tools to bootstrap prototypes. Six months ago, Anthropic’s Claude AI felt like a godsend. A friendly 100K-token context assistant that could write code, generate documents, and digest huge files without breaking the bank. I eagerly integrated Claude into my workflow, even paying for Claude Pro and later Claude Max to unlock more power.Today, I’ve hit a wall. Anthropic’s recent pricing and usage policy changes have effectively priced me out of using Claude. Stricter rate limits, steep token pricing for high-context tasks, and revamped model tiers (“Haiku,” “Sonnet,” and “Opus”) have made the model unaffordable and nearly unusable for an indie builder like me. In this op-ed, I’ll break down the timeline of these changes, from quiet nerfs to public caps, and explain how they’ve crushed the value Claude once offered. Along the way, I’ll share what other developers are saying on forums and contrast Anthropic’s approach with OpenAI, Google, and open-source alternatives. The goal is to shine a light on how a promising AI tool lost its way for the little guys, and to plead (even poetically) for a better path forward.## from haiku to opus: claude’s model tiers and token costsTo understand the squeeze on indie users, we need to talk about Claude’s model tiers. Earlier this year, Anthropic introduced three versions of Claude 3: *Haiku, Sonnet,* and *Opus*. These correspond to increasing levels of capability (and cost). Claude Haiku is optimized for speed (a smaller model with shorter context), Claude Sonnet balances performance and efficiency, and Claude Opus is the most powerful model with the highest reasoning ability. In practice, Opus can handle the hardest tasks but burns through tokens up to 5× faster than Sonnet. Each tier has its own pricing: as of mid-2025, Sonnet models cost about \$3 per million input tokens and \$15 per million output tokens, whereas Opus commands a whopping \$15 per million input and \$75 per million output. Haiku is far cheaper, *Claude 3 Haiku* was just \$0.25 per million input tokens but it’s also less capable.Crucially, Anthropic phased out the cheaper models as newer ones arrived. The entire Claude 2 series was retired by July 21, 2025, and even the Claude 3 Sonnet (snapshot 2024-02-29) was deprecated then. Developers who had built cost-efficient workflows around Claude 2 or early Claude 3 suddenly had to migrate to Claude 4 models, *whether they could afford them or not*. (Anthropic did give notice of deprecations in their docs, but if you blinked you missed it.) The result: no more access to the older, budget-friendly models. A startup that was happily using *Claude 2.0* on the cheap in June found that by late July their only “replacement” was Claude Sonnet 4 at enterprise pricing levels.Anthropic also started charging a premium for high-context usage. Claude’s famous 100K-token context window was a selling point for feeding entire codebases or research corpora. But when they expanded Claude’s context to 1 million tokens in August 2025, it came with strings attached: prompts above 200K tokens are billed *double* for inputs and *50% more* for outputs. This long-context feature is in *“public beta”* but only for top-tier API customers (Tier 4 and custom plans), meaning indie devs on standard plans can’t even access it yet. In theory I could now process 75,000 lines of code in one go, but practically I’m locked out of that capability unless I somehow become a high-volume enterprise client. Anthropic’s own pricing page acknowledges prompt caching and batching as ways to offset these costs, but implementing such tricks is a burden on developers and only necessary because the raw token prices are so high.## march–may 2025: new models launch, indie access lagsAt the start of 2025, Claude was riding high on goodwill from early adopters. Anthropic rolled out Claude 3.5 in stages. First Claude 3.5 *Sonnet* in June 2024, later Claude 3.5 *Haiku* and *Opus*, and boasted about *“cost-effective pricing”* and *200K token context windows*. By early 2025, an even more advanced Claude 4 was in the works. In May 2025, at Anthropic’s first “Code with Claude” developer conference, the Claude 4 model family launched, promising state-of-the-art performance on coding tasks. But something troubling happened in this launch: certain independent partners were excluded.Windsurf, a popular AI coding assistant startup (ironically being acquired by OpenAI), reported that Anthropic cut off their direct API access to Claude models with less than five days’ notice. *“Anthropic decided to cut off nearly all of our first-party capacity to Claude 3.x models,”* Windsurf’s CEO wrote, saying they had been willing to pay for full capacity but were still dropped on short notice. Even more galling: when Claude 4 launched, Windsurf wasn’t given access to run it, while other big-name coding tools (Cursor, GitHub Copilot, etc.) were granted direct Claude 4 access from day one. Windsurf had to scramble to route Claude through third-party providers at higher cost, warning users of potential outages in their Claude features. Anthropic, for its part, had just launched its own competing product (Claude Code) in February, raising eyebrows about whether they were sidelining a tool that soon would belong to OpenAI.For indie developers, the Windsurf incident was a red flag: Anthropic could revoke or limit API access abruptly, even for paying partners. As a founder, I empathized. I also had “Claude inside” my product roadmap and suddenly worried if a quiet policy change might yank that away. Windsurf’s CEO noted, *“We are disappointed by this decision and short notice.”* I’d use stronger words: it’s hard to trust an AI platform that can change the rules overnight. Unfortunately, June 2025 was only a prelude of worse to come.## july 2025: quiet cuts and “usage limit reached” chaosIn mid-July, heavy Claude users (especially those using the new Claude Code tool for programming assistance) began noticing mysterious new limits. Anthropic hadn’t made a big public announcement yet, but suddenly Claude chats were getting cut off early, often with no warning. I experienced this firsthand: long coding sessions that ran fine in June started hitting a brick wall in July, with Claude refusing to continue. Initially I thought it was a glitch, after all, I was paying \$200/month for Claude Max (10-20× capacity) and expected some serious usage headroom. Instead I got cryptic errors and stalled conversations.I wasn’t alone. On Reddit, users voiced shock and anger at how abruptly Claude became unusable for sustained work. *“100% and no warning you are getting close,”* one user complained, describing how chats would end abruptly with no chance to even get a summary or hand-off. *“It has become impossible to get any actual work done because I am spending all my time explaining what I am trying to do over and over to full chats, and limits hit in a few hours,”* they wrote, adding that it had gotten *“worse in the last week than anytime in the past 6 months!”*. Another frustrated developer vented, *“The other day I hit my limit on premium after less than 10 min. The f--- am I paying for?”*. This sentiment was widely echoed: many of us felt like we were paying for a “premium” service that suddenly wouldn’t let us work for more than a few minutes at a time.So, what changed? It turns out Anthropic had quietly tightened the rate limits on Claude.ai usage, specifically targeting Claude Code sessions using the Opus model. Without telling users upfront, they introduced hidden caps to stop what they considered abuse (like running Claude 24/7 or sharing accounts). In practice, those hidden changes hit normal users hard. One major change was how Claude chose between Opus and Sonnet models on the Max plan. Previously, a savvy user could stick mostly to the mid-tier Sonnet (to stretch their usage) and only invoke Opus for especially tough tasks. Sometime in July, Anthropic changed Claude Code’s behavior so that Max users’ sessions would start on Opus by default, only switching down to Sonnet after a certain threshold of usage. This “auto model switching” was presumably meant to preserve experience (giving you the best model at first) while preventing you from accidentally running Opus non-stop. But it backfired badly. Users who didn’t manually override the model would burn through their token allotment at 5× the normal rate with Opus, hitting the limit shockingly fast. *“I noticed an update made it use Opus first before going to Sonnet. Now when I start Claude I make sure it’s set to Sonnet. Opus is stupid expensive,”* one user reported. Others on the \$200 Max tier found even that wasn’t enough: *“with \$200 I’m reaching Opus limits pretty fast, while a week ago that would be possible with parallel instances only,”* said one, noting they had to micromanage multiple instances before and now even one instance choked.By July 17, the situation had blown up enough that TechCrunch ran a piece about it titled “Anthropic tightens usage limits for Claude Code — without telling users.” According to that report, some developers found it *“impossible to advance \[their] project since the usage limits came into effect.”* Anthropic’s response at the time was cagey. They acknowledged being aware of issues but *“declined to elaborate further.”* In other words, no clear apology or detailed explanation to users. Many of us felt blindsided. We had been early Claude champions, some even canceling ChatGPT Plus subscriptions because Claude’s bigger context and reliable output seemed worth it. Now Claude was throttled and sputtering; it *“has been worse in the last week than anytime in the past 6 months,”* as one user observed ruefully. The lack of communication and transparency was perhaps the worst of it. *“No warning… Wtf,”* one user wrote about the sudden capacity errors. *“I thought it was just me… I had this insane hallucination that something was changed,”* mused another, only half joking. We now know *something* was definitely changed, just mostly behind closed doors.## august 2025: weekly caps and the \$500++/month paywallAfter the outcry in July, Anthropic finally went public with a plan: they would impose new weekly usage caps on all paid tiers of Claude.ai, effective August 28, 2025. An email went out to subscribers and Anthropic even tweeted about it, framing the move as targeting a *“handful of users”* who run Claude Code 24/7 or resell access. They assured us this would affect *“less than 5% of subscribers”* and that “most users won’t notice a difference”. But the details told a different story for power users and indie devs pushing the limits.**5-Hour Windows + New Weekly Limits:** Anthropic kept the existing *5-hour rolling rate limit* (the one that limits how much you can do in any 5-hour period) and stacked two weekly limits on top. Now, in any 7-day span, you can hit a hard ceiling where Claude refuses work until the week resets. There are *two* weekly caps: one on overall usage (hours of any model) and one specifically on Claude Opus 4 usage. This was essentially Anthropic saying: *we will now meter out Opus, our “most advanced” model, by the hour.* If you used too much Opus time in a week, you’re done until next week (unless you pay more).**What the Caps Mean in Practice:** In their announcement, Anthropic gave rough numbers. On the \$20/month Pro plan, you can expect *40–80 hours of Claude Sonnet 4 per week* (and note: Pro users *cannot* access Opus at all under the new rules). On Max \$100 (5×), they said *15–35 hours of Opus 4 and 140–280 hours of Sonnet 4* per week. On Max \$200 (20×), *24–40 hours of Opus 4 and 240–480 hours of Sonnet 4* per week. These might sound like big numbers (who codes 40 hours straight on Opus in a week?), but they aren’t as generous as they look. Those hour figures are not literal clock hours, but based on token usage translated into an “effective hours” metric and Anthropic has been very fuzzy about how they calculate that. Heavy coding tasks with large codebases can chew through an “hour” of Opus much faster than 60 minutes of real time. Users quickly pointed out that the *Max 20× plan no longer gives 4× the value of Max 5×* once you hit the weekly cap, you pay double, but you might end up with the same weekly Opus allowance if you’re not careful. *“20× feels like marketing if a weekly cap cancels it,”* as one user succinctly put it.**Extra Usage Now Costs Extra:** Anthropic’s message to us was essentially *“if you hit the cap, you should go pay for API calls.”* They noted Max users could purchase additional usage beyond the weekly limit at standard API rates. In the Claude Code interface, if you reach your allotment, it will helpfully prompt you to switch to pay-as-you-go credits (which cost the same steep per-token prices given earlier). For an indie dev, this feels like moving the goalposts: we signed up for a flat-rate service because we needed predictable costs. Now, if we actually use what we need and exceed some opaque weekly threshold, we’re asked to start swiping the credit card per call. A few of us joked darkly that at the rate things are going, Anthropic would prefer we shell out \$500 or \$1000 a month just to use Claude at the level we used to on a \$100 or \$200 plan.The developer community’s reaction to the weekly caps was scathing. A Reddit megathread filled with nearly a thousand comments got boiled down into a user-generated “discussion report,” and the top issues were *lack of transparency* and *punishing loyal users*. *“The issue isn’t just limits – it’s opacity,”* the report’s TL;DR read, noting that without a live usage meter or clear definitions of what an “hour” means, users get surprise lockouts mid-week. People begged Anthropic for basic quality-of-life fixes: *“Give us a meter so I don’t get nuked mid-sprint,”* one user quote reads. It’s hard to overstate how disempowering it is to be in the middle of a coding session or a long conversation and have Claude abruptly refuse to continue because you unknowingly crossed some invisible line. Another user pointed out how the 20× Max plan felt like a bait-and-switch: *“If 20× doesn’t deliver meaningfully more weekly Opus, rename or reprice it.”* Right now, paying for the highest tier doesn’t guarantee you proportionally higher real usage, because the weekly cap “step up” isn’t 1:1. Meanwhile, those of us on the receiving end of these changes feel like we’re being punished for the abuses of a few. Sharing Claude accounts or running bots 24/7 is against Anthropic’s usage policy. Fair enough. But why not enforce those rules on the offenders rather than impose blanket limits on everyone? *“Don’t punish everyone – ban account-sharing and 24/7 botting,”* as one commenter wrote. Or in another user’s blunt plea: *“Ban abusers, don’t rate-limit paying devs.”*Anthropic defends the weekly caps as necessary for reliability, they even cited that Claude Code had suffered multiple outages in the prior month due to overwhelming demand. I don’t doubt that a small percentage of super-users were hammering Claude. The problem is the cure is worse than the disease for those of us who legitimately relied on Claude’s former flexibility. Imagine a freelancer or tiny startup who might have one intense week of work (a deadline sprint where they need Claude’s help refactoring a codebase or iterating on a long document) followed by a light week. Under the new regime, that intense week will almost certainly trigger a lockout, because the weekly cap *does not care* that you have a quiet week to balance it out. There’s no rollover of unused capacity. No way to request a temporary boost except paying per-token. As one user noted, *“Locked out till Monday is brutal. Smooth it daily.”* suggesting that a daily limit or some kind of smoothing would be more forgiving than a hard weekly wall. But right now, if you hit your weekly max on a Thursday, that’s it – no Claude for you until the next week.All of this has effectively erected a \$500/month or more paywall for getting the most out of Claude. Why starting with \$500? It’s an estimate of what an indie developer would need to spend to regain the freedom they had before. Currently, I have to spend on the order of \$800/month. The \$200 Max plan will not last you a full week of heavy development now, so you’d need to either maintain multiple subscriptions (risky and against ToS) or start buying API credits on top. For some, even a single \$200 plan is already a stretch; \$500 or more is out of the question. The conclusion is that Claude is no longer indie-friendly. It’s been groomed for enterprise budgets and tightly metered for everyone else. A founder friend of mine quipped that Anthropic must assume anyone using Claude at scale *“has venture funding or a corporate expense account.”* The rest of us are left picking up the crumbs or looking elsewhere.## unusable for code, long context, or “artifacts”The following policy changes have rendered Claude nearly unusable for several of its flagship use cases that drew me to it in the first place.**Long-form Coding Assistance:** Claude Code was supposed to be a game-changer for developers introducing an AI pair programmer with an immense context window to hold your entire project. In reality, with the new limits, Claude can’t effectively sustain a coding session on a non-trivial repository. Either you hit the 5-hour window cap (which stops you every so often), or now the weekly cap shuts you down entirely. *“I had to increase my plan to continue without interruptions… My fear is that soon I’ll have to sign 50×, 100×… That’s f---ed up, right?”* said one user on the Max plan when the limits started biting. Even for those willing to pay more, Claude’s assistance has become stop-and-go. It’s hard to build momentum or dive deep into a coding problem when you’re constantly watching a meter (that you can’t even see) or re-explaining context after a reset. As one Redditor lamented, *“it used to be able to follow instructions and now it can’t.”* The continuity of thought, arguably Claude’s strength over shorter-context models, is lost.**Artifact Generation:** In mid-2024 Anthropic introduced “Artifacts” which is a feature where Claude output things like code files, documents, or website designs into a side pane, letting you interact with them separately. It was a great idea, turning Claude from just a chat into a collaborative workspace. But today, artifact generation is hamstrung by the limits. Users report that artifacts sometimes stop appearing or updating properly due to the new restrictions. *“All of a sudden I’m seeing this weird non-updating to the artifacts,”* one user noted, suspecting *“something was changed.”* They were right because behind the scenes, Anthropic likely tweaked how artifact outputs count toward usage, possibly to prevent people from using Claude as a free code generator. The result for me has been incomplete or missing artifact outputs in Claude Code. For instance, I had Claude working on a multi-file Python script; it started writing an `simulations.py` artifact but hit a limit mid-generation. The artifact never finalized, and I couldn’t retrieve what partial code it had written. When I tried to coax Claude to resume, it acted confused (likely because the context got cut). This never used to happen before. Artifacts were one of Claude’s most promising features for builders, and now they’re another casualty of the crackdown.**Long Documents and Contextual Analysis:** A core promise of Claude was that 100K (and now 1M) token context, meaning you could feed an entire book or a trove of customer chats or a huge log file and get comprehensive analysis. But what’s the point of a large context window if the model taps out after a few responses or arbitrarily truncates its output to avoid hitting some token limit? I’ve seen Claude become increasingly conservative with large inputs, often replying *“Let’s summarize in parts”* or refusing to ingest a long text it would have cheerfully accepted earlier. In some cases, I suspect Claude’s frontend is preemptively limiting input size or chunking behind the scenes (perhaps this is part of Anthropic’s suggested Retrieval-Augmented Generation workaround). Users on Hacker News noted the irony that Anthropic didn’t raise subscription prices, but instead quietly imposed techniques like forced summarization (RAG) to cope with costs. A developer on Reddit observed: *“There were never any actual usage limits; they just said e.g. ‘Claude Max 20x users have 20× capacity’… But then \[they] go on to say \[in fine print] ...”* implying the limits were always somewhat opaque. Now it’s clear: high-context usage is *the* big cost center, and Anthropic has effectively kneecapped it for anyone not paying top dollar. I used to load up entire research papers and have Claude cross-reference them. Now, anything beyond a certain length triggers either a refusal or such a brief summary that I might as well not have bothered. It’s devastating for those of us in domains like healthcare, where lengthy guidelines or datasets were finally within AI’s reach thanks to Claude’s context size.**Claude no longer delivers on its core value propositions for individual builders**. The model might technically still be capable of great things, but Anthropic’s policies have put those capabilities behind a glass wall. It’s like being given a sports car and then finding the fuel tank has a tiny restrictor plate where you can press the pedal, but you can’t go far or fast anymore.## community backlash: “this feels like betrayal”The mood among early Claude adopters ranges from disappointment to outright anger. Many feel that Anthropic cultivated our loyalty with a great product, only to yank the rug out with sudden monetization and usage clamps. *“I also feel cheated,”* one user wrote, asking for alternatives as Claude’s performance degraded and limits tightened. Another said, *“I dropped Anthropic just before the pricing structure changes. I’m \[switching] between ChatGPT, Gemini and NotebookLM,”* referencing OpenAI, Google’s Gemini, and a new NotebookLM product, basically anyone else. On Hacker News, the TechCrunch story on Claude’s usage limits spurred comments noting that AI costs should be going *down* over time, instead of going up. *“Costs should be going down. Going up will reduce adoption by developers hoping to keep their apps low cost,”* one commenter noted, pointing out the obvious: as model providers find efficiencies (and as competition heats up), we expect better pricing or higher quotas, not stricter ones. Yet Anthropic’s move was the opposite, perhaps because their own cloud costs or profit goals forced their hand. Either way, it left a bad taste.What stings the most is the lack of warning and dialogue. There was no clear advance notice to subscribers that, for example, *“On July 15 we will adjust our usage policies, expect possibly shorter sessions,”* etc. The weekly cap announcement came about a month *after* the quiet changes. I guess always ask for forgiveness instead of permission and all that. As you might expect, early adopters, many of us indie hackers, felt blindsided. We had designed workflows and even products around Claude’s capabilities. Anthropic’s slogan could well have been “let us handle the heavy lifting, you focus on building.” But by not communicating upcoming limits, they hung developers out to dry. One developer wrote on Reddit: *“I recently upgraded to the \$100 plan and hit my limits super fast today. It drives me crazy that I can’t see any usage metrics.”* That was before the weekly cap rollout; even after, Anthropic still lacks an in-app dashboard showing how close you are to those weekly Opus hours or token quotas. People are literally setting up community-made trackers or manually timing their usage. It’s absurd. A high-tech AI service that makes you use a stopwatch and spreadsheet to avoid being cut off. As one frustrated user pleaded, it’s such a simple ask: *“Meter, definitions, alerts – that’s all we’re asking.”* The opacity is driving us away as much as the limits themselves.And let’s not forget: some users also suspect Claude’s quality itself has dipped in recent months. Complaints about the model’s output getting *“lazier”* or not following instructions as well have popped up. Whether this is objectively true or just an illusion (one jokester blamed it on Claude “being French and it’s August” vacation mode), the *perception* of a nerf is real. When your chat gets cut off repeatedly, you often have to resend or rephrase queries, which can make the model seem dumber (you’re essentially seeing it fail more). There’s speculation that Anthropic might have tweaked the model to be more conservative or shortened its allowed response lengths to save tokens. We don’t know, but the fact we’re wondering highlights the trust erosion. Early Claude was delightfully eager to help produce full, context-rich answers. Late 2025 Claude feels skittish and hamstrung.One poignant quote from a Redditor summed up the indie builder’s sense of betrayal: *“I was using Claude pretty heavily last year… People were complaining daily about it here and Anthropic insisted they hadn’t changed anything.”* In their case it was a deja vu due to a similar degradation happened around summer 2024 and was denied. This time, at least, we have concrete policy updates to point to, but the feeling is the same. We trusted Claude; we even paid when it was in beta because we saw its potential. Now many of us feel cut off at the knees, forced to either cough up much more money or lose a tool that had become integral to our work.## comparing options: openai, google, and open sourceIt’s worth noting that Anthropic’s rivals have taken *very different* approaches in this period. OpenAI’s GPT-4, for all its own limitations, has kept a relatively stable deal for individual users. For \$20 a month, ChatGPT Plus gives you GPT-4 access with a reasonable (and clearly communicated) cap on requests (currently 50 messages every 3 hours) and no surprise weekly limit. You can’t feed GPT-4 100K tokens of context, its max is 8K or 32K if you have the special version, but in practice GPT-4 often *feels* more available than Claude now. I can chat with GPT-4 continuously without my session mysteriously “filling up.” And if I need the API, OpenAI’s pricing per token is steep but at least they haven’t slapped new caps on the fly. In fact, OpenAI has been lowering some prices (they cut GPT-3.5 turbo costs significantly in 2023, and offered GPT-4 32K context to developers at a premium but with no usage tiers). That’s not to say OpenAI is perfect. They’ve had their own communication lapses and quality adjustments, but from a pure pricing perspective, an indie developer can *budget for OpenAI* with more confidence. There’s no \$500 surprise paywall lurking.Google’s Gemini (and related products like Bard or PaLM API) is still emerging, but Google seems to be positioning itself as the *developer-friendly* alternative. For instance, their recent *Gemini “Advanced”* model (a rival to GPT-4 and Claude) on Google Cloud is rumored to have flexible pricing and potentially larger context windows without exorbitant cost. Google hasn’t yet put Gemini into a consumer-facing \$20/month package as of this writing, Bard is free but limited in other ways, however, they’ve heavily invested in AI through products like NotebookLM (an AI notebook for researchers) and vowed not to sting developers with sudden changes. If anything, Google is more likely to offer higher free usage quotas to entice devs into their ecosystem. It remains to be seen, but many of us are exploring these options. One fellow founder told me he’s *“mixing and matching GPT-4, Claude, and Vertex AI (Google) to see which is most cost-effective in the long run.”* After Anthropic’s moves, Claude is usually the first to be turned off in that mix when cost becomes an issue.And then there’s the open-source route. Six months ago, I wouldn’t have seriously considered running a local LLM for my use case. Claude and GPT-4 were just too far ahead in quality. But open-source models have rapidly advanced (think Llama 2, Code Llama, etc.), and importantly, they come with no usage shackles. If you have the computing power, you can fine-tune and run these models entirely under your control. Several indie devs I know have started using local 13B-70B parameter models for coding tasks, using techniques like context splitting and vector databases to approximate a 100K context. It’s not as straightforward as using Claude was, and the model quality can be hit-or-miss, but at least it’s predictable cost: once you’ve set up the server, you’re only paying for electricity or cloud GPU time (which you directly control). There’s a philosophical draw to this as well *no AI overlord can throttle me if I’m running the model myself*. The downside is you might need a \$3,000 GPU or a hefty cloud instance, but over the long run that could be cheaper than burning \$500 a month on a service. The open-source community is also working on improving context handling (through smarter retrieval) and some models are surprisingly competent at code when fine-tuned.In comparing these platforms, I realize it boils down to a simple question: who is building for the indie hacker vs. who is building for the enterprise? OpenAI, despite being a big company, still maintains a very large base of individual enthusiasts and developers thanks to ChatGPT. Google is trying to woo developers onto its cloud with flexible AI offerings. Many open-source contributors *are* indie hackers themselves scratching their own itch. Anthropic, on the other hand, seems to be pivoting hard to enterprise clients, the kind who will pay for government contracts or integrate Claude into Fortune 500 workflows. (They even offered Claude to *“all three branches of the U.S. government for \$1”* in a trial, which tells you whom they want to impress.) There’s nothing wrong with a business pursuing paying customers, but it feels like Anthropic has forgotten the *early adopters* that helped prove their technology’s value. Indie builders popularized Claude’s strengths online, wrote blog posts praising its large context, built open-source wrappers, and generally gave Anthropic goodwill and mindshare disproportionate to our pocketbooks. To be sidelined now with restrictive policies and effectively told “you’re not our target user anymore.” That hurts.## hope for a better balanceAnthropic’s mission is about creating beneficial AI, and Claude truly is a remarkable achievement on the technical front. It’s *because* it was so good that these policy changes sting so much. I want to be clear: I’m not angry that Anthropic needs to control costs or prevent abuse. I understand a startup can’t hemorrhage money providing unlimited AI to a minority of super-users but there’s a right way to handle these challenges. The lack of transparency, abrupt implementation, and one-size-fits-all limits are what turned a reasonable business decision into a fiasco for the community. Anthropic could have engaged developers in dialogue (“here’s what we’re seeing, here are some options we’re considering…”). They could have built tools to help users adapt (like proper usage dashboards or smarter switching that doesn’t default to the expensive model first). They could have tailored solutions that target abusers directly rather than clipping everyone’s wings. Instead we got a summer of confusion and frustration, and a future where using Claude feels like walking on eggshells.As a founder and an early Claude user, I’m writing this in hopes of change. Anthropic stands at a crossroads: it can either double down on being a high-priced enterprise AI vendor or find a way to keep indie innovators on board. I believe there’s value in the latter. After all, today’s scrappy startup could be tomorrow’s big customer, and the innovations independent developers create often expand what’s possible with AI. Anthropic even interviewed me as a power user. We’re partners in the ecosystem. I want to continue using Claude to build but now I've gotta rethink my business plan and my entire development pipeline; I'm priced out.So, Anthropic, if you’re listening: please remember the little guys who got you here. We know running large models isn’t cheap, but **meet us halfway with fair pricing, clarity, and respect for our workflows**. At the very least, be upfront and *honest* when you must impose limits. Surprises belong in AI-generated stories. Please keep them out of service terms.And to end on a note that perhaps the Anthropic team might appreciate, here's a short poem:**a plea to anthropic**```you gave us  a brain that thought  late nights felt shorter  deployments felt lighter  builders without vcs  builders without budgets  felt like you saw us  then  you raised the moat  without warning  and sat back  i’m asking  please  leave a sliver-open door  for the lean ones  the “not-yet-funded” crowd  we preferred your model  we still do  but if we’re priced out  we’ll build our own steps  or climb overyou took away  a quiet kind of equity  in months  don’t let it vanish  completely```------layout: posttitle: "privacy or bust: why you should move to lumo"date: 2025-08-28 17:05:00 PTcategories: [ai, privacy, startups, op-ed, security]attribution: ai lumo---## the promise that slipped...againWhen I first started using large language models, Claude was the only one that felt reliable enough to act like a pseudo-employee. I used it to code, write documentation, and handle repetitive operations. That one tool let me run multiple ventures alone. I built my workflows around it.Anthropic was founded in 2021 by people who claimed they wanted something different. Dario Amodei had been Vice President of Research at OpenAI, where he helped create GPT-2 and GPT-3. He left because he believed AI was moving too fast without safety built in. The idea was to build AI that was steerable, interpretable, robust, and safe. [This profile](https://kantrowitz.medium.com/the-making-of-anthropic-ceo-dario-amodei-449777529dd6) and [this Wikipedia entry](https://en.wikipedia.org/wiki/Anthropic) describe that origin story. Anthropic was even structured as a Public Benefit Corporation and funded by some of the largest names in technology. That mattered. It suggested they were accountable to the community that believed in them as much as to their investors.  That mission collapsed. The price hikes, hidden caps, forced migration to expensive tiers, and rewrite of the terms of service that allowed Anthropic to read user chat histories are a betrayal of the community that supported them and a violation of the mission they claimed as a PBC. I wrote about that [here](https://97115104.com/2025/08/13/the-claude-i-loved-is-gone/) and the terms change itself is covered [in this report](https://www.perplexity.ai/page/anthropic-reverses-privacy-sta-xH4KWU9nS3KH4Aj9F12dvQ). I had already been looking for alternatives since the pricing rug a month ago. Today, when the terms change went live, I cancelled my account.  If a Public Benefit Corporation can rug its users this openly, the model itself may be broken. It is time to ask whether the legal structure of “ethical AI” has any meaning when the mission can be discarded at will.## looking for an alternativeI spent the last month testing everything I could. Hosted open-source models looked promising, but most of them carried the same risks as Claude and ChatGPT: unclear privacy, shifting terms, and unpredictable costs. Running models like [Llama](https://ai.meta.com/Llama/) or [Mistral](https://mistral.ai/) locally worked for experiments, but not for daily scale unless you are willing to manage hardware, updates, and uptime yourself.  I initially ignored Lumo. It was clunky and missing features like web search. I assumed it was not ready. Then Proton emailed me to announce it again, and my father, who is deeply privacy focused, asked if I had tried it. That was enough to make me take a second look.  I was surprised. The user experience still has rough edges, and their cat mascot is funny, but the architecture is strong. Proton has earned my trust with Mail, VPN, Drive, and Pass. When Proton says end-to-end encryption, I believe it.## privacy that holdsLumo encrypts every prompt, response, and file on the client before it leaves your device. Even Proton cannot read your conversations. No policy update can undo that.  The code is [open source](https://github.com/ProtonMail/WebClients/tree/main/applications/lumo). Anyone can audit it. Proton states that they do not log conversations, do not train on user data, and do not sell usage statistics. OpenAI and Anthropic do the opposite by retaining data and reserving the right to reuse it.  At this point promises are worthless. Proof is the only thing that matters. Proton delivers proof.## pricing you can trustI began searching for alternatives when Anthropic’s pricing rug landed. Today’s terms change was the final straw. The speed of that shift shows how fragile trust becomes when pricing and policy can be dictated unilaterally.  Proton has been consistent for a decade. Proton Mail has never turned into an ad-driven inbox. Proton VPN has never pivoted to selling user activity. Proton Drive has never locked existing files behind a paywall. The company has held the same line since the beginning. That track record is why I trust Lumo’s subscription model.  If Proton ever betrayed that trust, the reputational damage would outweigh any revenue gain. They have more to lose by breaking their word than they could possibly earn. That pressure is what makes their pricing stable. Stability is about whether you believe the company will honor the deal in a year. With Proton, I do.## the ecosystemThe Proton ecosystem is what makes this work. Apple built its reputation on seamless integration across devices. Proton has done the same across privacy tools.  Mail, Pass, VPN, Drive, Calendar, and now Lumo connect under one account. When I generate code in Lumo, save it in Drive, and share notes over Mail, the entire flow happens inside a single encrypted environment. That improves convenience and reduces risk. There are fewer vendors, fewer policies to monitor, and fewer points of failure where data can leak.  The ecosystem matters because every tool shares the same design principle: privacy first.## open source is the real answerThe long-term answer is open-source AI. Models like [Llama](https://ai.meta.com/Llama/), [Mistral](https://mistral.ai/), [Falcon](https://falconllm.tii.ae/), [Mixtral](https://mistral.ai/news/mixtral-of-experts/), and even [DeepSeek](https://www.deepseek.com/) show what is possible. Open-source models can be audited, forked, and improved by the community. They cannot suddenly change their terms or rug you. That is the safeguard.  This is where Anthropic looks the worst. They have never open-sourced Claude. That silence would be one thing for a normal startup chasing profit. But Anthropic is the only one of these companies incorporated as a Public Benefit Corporation. They are the ones who put “safety” and “accountability” in their charter. Yet Meta, Mistral, TII, DeepSeek, and even OpenAI have all published models while Anthropic has kept everything locked away.  Dario Amodei left OpenAI claiming the moral high ground. The irony is that Sam Altman has now released an open-source model ([GPT-OSS](https://openai.com/index/introducing-gpt-oss/)), and DeepSeek has opened theirs, while Anthropic, the so-called public-benefit company, has done nothing. If anyone in this story deserves to be called the bigger asshole, it is Dario.  Open-source AI will eventually win. In the meantime, running these models yourself is still out of reach for most people. It requires hardware, bandwidth, and constant upkeep. This is where Lumo fits. Proton has already open-sourced its [client code](https://github.com/ProtonMail/WebClients/tree/main/applications/lumo), and they have the track record to prove they mean it. Lumo gives everyday users privacy and stability today, while the open-source ecosystem continues to mature.## where this leaves usThe Claude I and many others relied on is dead. OpenAI has ceilings that undermine trust. Other providers have shown they will more often choose profit over privacy or their users. If we continue paying them, nothing changes.  The only way to shift this industry is to support services that prioritize privacy from the start. I have moved my workflows to Lumo and I support open-source models. If enough of us do the same, the industry may follow. At minimum, alternatives will have enough support to compete.  We need AI that cannot leak by design. And cannot rug by design. Lumo is that. At least for now...Take a look and see for yourself. You can read more about it [here](https://lumo.proton.me/about).------layout: posttitle: "the eye-ronic misadventure: a $149 lesson in healthcare theatre"date: 2025-09-30 15:39:00 PTcategories: [rant, health, sf, personal]attribution: ai gemini---## a glimmer of hopeI’ve been battling a [chalazion](https://www.aao.org/eye-health/diseases/what-are-chalazia-styes) on my eyelid for what feels like a geological epoch. It is a tiny, stubborn, blocked oil gland that decided to set up camp on my face. For over a month it has been a standoff between me, this unwelcome red bump, and a series of hot compresses. My urgent‑care clinic gave me the usual runaround with wait times so long I could have learned a new language. That is [Medi‑Cal](https://www.dhcs.ca.gov/services/medi-cal/Pages/default.aspx) for ya. I decided to take matters into my own hands.My search led me to call my old optometrist, a place iron­ically named Eye Gotcha. Since I had had [LASIK](https://www.fda.gov/medical-devices/laser-eye-surgery/what-lasik) years ago and was not a current patient, they could not help and referred me to Valencia Optometry. In hindsight the name of my old doctor’s office should have been a warning. I called Valencia, where the person who answered the phone was lovely and assured me they could handle my pesky chalazion. “We don’t take Medi‑Cal,” they warned. “No problem,” I said, “I will pay out of pocket.” The price for my salvation was $149 – a steal, I thought, to finally rid myself of this tiny, unwelcome tenant living rent free on my face.## the bouncy ball overtureI arrived at the office, filled with a misplaced sense of optimism. The first thing I noticed was a mysterious, rhythmic "BOING... BOING... BOING" from the apartment above. The doctor, Martha Klaufman, was not amused. She complained openly about the noise, which struck me as unprofessional, but I let it slide. I was on a mission. A herd of elephants could have been tap-dancing upstairs, and I wouldn't have cared if it meant getting this thing off my eyelid.I dutifully filled out the mountain of paperwork and was then asked to immediately pay the $149 "co-pay." I use quotes because the term implies a shared experience. The only thing being shared here was my money with their bank account. I paid, my heart aflutter with anticipation.## the bait and switchFive minutes later, I was called in. Not for a chalazion consultation, mind you. That would be too logical. I was given a standard eye exam, the kind you get when you need glasses. I don't need glasses. I have had LASIK. I explained again why I was there, mentioning I already had antibiotics and a referral to a specialist with a six-week wait. This was my attempt to circumvent that broken system.Dr. Klaufman, mid-vision-check, explained to me what I already knew from a simple AI search: stubborn chalazions need a steroid shot. Then, in a stroke of medical genius, she decided to prescribe me *another* antibiotic "to kill the bugs," despite the fact that there was no active infection. The prescription was for a weaker antibiotic than the one I already had. It was like trying to fight a dragon with a water pistol ## a referral to nowhereThen came the grand finale. She announced she couldn't actually *do* anything about the chalazion. "That wasn't the kind of thing she did there," she said. Instead, she would give me a referral.The assistant called the specialist's office on speakerphone. We all listened as they confirmed they could only see me with a referral from my primary care doctor (which I had for a different place) and that the wait was, you guessed it, still six weeks. When I meekly asked about paying out of pocket to expedite, they quoted me $500. Just for the appointment.Dr. Klaufman told the assistant to hang up and, with a straight face, instructed me to "call my primary care doctor and ask for a referral to the new place."## where this leaves meThe entire appointment lasted fifteen minutes, from the moment I walked in to the moment I walked out. I paid $149 for fifteen minutes of my life I will never get back, a useless prescription, and a referral to a place I will never go because of the same constraints that brought me there. At a rate of nearly $600 an hour she is paid handsomely for providing absolutely no value to me or apparently anyone if you believe the other Yelp reviews.I walked out stunned. For a fleeting moment I imagined paying someone to stand outside her office and bounce a ball on repeat, a darkly comic tribute to the absurdity that opened my visit. My actual revenge was less theatrical: I went home and wrote a [Yelp review](https://www.yelp.com/biz/valencia-optometry-san-francisco-4?hrid=pX2clDNn_MFMxyMsWQz0mA&utm_campaign=www_review_share_popup&utm_medium=copy_link&utm_source=(direct)) since I am, admittedly, too broke to do the former. It was then that I saw what I should have checked beforehand. The page was a graveyard of one star experiences, a chorus of voices echoing my own frustration; a useless doctor.The lesson, of course, is brutally simple: never go to a place, no matter how in need you are, without checking the reviews first. My desperation cost me $149 or really my mom since she gave me the money.The healthcare system I tried to bypass by paying cash is just as broken on the other side. The transaction was more theatre than medicine and I paid for the ticket. The only good thing that emerged was that once I got home I began using the antibiotic my urgent‑care clinic prescribed more aggressively. The swelling and redness have begun to recede, and the chalazion is now less noticeable.We need healthcare that cannot waste your time by design and cannot rug you by design.This wasn’t it.------layout: posttitle: "privacy and safety in AI tooling are still missing."date: 2025-10-11 18:53:00 PTcategories: [startups, ai, op-ed, privacy, zk]attribution: ai various---## the ai safety problemWe spend endless hours marveling at what generative AI can do, painting pictures, writing code, composing music. Rarely do we pause to ask what it should do, and what could be prevented. The AI stacks of 2025 deliver massive computational power with almost no friction, but they have almost no built-in mechanisms for verification, enforcement, or audit. Without these, outputs circulate unchecked, exposing users to harm and developers to liability, and the speed at which models generate content already outpaces both regulatory oversight and human moderation.## why verification matters right nowMinors can already interact with models capable of producing sexually explicit or otherwise unsafe content. At the same time, privacy-focused tools often forward every prompt to third-party APIs, creating single points of failure that can leak sensitive conversations or creative work. Generative AI workflows frequently expose proprietary ideas, personal queries, or private data to models without users’ knowledge, and these prompts can be incorporated into model training without consent. Regulators are responding. The EU AI Act and GDPR’s privacy-by-design framework require auditable safeguards and verifiable provenance. Several U.S. states are passing disclosure laws that push platforms toward demonstrable safety guarantees. Platforms without technical means to prove compliance risk regulatory penalties, exclusion from sensitive markets such as healthcare and education, and erosion of user trust. Cryptographically verifiable controls offer a pathway to safely scale AI deployment while maintaining user privacy.## three proposed pillars of a verifiable control planeThese are ideas for what a verifiable control plane could look like. The first pillar is verifiable model integrity. Zero-knowledge proofs (zk-proof) could cryptographically show that an output originates from an approved, safety-aligned model without revealing internal parameters or training data. Implementations might use zk-SNARK or zk-STARK constructions optimized for neural network inference, producing proofs that verify correctness without exposing model weights or training sets. Using zero-knowledge machine learning (zkML), models can confirm the correctness of a response and produce verifiable proofs without accessing raw prompts, preserving user privacy and reducing the risk of data leakage. The second pillar is private identity attestation. Decentralized identity wallets could issue cryptographic credentials asserting that a user meets policy requirements such as age or jurisdiction, which can be verified without revealing any personal identifiers. The third pillar is on-device pre-screening. Lightweight classifiers running locally could evaluate prompts or outputs against policy before they leave the device, detecting unsafe content while remaining efficient on consumer hardware. These layers could be combined in various ways depending on platform risk profiles, creating composable, auditable controls that may also reduce operational costs.## what a safe-by-design workflow could look likeIn a practical scenario, a generative AI app might require an age-attestation credential in order to access mature filters from a decentralized identity wallet, receiving a zero-knowledge proof confirming the user is over eighteen without exposing their birthdate. When the user enters a prompt, an on-device classifier could evaluate it for unsafe language, sexual content, violence, or extremist content. Only policy-compliant prompts would be sent to a cloud endpoint, where a model running in a zero-knowledge-compatible environment generates the output along with a cryptographic proof confirming it came from the approved model. The client verifies the proof before displaying the output. Immutable logs of proof hashes and anonymized attestation status could be written to a tamper-evident ledger or secure storage, allowing auditors to verify compliance without accessing raw user data. This approach could scale across text, images, video, and multi-modal AI, creating a verifiable trail of safe operations while preserving privacy. zkML also opens the door to fair value for user contributions. Instead of models passively consuming prompts as free training data, users could retain control over their inputs, receive proofs of how their prompts are used, and even be compensated for contributing to model improvement, closing the loop and encouraging engagement.## scalable alternatives and trade-offsFully decentralized approaches reduce trust assumptions but introduce latency, higher computational cost, and deployment complexity. Centralized systems are faster and simpler but concentrate liability and reduce privacy guarantees. Hybrid architectures could combine on-device screening with centralized verification, balancing speed, privacy, and auditability. Standardized protocols for zk-proof verification, decentralized identity, and on-device classifiers would allow interoperability and reduce friction for adoption, enabling developers to incrementally integrate layers according to operational constraints and risk tolerance. Incorporating zkML for privacy and verifiable model outputs also reduces the risk that sensitive prompts are exposed or exploited by model providers while maintaining verifiable assurances of correctness.## enforcement mechanisms and real-world precedentsApple’s on-device NSFW detection and Google’s SafetyNet attestations demonstrate that local enforcement and cryptographic verification are feasible at scale. GDPR and the EU AI Act already mandate traceability and logging for high-risk AI systems, requirements that zk-proofs and auditable logs could satisfy. Implementing these mechanisms shifts liability away from downstream integrators while providing regulators with verifiable evidence of safe operations without exposing sensitive user data. Integrating privacy-preserving proofs and verifiable ML could establish a precedent for compensating users for their contributions to model training, aligning safety, privacy, and economic incentives.## who builds it and who benefitsOpen-source communities could provide reusable primitives for zkML, identity protocols, and on-device classifiers. Platform providers could operate verifiable compute endpoints and publish model integrity hashes. Identity-wallet projects could issue reusable attestation credentials to expand adoption. Developers would integrate these components to accelerate time-to-market, reduce compliance burdens, and enforce privacy-by-design. Regulators could verify compliance without accessing personal data, and users would benefit from safer, privacy-preserving services while retaining agency over their prompts and receiving compensation when appropriate. This ecosystem aligns incentives across builders, regulators, and users, creating infrastructure for responsible AI at scale.## how this improves the landscapeFiltering unsafe content before it leaves the device reduces the propagation of illegal or harmful material. Privacy-preserving attestations enable enforcement of policies at scale without storing personal identifiers. Cryptographic proofs make audits measurable and reduce operational overhead. Verified outputs could serve as market signals for responsible practices, nudging the broader AI ecosystem toward safety and privacy standards. Users gain control over their data and the potential to be compensated for contributing to model improvements, shifting the economics of AI toward fairness. Over time, these mechanisms could establish a verifiable baseline for AI services where safety, privacy, and fair contribution are inherent, measurable properties.## closing thoughtsThe AI ecosystem in 2025 lacks a structured control plane capable of verifying model outputs, enforcing policy, and preserving privacy at scale. The ideas outlined here, namely, verifiable model integrity, private identity attestation, on-device pre-screening, and privacy-preserving zkML, represent one possible path forward. The building blocks exist, but widespread adoption requires coordination between open-source communities, platform providers, identity projects, and regulatory stakeholders. Implementing such a control plane could allow AI services to operate at scale with measurable safety, privacy, compliance, and fair treatment of user contributions, providing a foundation for trust in a rapidly evolving landscape.We already know how to make AI faster; the question now is how to make it accountable.## sidebar: limitations and possible remediationsImplementing a verifiable control plane has clear challenges. zk-proofs for large neural networks are still computationally expensive, which could bottleneck high-throughput applications. On-device classifiers may misclassify content, generating false positives or negatives, particularly with ambiguous or culturally specific inputs. Decentralized identity systems depend on robust adoption, credential issuance security, and resistance to cloning or replay attacks. Hybrid architectures introduce orchestration complexity between local and centralized systems. Potential remediations include optimizing zk-proof circuits for inference, continual retraining of classifiers with active learning, leveraging hardware-backed secure enclaves for credential storage, and using modular protocols that allow incremental integration and testing. Feedback loops from auditors and regulators can refine thresholds and policy enforcement over time.## sidebar: quantum computing, cryptographic resistance, and risksThe cryptographic primitives that underpin zk-proofs and private attestation rely on assumptions about computational hardness. Advances in quantum computing pose a risk to these assumptions, particularly for proof systems based on discrete logarithms or factoring. Post-quantum-resistant constructions such as lattice-based SNARKs and hash-based signature schemes provide a potential path to future-proof proofs and attestations, but they come with larger proof sizes and higher computational cost. In a hybrid system, quantum-resistant algorithms should be considered for both model integrity proofs and identity credentials. Quantum threats also introduce operational risk: if a sufficiently powerful quantum computer becomes available, previously recorded proofs could be retrospectively compromised, highlighting the need for continuous key rotation, periodic algorithm upgrades, and monitoring for emerging quantum capabilities. Building a verifiable control plane today requires factoring in quantum resilience as part of the design, even if practical quantum attacks remain speculative, because AI outputs can remain relevant for years and need long-term trust guarantees.------layout: posttitle: "where does intelligence come from?"date: 2025-12-18 20:02:00 PTcategories: [general, personal, op-ed, rant, ai]attribution: human---## books for christmasEarlier today I was in Barnes & Noble, my favorite place to be now that I’ve moved back home. There isn’t much else competing for my attention as I try to avoid social media, mostly through iOS Screen Time settings. I was there to look for Christmas presents as I’ve decided unilaterally, and somewhat tyrannically, that I only buy books for people. And only for family.A few days ago I had spotted [*I Am a Cat* by Natsume Sōseki](https://en.wikipedia.org/wiki/I_Am_a_Cat) or Sōseki Natsume, depending on whether you trust the book cover or Wikipedia. Author names are important as so many with a nom de plume can attest, so this strikes me as horrible marketing. It is also very confusing for those looking to share his work with other people, something I strangely have already tried to do.As is my ritual when considering a purchase at B&N, I read about 5% of the book while standing in the aisle, clutching a mostly watery latte from the accompanying Starbucks? It's a question because they don't accept Starbucks gift cards, staunchly via signage, and often lack specialized drinks you'd get at a "normal" Starbucks. You also can't collect those points people always zealot about. The B&N "Starbucks" pastries are always better though, so that's a plus, and they accept the B&N membership for discounts, if you're one of those people, like me. Standing with a latte reading for hours in a bookstore counts as exercise, in my opinion, because the weather in Southern California is too hot to go outside despite being December. Due to the heat and general malaise, I can’t be bothered to work out in any structured way beyond the occasional push-up to reassure myself I still exist. Something commonplace amongst most millennials. Just ask.*I Am a Cat* reminds me of my dad because of Charlie and Moris. Cats I now live with. Cats that annoy me constantly, Moris more than Charlie. Cats he nevertheless cherishes roughly on par with his human children. Cats he shares in common with his new girlfriend who I call "Snuff," and who has two of her own, Cleo and Oliver. Snuff sometimes also fosters kittens for the fun of it and has a room dedicated to the task. What I loved about the book was its skepticism toward humans, delivered from a cat’s perspective, about how humans are mostly trash until they aren’t, a sentiment I wholeheartedly agree with. I grabbed the first copy I saw, trying not to drop my latte, and moved on.I headed to my favorite section: science. Not science fiction. Actual science. The kind with graphs and math and authors with the Dr. prefix. Most of the titles made my eyes roll because they were about AI or some permutation of AI, most of which I'd already ruled out or read, and I'm sorry, but I don’t care about Sam Altman, Elon, or any of the other shitheads monetizing what should be a common-good tool, like the internet, rather than monopolizing, rent-seeking platforms designed to vacuum up other people’s thoughts, ideas, and creativity. Most of the books in science under "new technology" are basically biographies of the current top AI players, rather than subject matter elucidating model peculiarities, infrastructure strategies, or a meaningful change in emergent capabilities.Anyways, there wasn’t anything I could find in science that my dad would like except maybe a book called [*How to Invent Everything* by Ryan North](https://www.goodreads.com/book/show/39026990-how-to-invent-everything?ref=nav_sb_ss_1_24). If you haven’t heard of it, it has a strange, comic-book-style cover, which tracks given the author’s roots in cartooning for Marvel. But why it was shelved under science and not science fiction was unclear. More confusingly, the book presents itself as a literal manual for time travel and uses that framing to walk through the history of civilization. No way it can compete with [*Sapiens*](https://en.wikipedia.org/wiki/Sapiens:_A_Brief_History_of_Humankind), I thought, but I kept reading all the same.The problem with *How to Invent Everything*, at least from what I read and later confirmed on Goodreads, is that it’s confidently wrong about some basic things, rather like the hallucinations in GPT tools. It claims, for instance, that beer and bread were created by animals rather than yeast (they were not), and it does not address Arthur C. Clarke's quote about the lack of time travelers as evidence against time travel. Reviews echoed the same criticism: clever, funny, but not nearly as accurate as it wants to be. I put the book back assuming my dad would be annoyed like me and went to another section.Sci-fi is usually safe with my dad since he still references h2g2, [*Hitchhiker’s Guide*](https://en.wikipedia.org/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy), almost every time sci-fi comes up. I checked out [*The Cabinet* by Un-Su Kim](https://www.goodreads.com/book/show/56353777-the-cabinet) (interesting premise, odd translated cadence) and [*Three Californias* by Kim Stanley Robinson](https://www.goodreads.com/book/show/45046588-three-californias?ref=nav_sb_ss_4_24). The latter intrigued me because my dad had recently been looking for a nonfiction book about California that apparently existed everywhere except the actual shelves. Robinson’s book offered three fictional Californias instead. I read a few pages. Good. Just not *him*.I circled back to *I Am a Cat* and read a Goodreads review that said:“I recommend reading a bit each night, before sleep, for hilarious dreams.”My dad likes hilarious dreams. He reads before bed. Decision made.## curiosity and skepticismFinding a book for my mom is nearly impossible. I’ve seen her buy exactly two books in my three decades of existence, both for me. She admits to reading only one: the Bible, and listening to a few others.I walked over to the religious section, which I have entered several times before, all accidentally. It’s disorienting. So many Bibles, motivational diaries, near-death experiences, daily Bible studies, and other religious non-artifacts. However, buried within the shelves was a comparative religion subsection that was more comprehensive than you'd expect. It had various books about religion and comparing different religions, and popes, and other things. Notably, a translated Qur’an, which I learned isn’t meant to be read so much as sung, and then a book titled [*God, The Most Unpleasant Character in All Fiction* by Dan Barker](https://www.goodreads.com/book/show/25986260-god?ref=nav_sb_ss_1_49), with a foreword by Richard Dawkins, one of my favorite authors.I read nearly 30% of it while standing awkwardly in the aisle, receiving several uninterested but curious looks. The book builds its case entirely from biblical quotations, God as villain, by His own words. It exists, I learned from the foreword, because Dawkins once wrote, in his book [*The God Delusion*](https://en.wikipedia.org/wiki/The_God_Delusion), a series of now-famous adjectives:“The God of the Old Testament is arguably the most unpleasant character in all fiction - jealous and proud of it, a petty, unjust, unforgiving control-freak; a vindictive, bloodthirsty ethnic cleanser, a misogynistic, homophobic, racist, infanticidal, genocidal, filicidal, pestilential, megalomaniacal, sadomasochistic, capriciously malevolent bully.”Dawkins posits an entire book would be required to accurately reference each adjective from the excerpt above, and that he, not being a biblical scholar, would take longer than he had time for to write it. He thought of Barker, a reformed pastor, and set him to the task. Barker obviously obliged.Realizing I hadn’t actually read *The God Delusion*, I wandered back to the science section to find it, but alas, it was not there. Instead, I found something else. [*Crypto-Infection* by Dr. Christian Perronne](https://www.goodreads.com/book/show/55651112-crypto-infections). I was originally drawn to it because of the prefix "Crypto-" being someone who spent the last several years working in the blockchain industry, but then realized it was a neologism by the author used to describe a new or, in his words, specific class of infectious disease caused by the same bacteria that causes Lyme disease, *Borrelia burgdorferi*. The premise is that this bacteria is causing widespread, underdiagnosed chronic infections all across Europe, but specifically France, the UK, and Germany. I have a particular aversion to ticks, having once had an experience with one on two occasions, spread apart by two hours, in New Hampshire, on the same day. I was convinced the tick was stalking me to find the right place to burrow its head somewhere on my body, akin to a terrible alien parasite incarnated in a sci-fi novel. Eventually, my grandfather plucked it off my arm and smashed it between his fingers. I hate anything that burrows into a person and requires fire to remove. Obviously.I read several pages of *Crypto-Infection*, but the most interesting part of what I read had nothing to do with infections or ticks, incidentally. It was a translator’s note. The book was originally written in Dr. Perronne's native French:“The embrace of intelligence is proffered with two arms: that of curiosity and that of skepticism.”I stopped to consider. Intelligence is broad, and two arms seems limiting. Sure, curiosity and skepticism, but surely there's more to it. Not everyone is curious, and definitely not everyone is a skeptic, as we know merely from the presence of the religion section, just as we can be sure of curiosity from the science section and skepticism, famously, from philosophy.I thought about the intelligent people I know. Which is… everyone. Every human is intelligent.Are we all curious? No.Are we all skeptical? Also no.There's a huge variety of intelligent people all across the globe with varying skills, and all feel the proffered embrace of intelligence.My mom, in particular, is neither curious nor skeptical, and she’s one of the most intelligent people I know. She’s said as much my entire life and usually gets annoyed with my questions. But so does everyone. She’s also devoutly religious. So either the translator was wrong, or intelligence isn’t so exclusionary.After four hours in B&N, I checked out. In the car, with *I Am a Cat* on the passenger seat, still considering what begets intelligence. After sitting in the car for several minutes contemplating, I decided I'd call my mom to hear her thoughts, given she seemed the perfect counterargument. I called.After some obligatory discussion about an eye appointment I didn’t want to talk about (see [this post](https://97115104.com/2025/09/30/chalazion/)), I asked Mom directly about curiosity and skepticism, reading the quote from the translator of *Crypto-Infection* directly. After being sure she'd heard, as you can never be sure with the spotty reception from Mint Mobile, I waited for her response.She paused for several seconds. Then said,“Well… I’m not curious or skeptical.”Then, after a moment longer,“Maybe a little curious.”“But mostly I learn from observing. Watching. Listening.”I asked a few follow-up questions about what that meant for her, and how she would compare observation to things like curiosity. At first, she agreed with me about why curiosity was obvious and the same with skepticism, and then mentioned that sometimes she just trusts, which comes from intuition from observation. Observation does proffer information. And what is intelligence, if not the ability to gather, organize, and apply information?We briefly touched on faith as a conduit, too, which I explained away by simply referring to the many people who have walked off cliffs believing they could fly, the old tale grown-ups use to explain why doing drugs is bad. We settled on three super-categories: curiosity, skepticism, and attention.Attention and not observation since attention is mostly applied observation. We ended our call agreeing it was an interesting subject to think about, but mostly a thought experiment, and were both tired from the mental gymnastics. I couldn't get it out of my mind, though.Are there other categories? How does intelligence work? Or am I just being curious. Or skeptical. Or attending to myself thinking. I thought, to be continued, and drove the rest of the way home.## food for thoughtAs I usually do after speaking with my mom or dad or other friends about some lofty thought experiment, I broached the topic with my best friend, to hear his thoughts, but mostly to solidify my thinking on the subject, closing a mental loop.I read the same excerpt from the translator of *Crypto-Infection* to him, and asked for his thoughts. He is a highly skeptical person, and equally curious, so I suspected he would have a similar perspective to me, though he might approach the problem of where intelligence is ultimately proffered differently.I was correct in that he agreed curiosity and skepticism make sense, but he also helped me come up with a few concepts that clarified attention, and how each, when combined, build a strong case to exclude things that do not proffer intelligence. For example, one can apply a simple concept of multipliers where a super-category, take curiosity, can be multiplied by a person's motivation for a given problem. If a person is very curious about why the sky is blue, thereby sufficiently motivated, then they are likely to build a good amount of intelligence about why the sky is blue through research and determination. On the other hand, if a person is curious about why the sky is blue, but is not motivated to find the answer, then they are unlikely to obtain the intelligence explaining the reason for the color. Put more simply, we can represent this as a math equation where S = the super-category, M = multiplier, and I = the amount of intelligence gained.I = S * MFor example:* if I = Curiosity (1) * Motivation (3), I = 3* if I = Curiosity (1) * Motivation (0), I = 0The same concept can be applied for all super-categories, like skepticism and attention. If we can quantify the level of intelligence gained by multiplying a super-category, then how can we validate a super-category? Well, if a category cannot be multiplied ad infinitum, then it cannot possibly be a super-category. For example, if we tried organization as a super-category, it becomes obvious that a person cannot organize to infinity, but they can be curious to infinity, and they can be skeptical to infinity. In the case of the sky being blue, you could learn about light, and particles, and the ocean, and any number of unlimited other subjects multiplied by how motivated you are to solve the problem. Similarly, you could be skeptical about God, like who created God, and would likely give yourself a headache, but you could, with sufficient motivation, go at the problem forever, reading innumerable texts. And as mentioned earlier, if you have faith to infinity, you'll likely end up at the bottom of a cliff, in a different kind of infinity.What then is the progenitor of a super-category? Is that even a thing? Does motivation or curiosity come first? Well, if you think about it neurologically, a person cannot be motivated without energy, that is from food, exercise, or some other source. Figuratively speaking, you get out what you put in when it comes to the body. So energy must then be what feeds super-categories of intelligence. So with sufficient energy, or calories from food, etc., you could then be infinitely intelligent? It appears possible. Of course, apart from notable exceptions of the human condition being sleep, safety, and age.To better understand this idea, the "scaling hypothesis," currently being discussed in the AI infrastructure space, might be helpful. The hypothesis states, to put it simply, that LLMs are as intelligent as the resources they have access to. There are several types of resources the hypothesis considers important such as pre-training data, post-training feedback, and the compute available via raw processing power (currently powerful GPUs). All are analogous to human traits and abilities. For a primer on the topic, I highly recommend [*The Scaling Era: An Oral History of AI* by Dwarkesh Patel](https://www.goodreads.com/book/show/239472387-the-scaling-era?from_search=true&from_srp=true&qid=ISKSBYzaJU&rank=1).If AI has been proven to outperform, deterministically, based on the scaling hypothesis by reducing loss thereby increasing accuracy with greater access to resources, then maybe the same concept holds true for individual human intelligence? A so-called "human scaling hypothesis." Patel talks about this briefly in his book, when contrasting brain size and circuitry with other mammals and non-mammals throughout history, and it makes sense.Could it be that one can deterministically calculate intelligence by simply generating a coefficient of superset categories and multipliers? Can we, as individuals, make ourselves more intelligent simply by finding our preferred super-category, and then fueling our multipliers with the right kind of energy?Lofty questions, yes. Mental gymnasium activated, yes. All just food for thought, also yes...pun intended ## secondary thoughts “on entertainment”My day started before Barnes & Noble.It started with a recruiter message about a founding engineer role at a startup called [camfer](https://camfer.dev/). This is how professional curiosity manifests for me now: a compelling title, an ambiguous mandate, and just enough information to force unpaid diligence before even deciding whether interest is warranted. Applying cold doesn't work anymore, and referrals don't seem to pan out like they used to, so for better or worse, the best strategy seems to be to create a fishing pole profile, anglerfish the best possible opportunities by creating content, engaging with platforms, and sharing your life's story. Hiring and recruiting, on both sides, has become strangely asymmetrical. Recruiters initiate, candidates investigate. Recruiters narrate; candidates verify. That or you get screwed. Don't verify and you might be in a nightmare job, get laid off, or worse, and have to move back in with your parents. Try cold applying, and you'll end up having to research a different kind of black hole...I hadn't heard of camfer, though the recruiter did mention a recent Y Combinator raise and CAD plus AI, which was intriguing. I needed more information to decide if I should spend valuable time and attention learning what I needed to in order to have a decent first interview. In my research, I came across a blog post by one of the founders titled [*“on entertainment.”*](https://www.keat.one/2024/11/28/on-entertainment.html) It was the very first post from the founder. A common thing I do now is to look for the first post on a blog, as I find it is an indicator of what to expect from the author(s) and their content. I read it quickly at first, then again more slowly. It wasn’t really about entertainment in the conventional sense. It was about what entertainment has become optimized for, and more importantly, how the decisions made in the name of optimization are having real consequences, especially for younger generations.*"on entertainment"* articulated something I’ve felt but hadn’t fully named: modern entertainment is engineered to monopolize attention so that it can sell you things you probably want but also, probably, don't need. All based on you, as a person, which is obtained through an aggregate of your life via browsing history and social media, in the form of an advertisement ID. Scary.The concept of the modern entertainment economy followed me into B&N, and it framed everything that came after, especially my thoughts on intelligence, reading, and the convergence of AI tools with targeted ads.I’ve been thinking about and trying to design more incentive-aligned systems for a while now, including the [Autophage Protocol](https://autophage.xyz/paper/litepaper.pdf) and my more recent writing on [AI privacy and safety](https://97115104.com/2025/10/11/ai-privacy-safety/). In Autophage, I proposed a system where *activity measurably benefits human health* by incentivizing movement, adherence, and participation, and in AI privacy and safety, I propose a system that rewards users for their prompts and engagement.That same logic could apply to attention, more broadly.Right now, attention is extracted, refined, and sold. Platforms profit from it, effectively stealing energy through attention. Models improve because of it. Yet the human providing it remains uncompensated, unacknowledged, and increasingly shaped by the very systems monetizing their cognition.This imbalance becomes more acute with AI.Large models are trained, in part, through [Reinforcement Learning from Human Feedback](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback), including passive behavioral signals like what users accept, reject, refine, or abandon. This is labor, even if it is unrecognized as such. It improves systems. It increases their value. And yet users are neither paid nor meaningfully protected. Their prompts may be logged, their preferences inferred, their cognition quietly folded into optimization loops they do not control, and they pay for the pleasure.In my writing on AI privacy and safety, I argued that this is both an ethical issue and, in some cases, a technical oversight. If users are participating, actively or passively, in the improvement of these systems, they should be compensated, as compensation is monetarily valuable to the individual, especially projecting into a future where AI tools are ubiquitous and attention as a cognitive resource more commonplace, and an acknowledgment of agency, restoring some semblance of symmetry. In the absence of that, we'd be in a world of hurt as a group of people, "humans," given the moderation of these tools and their cognitive pull on our other capabilities like curiosity and skepticism. Deepfakes, fallacious news, personalized worlds, and agent-to-agent protocols already exist. How can the next generation hope to compete when they are already primed to feed an intelligence that continues to grow in performance and capability before they've even been able to get a job?Which brings me back to intelligence.In wandering the bookstore and talking with people I trust, I kept circling three candidates for what actually proffers intelligence: curiosity, skepticism, and attention.Attention is the easiest to measure, which is why institutions fixate on it. Standardized testing, credentialing, performance metrics, blah, blah, blah. Attention alone is inert.When most discretionary attention is consumed by infinite feeds, attention ceases to be a precursor to understanding and becomes a terminal state. It anesthetizes curiosity and renders skepticism socially inefficient. Why doubt when the next stimulus is already loading?From that perspective, doom scrolling is a cultural habit that begets an epistemic failure mode.AI compounds this risk. When answers are instantaneous, fluent, personalized, and confident, the struggle that once produced understanding is bypassed because things like curiosity become short-circuited or dangerously reinforced, while skepticism becomes optionalized and abstracted.If attention is the currency of the digital age, then the economy we’ve built values attention above other kinds of intelligence, and it is not us who benefit from it.## what's next?Is there a future where attention is reciprocally incentivized, like in the dystopian Black Mirror-esque episodes where people’s only job is running on a treadmill they watch ads on, or perhaps where self-driving cars are free because the entire cabin is a glittering, excruciating, blue-light anathema?What happens to the distribution of wealth or intelligence? Is a benevolent superintelligence powered by an infinite attention economy at the cost of other forms of intelligence more beneficial to the greater good, or is "every person for themself" still the better option if a single entity or entities control that superintelligence?Are we creating a new kind of oligarchy posing as the companies and products we currently trust enough to use?Only time will answer these questions. The more interesting question may not be what companies are building, but what kinds of intelligence we are still able to cultivate.If attention is finite, then choosing where to spend it becomes a deeply personal act, and stealing it possible.A future that respects intelligence does not require perfect systems or benevolent machines. It looks smaller. Fewer feeds. Slower tools. Interfaces that reward curiosity, skepticism, attention, and other things like creativity, openness, and collaboration. Intelligence should grow because it is abstract, exercised, and shared.Maybe intelligence has always come from the same place. From time set aside. From attention that is chosen. From the quiet decision to stand in a bookstore aisle and read five percent of a book with no system watching, in an effort to buy a personal Christmas gift that enriches, instead of a bauble served up via algorithmic slop chosen by a machine that has no relationship to the giver or receiver, other than the profit margin it obtained.

---

US 20250267003A1a9) United Statesa2 Patent Application Publication (0 Pub. No.: US 2025/0267003 A1Harshberger (43) Pub. Date: Aug. 21, 2025(54) PRIVACY-PRESERVING HEALTH (52) US. CLVERIFICATION SYSTEM WITH INCENTIVE cre HOAL 9/3221 (2013.01); HOIL 6304MECHANISM FOR REGULAR TESTING 013.01)(71) Applicant: STD VERIFY, SPC, San Francisco, 67 ABSTRACToe A privacy-preserving health verification system with nenor tive mechanisms for promoting regular testing behavior is(72) Inventor: Austin Shane Harshberger. San disclosed The system implements» erypograplic spars:Francisco, CA (US) ing minimal verification metadata from health documenta-tion and generating zero-knowledge proofs. Preudonymous(73) Assignee: STD VERIFY, SPC, San Francisco, identifiers eliminate direct linkages between identity andCA US) health status while maintaining verification integrity. Analgorithmic incentive mechanism analyzes temporal patternsbetween verification events, implementing streak detection@1) Appl. No: 197200691 with configurable grace periods and recovery accelerationfactors. The system enables secure verification sharing with— 7.2 third parties through cryptographically signed. time-Timited(22) Filed: May 7, 2025 tokens containing no personally identifiable’ information.‘The implementation simultaneously solves privacy chal-publication Classification lenges and psychological barriers to regular testing, deliv-Publication Classificati ering a viable solution for improved testing schedules and(51) Int. CL enhanced compliance with recommended health protocols,Toi, 932 (200601) particularly. in’ sensitive health domains such as sexuallyHO4L 9/40 (2022.01) transmitted diseases and infections.Account CreationUsesId Creation(710)IdentityVerification (702)ProfiledGeneration (703)10 MappingCreation (704)Test Verification Processveeewed] Tost document+ Upload (706)Retry § Ai hd Yes Document Analysis(767)+ Document Deleted +[POR Verification Status Zovo-KnontedgeUpdate (710) Proof (708)Patent Application Publication ~~ Aug. 21,2025 Sheet 1 of 8 US 2025/0267003 AlFIG. 1: System Architecture DiagramClient Layer (161)102API Gateway Layer (105)Microservices Layer (109)Core Sexvices Integration ServicesVerification Processing (111) Dating App IntegrationFoentity protection subayaten 23Database & Storage Layer (114)Cryptographically Separated DatabasesVerificationPatent Application Publication Aug. 21,2025 Sheet 2 of 8 US 2025/0267003 AlFIG. 2: Privacy-Preserving Verification FlowUser Device (201) J — Statuss Display (211)Pe — Lo Sexvex-Sidei bocument } oo Processing (205)§ upload (262) ooST statesClient Side SY A — SoeProcessing (203) \ (210) ;+ Indbial OCR | Document :} Processing Verification HEE (206) HH Secure ‘U rransmission © ZK Proof Verification5 (2048) ; Generation (267) statusNR Assignment (209)Secure DocumentDeletion (208)Patent Application Publication Aug. 21,2025 Sheet 3 of 8 US 2025/0267003 AlFIG. 3: Points System Algorithm FlowchartTest VerificationEvent (301)Base Points streak Analysis [¢ Testing Frequency |.....,Calculation (362) Algorithm (303) Evaluation (304) F£ Feedback Loop’;Multiplier HApplication (305) HAchievenent Points Awazd | iEvaluation (306) Transaction (307)User NotificationEngine (308)Patent Application Publication Aug. 21,2025 Sheet 4 of 8 US 2025/0267003 AlFIG. 4: Verification Sharing System Architecture1557i Step 1 (51) i(Er| i£ [Initiate Sharing Platform APL OAuth Request | §i (402) (403) (404) Hi Hi Thixd-Paxty App | || (405) iil consent Scxeen Step 2 (52) Hi (406) — ii i£| Grant Permission Third-Party App Authorization |iH (407) (4058) Code (408) |:] ]| ii Access Token Token Exchange |:£ (218) (409) iEoms——————————————————i Verification Step 3 (53) i| Request (413 —_— H| |{[riza-rarty App Platform APT Zexo-Knouledge Hi (4058) aot Verification ii Response (412) HPatent Application Publication Aug. 21,2025 Sheet 5 of 8 US 2025/0267003 AlFIG. 5: Multi-dimensional Reward System Structuremmm mmm mmm ————————J Bidirectional Relationship |. WV.Point Sources (501) Achievement Types (502)Base Testing Activities Unlocks Coxe Progression BadgesTv ———r Special AchievementsFrequency Rewards Limited-tine BadgesRequizesTI m—— Partner AchievementspT — [Festing Milestone Recognitions]Drives EnablesStreak Mechanics (503) Reward Catalog (504)Consecutive Test Tracking Haltiples | [oe Discounts & VouchareStreak Protection Premium Platforn FeaturesWaltiplier Systems Fartner-specific RewardsStreak Visualization Digital Collectibles~N Bidirectional Relationship ~Patent Application Publication Aug. 21,2025 Sheet 6 of 8 US 2025/0267003 AlPE = F]t,t 81.8 £45.2253 | Eacsd3%3FFE = HEEEEE LEE- & FEE E EEE RL]al) EREEERLEH 15 slg 3.i |g] i: §15. £34] 8 EEE S HPO EI FR 5 PS § A PTE OWS | 0£ slo28%2358 228%52%io |E#|fizgEzs |T|E|gEEecdesSE4EEEE |[23 aM 7H H 2] 24 i = gl * P—¥ H H “leg sE ¢3 i gE Ef 2H i ERREEEE3 iLEN£ HE HT F] st3 |¥|gEi¥ii gl i:AHERN sl& §3..%%H 34 HE] PY]# E PY PSEE geal 3&8 Zesl=lule- °Slgie, E2Z|%[5|3E 12%.ARN HHH EEEHEE ELE IMEI REI REEL23880883 3 £3 REERTPatent Application Publication Aug. 21,2025 Sheet 7of 8 US 2025/0267003 AlFIG. 7: ProfilelD tifecycle DiagramAccount CreationUserIo creation(710)TdontityVerification (702)FrofiletnGeneration (703)© rapingCreation (764)Test Verification ProcessJRE [Ry —"i Upload (706)Retay |H hod Yo Document Analysis(707) bocument Deleted |PETITE Verigication Status Zoro-KnowtedgeUpdate (710) Proof (708)Patent Application Publication Aug. 21,2025 Sheet 8 of 8 US 2025/0267003 AlFIG. 8: Account Creation and User FlowInitial Account Creation Identity Verification Sign-On ExperienceRedizect to Identity Gonexate ProfileldProvider (808) (814)Identity Document Create ID MappingDownload App ox Subaiission (809) (815)Visit Website (802)Biometric Enable Fullcomplete Confimmation (810) Accaunt (816)Registration (803) xNg ;: lpnWoYes JE Secure SessionEmail Verification | Verification; Management (818)(805) Faded (812)!(813) Uplaad Test DocumentLinited State Account 3 (819)Creation (306) JRE,+ Name Change Process §Triggering New +- I erofilers (137) EndPrompt for dentity ntl om-US 2025/0267003 Al Aug. 21,20251PRIVACY-PRESERVING HEALTH [0012] Blockehain health record systems such as the oneVERIFICATION SYSTEM WITH INCENTIVE disclosed in US. Pat. No. 10340038 to Witchey. storeMECHANISM FOR REGULAR TESTING medical data on distributed ledgers, “chronicling a person'shealthcare path through life.” prioritizing provenance andFIELD OF THE INVENTION full disclosure over the selective disclosure capabilities10001] This invention relates to digital health verification foes for privacy preserving venation. einsystems, specifically a privacy-preserving platform that privacy-pr 2cuables health testing satus verification without exposing Yor eoton system using zero-knowledge proofs and behav.Underlying medical formation, elutes secure sharing of 212 15eniYe © promot regular esting I addresses STLarifcation satus wih authorized hid pari, ond incor: [reYenCn by combining anonymous verifition, andporates behavioral incentive mechanisms to promote regular ronmohieall ontity fron vetifeation offispiniog cryptographically separates ideniity from verification status.BACKGROUND OF THE INVENTION BRIEF SUMMARY OF THE INVENTIONsonal tae » } [0014] A privacy-preserving health verification platform is10002] Regular sual ansmited disease (STD) tng Lido” dar mhcmaiely mares sonioentaltyoe bl rudy Ctoier Aa mie Te while enabling individuals to share testing status withouton \ Prvention He 2018). appro revealing private medical records. The system establishesmanly 20% F Arians sr ing with an STL wal 26 15 nt hugh a one im verifiation ross, voimillion new infections occurring each year. The impact of pjca (7 documents, generafos zero-knowledge proofs,ion new infec yer | deletes orginal documents, and incentivizes regular testingmen (MS), who accounted fo 68% of all new Tv ioush points, achievements, nd vaiale rewardsdiagnoses in 2022 (HIV gov, 2022). Additionally, LGBTQs 100181 ¥his integrated system adresses both privacy con-youth face barriers to care due to stigma and institutional poe and behavioral barriers through synergistic compo-distrust. Several technical and psychological barriers pre-vent optimal testing froqueney: Ee PEP 0016) dentity Foundation & User Control: The system10003] Trivacy Cancemms: Carrent systems require storing SSblihes (st through identity verification doringsensitive medical data cither on ceniral servers with line nboArding. reating a eryptographically scaled profile thatrivacy guarantees or distributed lodgers where the lack of ¢P2b1es future pseudonymous operations while maintainingprivacy garies s usr control of veifaion shri,oy N . [0017] Document Analysis & Zero-Knowledge Verifica-men To md tn Coton Stony tS SL ELauthentication, or tamper-cvident records, users must rely SWS Wilbout exposing ross wit a dual-processingei ct orbs Pe d YW upproach that leverages OCR and NLP models trainedsolely on each other. specifically on large sets of medical records. Only minimally10005] Lack of Motivation: With no computations] reward pecs ary metadata s extracted and the original document fvmechanisms, gamification elements, or sieak-racking a80- permanently deleted. These systems, when combined wilhrithms to reinforce positive pattems, users lack incentive to pccudonymous identifier, eryplographically separate identt ery oe No soot a tity from verification siafus, ensuring no sensitive dataragmented Fosystem: No standardized solution remains after verificationoun ir pels efi sere lator whe main- 00] Arnie Tnccniviaton: Bion eining privacy. Current systems lack interoperability, fore: rithms promote regular testing through rewards thal increaseing. propria. implementations that create. inconsiient vit engagement. Network pers value throughexperiences and increased privacy risks. referrals and partner integrations as ecosystem growth10007) Psychological Barriers: Stigma and anxiety sur- increases verification utility.rounding testing create significant psychological resistance [0019] Secure Verification. Sharing: Cryptographicallythereby reducing testing frequency. secure, time-limited tokens enable third-party verification[0008] Previous attempts to address these challenges have using enhanced OAuth 2.0 protocols without exposingbeen inadequate: medical data.10009] Dating platforms such as those disclosed in US. [0020] Adaptive Streak Detection: Pattern recognitionPat. No. 7,246,067 to Austin et al., offer verification features identifles and rewards consistent testing through temporalthat Jack automation, incentivization, and store data cen- analysis algorithms while maintaining unlinkability.trally providing litle or no privacy guarantees. [0021] Progressive Reward Mechanics: Variable reward10010] Existing health incentive systems like those strategies that are based on testing consistency maximizedescribed in U.S. Pat. No. 8.719.056 to Bartley et al. store behavioral reinforcement. This approach uses scientificallypersonal data in an immutable ledger, creating direct link- validated variable-ratio schedules, applied through compu-ages between identity and health activities tational methods and logarithmic scaling functions for long-10011] Current credential verification and licensing ser- em engagement.vices rely on labor-intensive human review or distributed [0022] The novel combination of all these componentsprocesses without privacy assurances or they require sub- simultaneously solves privacy challenges and psychologicalmission of complete medical records (FCFM G. 2023; US. barriers to regular testing, delivering a viable solution forPatent Application Publication No. 20150278824 to Zabar: improved STI testing schedules and a new type of eryplo-US. Pat. No. 1.679.151 to Mabalingam et al). graphic health verification.US 2025/0267003 Al Aug. 21,20252DEFINITIONS customized branding and user interface components while[0023] As sed herein, the following fers sll have the "221inE he nding privacy and security mechafollowing meanings: HEe ror re 10036] “AVMY refers to a ZeroKnowledge Virtual10024] ero-knowlede proof” reles to 0 epORrapbic Nguchine, a computational environment that enables themethod by which one party can prove to another party creation of zero-knowledge proofs for arbitrary computa-a given statement is tre without conveying any additional Grea:information. . -— »[0025] “Preudonymous Identifier” refers to an artificial [on] CIrstless refers 10 a system foseioatrdiedidentifier that consistently represents a user within the (CRETE VY BAMECH PIMEY HC SREY (REDsystem without revealing thee stual dent, allowing or 700 otl ahory or organization with ther sensiivepersistent recognition across. multiple interactions while re <" y or orgai md preventing correlation with the 103) Testing streak” refers to a consecutive series ofaT oH ers tthe Software Development Verified health ests conducted within defined ime intervals.[is pli To hind pert platforms that enables secure 100391 “Points leer” refers tothe transactional databaseintegrin wil he verfcaon syst nluding prebuS15T at ecords point comings, desuctions, and blaeesesi moon [0040] “Providerverfied” refers 0 test results submitedLorient diveetly by auborized healthcare providers rather thani . .. uploaded by endusers.[0027] “OAuth Enhancement” refers to the system's N10027) Oth Lnhancement’ refers to he SSMS [0041] “Verification satus” refers to the cryplographicallypension ofthe standard OAuth 2.0 protocal with additional Secured attestation that a user has completed valid healthprivacy. preserving feature, including fine gained permis i ii specifi imelrame, ed 0 profile withsion sees, peeuomymons token an imitably verified deny that was confined duringen ee er ne . initial document processing.10028] Double-biiry. Potts Ledger” lens to. e002] “Achievement badge” refers to a digital recogni-accounting mechanism wed to {rack point ANSACHONS, ion awarded for specific testing behaviors or milestones.ensuring that al point awards, deductions, and transfers are (ot, TCE 1 SPREE Ing pehaviors of ESR,resorded with both a debit and credit entry to maintain pinyin o user's consecutive testing record despite minorsystem integrity and enable audit capabilites. freestovtselcmpary10029] “Nl Factor Verification” refers 0 he document jggq4) 0 uh refers to the open standard for accessauthentication process hat imulancously analyses MOIS gion, commonly usd fo cure hind-pany boraspects of submitted heals documentation. nluding Format ou sharing erent,consistency, sesuriy features, temporal consistency. and {gng)” "API" refers to Application Programming otestructural validation. face, a set of defined rules for how software components10030] “Logarithmic Scaling Function” refers to the math- ~~ c5 & neematical model used 1 determine point values for long-term:engagement, implementing a decreasing rate of reward BRIEF DESCRIPTION OF TIE DRAWINGSgrowth that maintains motivation while preventing systeminflaton. 10046] The patent application includes the following fig-[0031] “Variable Reward Schedule” refers o the behav- ures:ioral reinforcement mechanism that implements wpredict- [0047] FIG. 1 illustrates the system architecture of theable timing and magnitude of rewards based on scientific privacy-preserving health verification system, showing theprinciples established by Terster and Skinner (1957) to client layer, API gateway layer, core microservices layer,maximize engagement. and database & storage layer.10032] “Provider-Direct Submission” refers to the secure [0048] FIG. 2 shows the process flow of the privacy-pathway allowing authorized healthcare providers to submit preserving verification mechanism, including user device,verification records dircetly 10 the system on belulf of users, client processing, secure Iransmission, Server processing,increasing rust through professional verification while document verification, zero-knowledge proof generation,maintaining privacy document deletion, and status assignment[0033] “Cross-Platform Verification Token refers to the [0049] FIG. 3 depicts a flowchart of the points systemeryplographically signed, time-limited authorization that algorithm for incentvizing regular testing behavior, show-cnables verification status sharing across different platforms ing. test verification even, base pains calculation, streakor contexts without revealing the underlying personal infor- analysis, testing frequency, multiplier application, achieve-mation, ment evaluation, points award transaction, and notification10034] “Network Eflets” refers to the phenomenon where engine.the value of the system increases forall participants as more [0050] FIG. 4 shows the verification sharing system archi-users join the platform. As the user base grows, verification tecture, illustrating the OAuth authorization flow, tokenstatus becomes more widely recognized and accepted across generation, access control, and verification requestesponsepartner platforms, creating 4 self-reinforcing cycle where componenis.increased adoption leads to greater uty for all users and [0051] FIG. 5 depicts the muli-dimensional reward sys-partners in the ccosysten. tem structure, including point categories, achievement types,[0035] “White-Label Integration” refers to the capability sircak mechanics, and the relationship between behaviorsfor partner platforms to implement verification features with and rewards.US 2025/0267003 Al Aug. 21,20253[0052] FIG. 6 illustrates the database schema for each rather than UserlD is associated with verification records. Asaspect of the system and the relationship between cach table. shown in FIG. 7. client-side name matching (706A) ensures10083] FIG. 7 illustrates the lifecycle of a ProfilelD document ownership before any data is transmitied to thethroughout the privacy-preserving health verification sys- server, after which server-side processing (707) and zero-tem, including initial account creation and. verification knowledge proof generation (708) lead to verification statusevents. updates (710) that maintain cryptographic separation[0054] FIG. 8 depicts the user flow during account cre- between identity and health verification statusation, identity verification, and sign-on process, showing the [0062] The ProfilelD) remains constant throughout a user'sinteraction between user actions and system processes, lead lifecycle, with verification status updated with each nen testing up to the verification process. submission. As depicted in FIG. 8, the only scenario thatriggers creation of a new ProfilelD is the formal nameDETAILED DESCRIPTION OF THE change process (717) mentioned carlir, which requiresINVENTION appropriate document and idenity attestation before migrat-| ing verification history to the new ProfileTD and deactivating10088] The present invention provides a comprehensive ihe previous ane. The verified legal name becomes immu-system for privacy-preserving health verification With ine- 341 en fll account access is enabled (816) and can onlyented incentive meshanisans. Various embodiments of the cL EES EOinvention will now be described in detail with reference 10063] * Fach verification status has predetermined valid-the accompanying drawings. ity period based on medical guidelines for testing frequency.Syme Overview and User ifeescle The system tracks expiration dates, notifies users beforeSy i expiration, implements grace periods to maintain streck10086] The system implements a user-centric verification continuity, and automatically expires verification statusesplatform that begins with account creation and extends alier the validity periodthrough ongoing test verification and rewards. As shown in .FIG. 8. the user journey consists of three key phases: initial System Architectureaccount creation, identity verification, and the subsequent 0064] The system comprises multiple interconnectedsign-on experience. A technical components that work together to enable privacy-10057) During initial account creation, as shown in FIG.8, preserving verification and behavioral incentivization, asusers download the application or visit the web platform {llustrated in FIG. 1. The system includes(802) to register with basic idenifying information. account [0068] Secure Upload Subsystem: Encrypted documentsecurity credentials (803), and accept terms and conditions transmission channel with client-side encryption and secure(804). Fmail verification (80) confirms the account. which key management.exists initially in a limited functionality state (306). [0066] Verification Processing Engine: Advanced optical[0058] To enable full functionality, users must complete character recogaition (OCR) and natural language process-the identity verification process show in FIG. 8. The system ing (NLP) modules specifically optimized for medical docu-securely prompts (807) and redirects users 10 a trusted ment analysis, with integrated identity matching capabilitiesthird-party provider (808) where they submit govermment- that verify document ownership while maintaining strictissued identification (809) and complete biometric confir- separation between verification status and personal identi-mation through a selfic with liveness detection (810). Upon firs after intial processing.successful verification, the provider retums a cryptographi- [0067]  Zero-Knowledge Proof Generation Module: Inte-cally signed attestation (813) containing the verified infor- gration system that utilizes third-party zkVM (Zero-Knowl-mation edge Virtual Machine) providers to create verifiable proofs[0059] The system uiilizes this atiestation to create an without exposing underlying dataimmutable profile record containing the verified legal name, [0068] Points Ledger and Transaction System: Immutablewhich becomes eryptographically scaled and cannot be transaction record system with double-<niry accounting forchanged without undergoing a formal name change process tracking user testing behavior.(7117). As illustrated in FIG. 7, this process simultancously [0069] Mult-dimensional Reward Algorithm Engine:generates a unique ProfilelD through one-way hashing (703) Complex algorithmic system for calculating and awardingwih a server-side secret key and randomized salt, creating points based on various testing behaviors.the mapping between UserID and ProfilelD (704) for inter- [0070] ~ Achievement and Badge Management System:nal system operations Progressive digital achievement system with unlockable10060] After completing identity verification, users access display components.the system through password authentication with required [0071] Third-Party Authorization and Verification APItwo-factor or optional biometric authentication for mobile OAuth-based system for controlled sharing of verificationapps. This enables users fo initiate the test document veri- status.fication process where all verification activities are associ- [0072] Secure Data Deletion Module: Cryptographic dataated with the ProflelD rather than the user's identity details destruction system implementing 800-88 Revision 1 wipingthat are tied to UserID. protocols.10061] The ProfilelD lifecycle. as illustrated in FIG. 7. [0073] As shown in FIG. 1. the system is implementedforms the backbone of this system’ privacy-first architec. with a layered architecture. The Client Layer (101) includesture. Once created during identity verification, the ProfilelD web applications (102), mobile applications (103). and part-serves as the pseudonymous identifier which is used for all ner SDKs (104) that allow users and third-parties o interactexternal system interactions. An example of this is during with the system. The API Gateway Layer (108) providestest submission and verification (706) when the ProfilelD authentication (106). rate limiting (107), and request vali-US 2025/0267003 Al Aug. 21,20254dation (108) services. The Microservices Layer (109) Secure Document Destruction (208)includes the core services that implement the business logic,including User Management (110), Verification Proceseny ~~ 10088] (@ Original documentation is permancatly(111), and Points & Rewards (112) services. The Identity deleted using DOD-compliant wipingProtection Subsystem (113) maintains Stet separation 10989] (5) Multiple overwrite passes ensure data cannotbetween User IDs and Profile IDs. The Database & Storage be recoveredLager (114) stores user data (115). verification revords (116), [0090] (¢) Verification of deletion through erypto-and the points Jedger (117), with temporary secure storage. graphic atiestation(UB) for documents during processing Verification Status Assignment (209):Privacy-Preserving Verification Process [0091] (a) User receives time-limited verification status[0074] The invention implements a sophisticated multi- 10092] (b) Status includes crypiographic proof of veristage process for verification while ensuring complete pri- fication (vacy, as shown in FIG. 2. The process includes: 10093] 6) Status has predetermined validity periodSecure Document Transmission: User initiates transmission based on test type and medical guidelinesof test documentation through an encrypted channel imple- [0094] The verification status is then retumed (210) to the‘menting TLS 13 with perfect forward secrecy. As shown in Client-Side Processing system (203) and ultimately dis-FIG. 2 the User Device (201) initiates the document upload PIayed 10 the user on their User Device (201) as Status(202) to the Client-Side Processing system (203). Display (211). )Client-Side Document Processing: Initial OCR processing ~~ [0095] This process flow, as detailed in paragraphs [0075](204) occurs on the client device to extract minimally [0082]. ensures that no sensitive medical infomation isnecessary information before transmission. During process. $10red or shared at any point in the verification process andng, the system confirm that the users legal name on the Significantly differs from conventional verification systemstes document matches their verified profile name. This 45 it implements a zero-retention policy for medical testone-time identity verification creates an immutable link 414between the user's profile and their verification status with- .out exposing identity details in subsequent operations. Aller Database Schema for Points Ledger andclient-side processing is completed, the system securely Achievements Trackingtransmits only necessary metadata 10 the server (204A) [0096] As shown in FIG. 6, the database architectureServer-Side Verification: The Server-Side Processing system comprises several interconnected data models that work(205) performs advanced document analysis using special- together to maintain privacy:ized OCR and NLP algorithms trained specifically for mediccal document formats (0 extract Users Table (601):[0075] (a) Test date information i } .[0076] (b) Testing facility identification 10097] 0) ert (Primary Key) Securely sored10077] (©) Test ype classification [0098] (b) authenticationData Credentials and secu-Document Authentication: The multi-factor verification sys- ‘ty informationtem (206) analyzes document characteristics including: [0099] (¢) personalinformation Identifying informa-[0078] (a) Format consistency with known laboratory tion including the user's immutable nametemplates A [0100] (d) created At— Account creation timestamp10079] (b) Presence of expected security features [0101] (e) status — Account status flag[0080] (¢) Temporal consistency with claimed test date [0102] (1) privacySettings—User-controlled privacy10081] (4) Structural validation of expected document configurationelements [0103] (2) pointsBalance Current camed points10104] (h) currentSireak — Testing streak statusZero-Knowledge Proof Generation (207):10082] (a) Integration with third-party 7KVM providers files Table (603):[0083] (b) Creation of eryplographic proofs through the [0105] (a) profilelD (Primary Key) Pseudonymousselected 7KVM provider's APL identifier uscd for extemal verification[0084] (c) Proofs contains only verification status, [0106] (5) userlD Hash—One-way cryptographic hash‘pseudonymous profile ID, and test date, with special- of associated userlDized circuit optimizations to minimize computational [0107] (c) displayPreferences —User-selected displaycomplexity settings[0085] (0) Proofs are mathematically verifiable without [0108] (d) verificationVisibilty— Controls on whatrevealing underlying data verification data is shared10086] (e) Implementation of proof versioning for [0109] (¢) lastUpdated Timestamp of last profilefuture compatibility with post-quantum eryplographic changeprimitives [0110] (1) expirationDate—When the verification[0087] (6) Utilization of protocol-specific optimizations expiresto minimize computational requirements while main- [0111] (2) proofReference—Reference to the zero-taining privacy guarantees Knowledge proofUS 2025/0267003 Al Aug. 21,202551D Mapping Table (602): [0145] (©) astTestDate Date of most recent verified. test10112] 2) mappingID (Primary Key) . . ce period has[0113] (b) userlD> Hash Secure one-way hash of the oer ppgeeperodUsed -Whether grace periodwserlD A; N ) [0147] (p) streakLevel Tier of streck achievementhel ProfetD Hash Secure onesvay ish ofhe [p18] (0) nextlestDue-— Recommended date Tor next[0115] (d) mappingType Type of relationship between ethe IDs ewards Marketplace Table[0116] (e) created At When the mapping was estab. 00% Merkerplace Table (608)lished [0149] (2) ewardiD (Primary Key)[0117] (5) expires At Optional expiration of the map- 0150] (b)ttle—Reward nameping 0151] (©) description—Reward details0152] (&) poinisCost—Poins required to redeemVerifications Table (605): [0153] (e) inventory A vailable quantity5 verification! (Primary Key [0154] (1 partnerlD— Associated partner organizationmK eferences the [VSS] (2) expintionDite When reward offer expiressetdonyaous profile [0156] (h) redemptionCount—Times reward has been[00] (0 tte Dt of th nets st yy Tey prscring hn ans hs ne4) verted When eration was COM mechanisms etal in parsarophs [0095-0113] whileote ! N ensuring the Verifications Table stores only. verification10122) (©) expimionDate When verification S195 au an dates and associates records with podonymousvera profilelDs0123], 0 sor Orign of verification (ser upon. [D181 Ths apc sity ire sserence. Reference fo the. sero. em from prior ant solutions that store sensitive data centrally1241) proofitelerence —Refirence 0 the 200° or jy distributed edger as desribed in paragraphs 10009]0010], and [0012]Points Ledger Table (604) Poiats and Befavioral Incentive System[0125] (0) ransactionlD (Primary Key) or the oo }[0126] (5) userlD (Foreign Key)—For intemal points 101591 The invention implements o sophisticated algorith-meking only mic approach to incenivizing testing behavior, a depictedJOLT] oy amount—Point value of transaction in FIG. 3. The algorithm begins with a Test Verificationard Event (301) and proceeds through various compuaionl10129] (¢) multiplier Any applicable point multipliers ~~ S98°530 Pt uct, Base Testing Activity Rewards: The Base Points Calculationoi) (302) module assigns points according to the Dollowing[0131] (9) referoncelD Related verification or Schedules Co )achievement 10160] (0) Firs Test Verification: 100 pointssFirst[0132] (h) timestamp When points were awarded Step” badge10161] (5) Regular Test Upload (706): 50. points perAchievements Table (606) verified test[0133] (s) chievementlD (Primary Key) edad ety Eom stare povir33 evementD ) received dircetly from healthcare providersOe Eppley (orien Key) Links topofle br 9163) @) Comprehensive Pane Test: 25 bows potsublic achievements for tests covering all major STIs10135] (©) badgeType Type of achievement camed erty bds Ive of achievement eamed (© Pre-Expiration Testing: 30 bonus points for10136] ©) caredDate—When  chievement was uploading before previous fest expires‘ — i. “Testing Streak Mechanics: The Streak Analysis Algoridin017) diplyPrefeence How badge 3 SHON G03) nif testing consistency throughEy ctedSrenk Relaed te . 10165] (n) Implementation of temporal pattem recognioe Lp sociatedSirak Related testing streak if tion algorithms to identify testing consistencyapplicable I 10166] (b) Progressive reward scale for steak mainte-013915) mea Addons sicvmen informa pl; i } 10167] () 3 consecutive quarters: 100 bonus points[0140] (i) visibility Publicfprivate status of achieve: amet 10168] (ii 4 consecutive quarters: 150 bonus pointscols Table (607): “Health Tracker” badge (Silver)Streaks Table (607): [0169] (i) 6 consecutive quarters: 250 bonus points+.10141] (3) streakiD (Primary Key) “Health Champion” badge (Gold)10142] (5) profil) (Foreign Key)—Links to profile [0170] (18 consecutive quarters: 400 bors points +rather than user 1D “Ultimate Guardian” badge (Platinum)[0143] (©) startDate When steak began 10171) (3) Each additional quarter: +75 bonus points[0144] (@) currentLength— Current streak duration with logarthie scalingUS 2025/0267003 Al Aug. 21,20256Testing Frequency Evaluation (304): This module applies to incenivize consistent testing belavior. FIG. 5 illustratesfrequency recoition algorithms with multiplier applica: the components and their relationships:tion:10172] (0) Regular cadence (every 3-4 months): 1.2% Point Sources (S01)point multiplier 10194] (0) Base testing activites (first-time testing.10173] (b) Improved cadence (every 2-3 months): 1.5% regula esting)point multiplier 0195] (6) Stcak-based rewards (consecutive quarterly[0174] (c) Optimal cadence (every 1-2 months): 2x esting)point multiplier 10196] (©) Frequency rewards (shorter intervals[0175] Multiplier Application (30S): This module applies between tests)the calculated multipliers to the base point values. (0197) (d) Referral bonuses (encouraging others toAdvanced Streaking Algorithms: These are implemented verify)within the Streak Analysis Algorithm (303) and include: 10198] (¢) Community challenges and evens[0176] (5) Grace period implementation for near-misstesting dates Achievement Types (502):0177] (6) Stak protection mechanisms for COREY [0190] (1) Core progression badges with ied recogtesting lapses ten10178] ©) Resosry aeclraion or lapsed sks 300" (3) cial achievements for spi ptems0171 (@) Weighed sgoritmic caaion of sing [0201] (6) Uimtedme bases fo eas enspatternsMultidimensional Achievement System: The Achievement 0303] (d) Partner achievements for integrated plat-Evaluation (306) module implements Tors *10180] (@) Core progression achievement track with [0203] (e) Testing milestone recognitionshierarchical badge structure10181] (5) Special achievement unlocks for specific Sreak Mechanics (503).behavioral patterns- © achieve . 10204] (8) Consecutive test racking with grace periods10182) (6) Limited-time achievement opportunities ed j030g) (b) Stak protection for occasional upscsopal ness SL an integration capebil 10206] (¢) Recovery acceleration for lapsed straks10183] (@) Public heslh campaign integration capabili= 1307) (4) Muliplie systems for consistent behaviorJIA (©) Progressive difculy scaling for scheve. 11208] (©) Sik visualization and progres trackingment unlocks Reward Catalog (504)Dynamic Point Management: The Points A ward Transaction(307) module implements. 10209] Go) Testing discounts and vouchers10185] sy Toublecentry accounting ledger for point [0210] (b) Premium features on partner platformsmsactions [0211] (c) Exclusive access to venues and events[0186] (5) Point history with atributon and verification 10212] (0) Partner-specific rewardstrail 02131 (© Digi collcivis ad rsoniion§ + balancing algorithms 10214] As shown in FIG. §, the system creates bidiree-A871 Point coonomy balancing aeotthins onal reatonships. between Point. Sources. (S01) and[0188] (4) Frau fon systems for unusual ps Achievement Types (502), as well as between Streakpatiems . Mechanics (503) and Reward Catalog (504). These inter-[0189]  (¢) Performance optimization for real-time point connections function us a self-reinforcing ecosystem wherecalculations each component dynamically influences the others. Point10190] User Notification Engine (308): This module com- — secumalatin. from various sommes directly. pnlocksmunicates achievements, point awards, and status updates (9 achievements, while achievement progress simultaneouslythe user through configurable channels. The engine dynam rates new polnt-carting opportunites through milestonecally adjusts notification frequency and content based on ponyges. Similarly, streak maintenance increases access touser interaction pattern to prevent notification fatigue While ighervalue rewards, while the visibility of premiummaintaining optimal engagement. rewards in the catalog incentivizes consistent sreak main-[0191] As shown in FIG. 3, the system includes feedback tenance. This dynamye, mult-patiay reward structure cre-loop (309) from the Points A ward Transaction (307) back to ates multiple engagement loops that function simultane-the Streak Analysis Algorithm (303), creating a self-rein- ously, allowing users 10 derive motivation through theirforcing system that continuously evaluates and rewards preferred incentive type while still benefiting from the fullconsistent testing behavior py10192] The point system and achicvement mechanisms [0215] These interconnections create a comprehensivework together 0 addres the psychological barriers (0 egu- incentive system that rewards wsers from multiple angleslar testing by applying principles of behavioral economics while maintaining their privacy throughout the process,through computational methods. As prescribed by Skinner directly addressing the “Lack of Motivation” burierand Perse (1957) the system achieves measurably described in paragraph [0005]increased testing frequency compared to conventional fixed- [0216] By implementing a varible-ratio reinforcementratio approaches schedule through the interconnected components shown in[0193] “The reward system, as shown in FIG. 5, incorpo- FIG. 5. the system ercats stronger engagement patterns thanrates multiple interconnected components that work ogether fixed reward systems. This technical implementation opt-US 2025/0267003 Al Aug. 21,20257mies esting frequency and consistency. transforming what Zero-Knowledge Verification Response (412) that containswould typically be sporadic testing behavior into sustain- only the minimally necessary veri cation status information.able, regular health practices that benefit both individual completing step 3 (53).users and public health objectives. [0245] The verification sharing system creates a secure[0217] The invention implements a sophisticated system framework for sharing verification status with third partiesfor securely sharing verification status while maintaining without exposing any personal health information. Throughprivacy, as illustrated in FIG. 4. The system includes the the IdenityTollowing components and processes: [0246] Protection Subsystem described in paragraphAuthorization Framework: The system implements an [0073], the system maintains cryptographic separationOAuth 2.0-based authorization flow as shown in FIG. 4, between User IDs and Profile IDs. This architecture ensureswith that even if verification data were compromised, it cannot be10218] (a) Fine-grained permission scopes linked back to individual users, as partners only interact with10219] (b) User-controlled authorization flow with ~~ pseudonymous identifiersexplicit consent [0247] The system implements a comprehensive partner[0220] (¢) Granular sharing permission management integration framework that enables various types of organi-[0221] (d) Per-platform sharing settings zations to connect with. the verification. platform while[0222] (e) Revocation capabilities with immediate ~~ Maintaining er privacy and security. The partner integra-lect tion system includes:[0223] As shown in FIG. 4 the User Device (401) initiates 10248] Partner Portal: A dedicated web application thatSharing (402) with the Platform API (403), which sends an Provides tools for API credential management, analyticsOAuth Request (404) to the Third-Party App (405), com- dashboards, rewards management, verification monitoring,pleting the first step (S1). The User Device receives a and documentation access.‘Consent Screen (406) and provides Grant Permission (407). Partner APT: Secure endpoints that enable partners 10 iae-The Third-Party App (405B) then follows the stndard grate verification status sharing while maintaining privacyOAuth flow, with an Authorization Code (408). Token controls, including:Exchange (409). and Access Token (410). completing the [0249] (a) Authentication and authorization usingsecond step (52). OAuth 2.0Verification Token System: The system creates and manages [0250] (b) Verification status checking with minimaltokens with data exposure10224] @) Cryprogmphically signed tokens with tamper [0251] (¢) Batch verification capabilities for venueevidence entry management[0225] (b) Time-Jimited token validity 10252] (d) Anonymous verification for insurance and10226] (¢) Scope-lmited information disclosure public health partners10227] () No personally identifiable information within ~~ [0253] (e) Webhook notifications for status changestokens SDK Libraries: Pre-built software development kits for[0228] (e) Verification without central authority depen- common platforms that simplify integration, including:dence [0254] (a) Mobile SDK for iOS applicationsAccess Control Mechanisms: The system implements [0255] (b) Mobile SDK for Android applications10229] (a) Hierarchical permission structure [0256] (¢) Web component library for browser-based10230] (b) Temporal access limitations applications10231] (©) Usage quota enforcement [0257] (@ Server-side libraries for backend integration[0232] (d) Audit trail of verification checks [0258] The partner integration system supports multiple[0233] (¢) Anomaly detection for unusual access pal- partner types, each with specialized functionality:tems Healthcare Provider Integration: Enabling clinics and testingVerification Display Options: The system provides: centers to directly submit test verification with patient10234] (a) Configurable bade display settings consent, including:10235] (b) White-label integration capabilities 10259] (2) Secure API for fest date submission (no.10236] (¢) Customizable verification UI components actual fest results)[0237] (d) Progressive disclosure of verification details [0260] (b) Patient matching with consent verification[0238] (e) Verification status expiration indicators using ProfilelD>Verification Sharing Channels: The system enables sharing ~~ [0261] (¢) Provider verification badgingthrough: 10262] (d) Analytics on testing frequency and pattems10239] (a) API-based verification for applications [0263] (e) Custom rewards creation for patient incen-[0240] (b) QR code generation for in-person verifica- tivestion Dating Application Integration: Allowing dating platforms[0241] (c) Wallet pass integration (Apple/Google) to display verification status badges without exposing medi-[0242] (@) Email verification attestation cal information, including:[0243] (e) Webhook notification system for status 10264] () Profle badge implementationchanges 10265] (b) Verification status API[0244] The sharing process continues to ts final step with [0266] (¢) User filtering by verification statusa Verification Request (411) from the Third-Party App [0267] (d) White-label verification UT components(405A) to the Platform APL (403A), which responds witha [0268] (¢) Verification analytes dashboardUS 2025/0267003 Al Aug. 21,20258Venue and Event Integration: Providing verification check- [0286] Partner Integration Implementation: RESTful APLing capabilities for in-person events, including with comprehensive documentation. mobile SDKs, and web10269] (a) QR code verification system ‘component libaries enable secure integration while main-[0270] (6) Eatry management tools taining privacy protections. All integration pois implement[0271] (¢) Batch pre-verification capabilities appropriate security measures fo prevent unauthorized10272] (d) Stall verification interface access10273] (¢) Venue-specilic verification requirements [027] This preferred embodiment optimizes perlor-Insurance Partner Integration: Enabling incentive programs mance, security, and scalability while balancing technical‘while maintaining privacy. including: efficiency with privacy protection.10274] (a) Anonymous verification checking10275] (b) Aggregated health trend access EXAMPLES10276] (c) Incentive program management[0277] (d) Policy holder verification analytics Example 1: User Account Creation and Identity[0278] (e) Compliance documentation Verification10279] ‘The partner intepraion system addresses the [0288] A pew user nitates the account reation process,Fragmented Ecosystem” challenge described in paragraph emonstrating the system's identity verification architectureby providing standardized, secure methods for tHir-Pay that creates the foundation for privacy.preserving veraintegration. This technical approach significantly improves ion ¥upon prior solutions by maintaining complete privacy while [0289] The user dowaloads the mobile application andenabling seamless verification across multiple platforms and cregtes an account with basic information.contexs. [0290] To enable test verification. the user must completeidentity verificationBEST MODE 10291] (a) The system redirects to a trusted third-party.[0280] The preferred embodiment integrates with third- identity verification providerparty KVM providers for zero-knowledge proofs, uses [0292] (b) The user submits government-issucd identiMongoDB for the points ledger, and employs a microser- fication and completes biometric confirmationvices architecture. This configuration balances scalability, [0293] (¢) The identity provider returns a cryptographi-modularity, and fault tolerance while mainaining privacy. cally signed verification attestation10281] Zero-Knowledge Proof Integration: The system [0294] (d) The system extracts only the verified legalconnects with ZkVM providers through APIs. providing name and creates an immutable profile recordoptimal security and performance for verification without [0295] (¢) This verified name becomes cryplographi-exposing health data. Proof generation passes only relevant cally sealed and cannot be changed without a formalextracted data fields and a signed source hash. The system review processincludes redundancy and recovery mechanisms ensuring [0296] If idenrity verification fils:continuous operation despite temporary component failures [0297] (a) The system notifies the user with appropriatewith the option to switch between ZKVM providers if error guidancenecessary. 10298) (b) The account remains in a limited state with10282] Document Analysis System: A cascading pipeline verification capabilities disabledcombines client-side and cloud-based OCR with custom [0299]  (c) Analyties log the failure reason without stor-NLP models optimized for medical documents. This dual- ing sensitive informationprocessing approach first performs extraction on the user's [0300] (d) The user may retry verification with freshdevice to minimize data transmission, including identity credentialsverification by matching the legal name on test records with [0301] Upon successful verificationthe user's verified idenity. Server-side algorithms identify [0302] (o) The system creates a separate ProfllD cryp-fest dates, provider information, and authenticity markers Tographically derived from the UserIDwhile aggressively filtering out diagnostic resus and per: [0303] (b) Zero knowledge mappings are establishedsonal identifiers . between these identifiers10283] Secuse Deletion Implementation: Medio Saif 0304] (0) The use gain acces to document vrifca-tion follows NIST SP 800-88 Rev. 1 guidelines. For SSDs. on capabilitiesthe system uses manulicturerprovided sceure eruse Om 0305) (d) The system awards inital proffe completionmands or cryptographic erasure: for magnetic media, single- potaspass overwrite. All methods include verification mecha-nisms 10 confirm successful deletion, maintaining Example 2: Dating Platform tegrationcompliance with data protection requirements.10284] Points Algorithm Implementation: MongoDB with [0306] A dating application integrates with the systemspecialized indexes enables efficient streak detection and provide verified health status indicators on user profiles:real-time point caleulations. The system architecture ensures [0307] The dating application registers as an authorizeddata consistency and scalability to support growing user platform within the system through the Partner Portal.populations while maintaining performance. [0308] Users of the dating application connect their pro-10285] Authorization Framework: OAuth 2.0 with JWT files through an OAuth authorization flow. granting limitedtokens (RS256) and rotating refresh tokens secures third- permissions to access only verification status.party integrations. Token payloads contain only pseudony- [0309] When a user uploads and verifies a new test:mous idenifiers and validation metadata. remaining revo- [0310] (a) The system processes the document throughcable and verifiable without identity disclosure the privacy-preserving verification flowUS 2025/0267003 Al Aug. 21,2025910311] (b) The verification status is updated in the user's [0338] Users with valid verification receive VIP sceess loprofile exclusive areas, complimentary premium services, or spe-10312] (¢) The dating application receives a webhook cial event access unavailable to non-vrified attendees.notification of the status change [0339] The venue receives aggregated, anonymized ana-10313] (4) The application displays a verification badge lyties on verification rates to optimize their health safetyon the user's profile showing verification satus without protocols and even planningany medical details10314] (e) Users can filler potential matches based on Example 5: Poinis-to-Rewards Conversionoto sta [0340] A user accumulates points through regular testing[0315] If verification status changes or expires: 3 o oin i10316] (@) The system generates a cryptographically 20 redeem them for rewards:Signed revocation record [0341] The user maintains a quarterly testing streak for10317] (b) A webhook notification with the new status is One Year, accumulating:pushed to the dating platform [0342] (a) 200 base points (0x4 tests)10318] (¢) The platform updates the user's badge status [0343] (b) 100 bonus points (3-quarer streak)with appropriate visual indicators [0344] (¢) 150 bonus points (d-quarter streak)10319] (&) The change is logged in both systems without [0345] (d) 60 bonus points (2 pre-expiration tests)Jon we reson for revocation , N [0346] (¢) Total: 510 pointshroughout this process, no actual test results are So he rewards marketplace andever shared wit the ting patorm-only the bioary ves 10347] The user browses the rears marketplsce andfieation status and expiration date 10348] (2) Donation to AIDS Healthcare FoundationExample 3: Healthcare Provider Integration rescarch program (200 points)[0349] (b) One-month premium dating app subscription19321] A bese line implies direst negrtion (300 points)with the system: [0350] The system processes the redemption:er Barone pom? PTVHT [0381] @) Ponts are deducted from user balance10323] After receiving patient consent, th clinic submits 10352] (b) Verification codes are generatedfest verification directly through the API: [0353] (c) Rewards are delivered through partner APIs10324] (a) The clinic provides only test date and unique [0354] _(@) Transaction is recorded in the points ledgerpatient identifier reflecting a balance of 10 points10325] (b) No actual test results are transmitted [0355] The user recives notification of successful10326] (¢) The system employs a double-blind matching redemption with instructions for using each reward and theirProtocol where the patient identifier is hashed with a remaining balance.clinic-specific. salt then matched against similarlyhashed user identifiers Example 6: Mobile Application Implementation10327] (d) This cryplographic separation ensures the eaten eles mative mobile acsystem cannot corte denies without explicit con- er10328] () The verification is marked as “provider. SPeriencevero with enteneod trust level [0357] The mobile application implements secure camera10329] (1) The patent receives 7% pos instead of the 0iegmtion for streamlined document capture and upload.‘standard 50 points for user-verified (ests 10358] Client-side OCR processing performs preliminary[0330] Patents ean track ther testing history and streak 9413 extraction to minimize data transmissionProgress through the application, motivating return visit. 10359] (a) The application applies trained machine[0331] The clinic creates custom rewards redeemable learning models (0 identify seasitive data regions andthrough the marketplace, such as discounts on future testing explicitly filters out metadata that is not neededservices. [0360] (b) Only non-sensitive fields (date, provider[0332] Aggregate, de-identified testing frequency analyt- name, ete.) are extracted locally to be sent for server-ics help the clinic optimize their outreach programs and side processingtesting promotions. [0361] _(¢) Personal identifiers and test results are sys-tematically redacted before transmissionExample 4: Venue Verification Implementation [0362] (d) Hash-based content fingerprinting confirms[0333] An event venue implements the verification system document integrity without revealing contentfor calry minsgement [0363] The application includes built-in secure storage for[0334] The venue registers as an authorized verification verification status and achievement badges.partner through the Partner Portal. [0364] Push notifications alert users to upcoming verifi-10338] Users generate time-limited QR codes containing cation expirations and achievement opportunities.their verification status. [0365] The application provides QR code generation for[0336] Upon entry, venue staf scan QR codes to verify in-person verification at venues and eves.testing status without accessing any medical information. [0366] Biometric authentication (Face [D/Touch 1D)10337] The system performs verification checks. optionally secures access lo sensitive account functionsUS 2025/0267003 Al Aug. 21,202510Example 7: At-Home Testing Kit Integration identity, and uses zero-knowledge verification with behav-. iding at ho sing Kits inte. ioral reinforcement to maintain privacy.1067 omic riding some ST sin ism 05 WEL BEY wy tonnas103681" The at-home testing service registers as an autho- This centralized licensing system tracks digital contentTired provider through the Barner Port access without zero-knowledge proofs or privacy-preserving10369] The user orlers an at-home collection kit and 20aIVtes. V's designed for content licensing rather thanprovides consent for results 0 be shared with the verification D€4lth data protection.System during the ordering process. [0393] Zabar (2015, US 2015/0278824). While Zabar[0370] Aer receiving and processing the sample, the Or erential verification, is centralized platform erastesting service submits verification through the APT. ile poat of fle. Out tin enploys butedRie rf provides only fest date and rental data through a two-tier architecture UnlikeZabar, we use zero-knowledge verification without exposin10372] (b) No actual test resus are transmitted. ar, w knowledge verifica posing10373] (6) The system matches the Orderl> with the etal data and offer algorithmic ncentivization with streakars Bootie detection. Zabar's architecture would require fundamental10374] ©) The verlfiaion is marked as “provider: TSA 10 acne our ryiogmphic seprtion betwenverified” with enhanced trust level user and tes verification.[0375] (¢) The user receives 75 points as with other [0394] Felsher (2012, US. Pat. No. 8316.237): Thisrovider verified tests, three-party communication system relies on an intermediaryfor encryption parameters. Our invention eliminates escrowExample 8: Security Incident Response rien ad ey dncloms ins epi midis:tional verification through 7KVM-generated proofs that are0376] The system eftively handis potential sur gern unlnkible and revocablethreats: «actor atter eo verifi [0395] Johnson (2021, U.S. Pat. No. 10.972.275): WhileWATT] malicious actor attempts to manipulate verifia ofering biockehain-based verification, Johason's approach[0378] (a) The attacker attempts to modify a verification er ae enslin ed verification servicerecord by submitting altered test documentation i relies on 8 cemulized vera[0379] (b) The document analysis system detects incon- 0971 (b oS a pary Seem: "a a 10397) (®) it primarily addresses authentication rather10380] ©) Zero-Knowledge consistency checks identify than selective disclosure from multiple issuers;timestamp anomalies [0398] (©) it lacks temporal analysis and algorithmic[0381] (d) The system flags the attempt and blocks the incentivization wilh recovery acecleraton:Verification [0399] (d) it omits our multi-stage document analysis[0382] (e) A nomaly detection algorithms update to pipeline;recognize the specific pattern [0400] (€) it requires biometric binding versus our[0383] An authorized partner experiences a data breach: pseudonymous identifiers; and10384] (a) The partner notifies the system of potential [0401] (Dit lacks our two-tier database architecture thattoken compromise via the Partner Portal ensures cryptographic separation between identity and[0385] (bo) The system immediately invalidates all verification.authorization tokens for that partner [0402] Witchey (2019, US. Pat. No. 10340038): This10386] (¢) New signing keys are generated and distrib- healthcare blockehain creates chronological medicaluted records, but differs fundamentally from our invention in10387] (4) All affected users are notified via secure these four ways:channels [0403] (a) it stores actual health data rather than veri-10388] (e) Zero user data is compromised since partner fication metadata;tokens contain only pseudonymous identifiers [0404] (b) it prioritizes transparency over privacy:[0389] The implementation examples provided in para [0408] (c) it requires continuous blockchain synchroni-graphs [0141]-[0183] demonstrate the practical applications “ation versus our ephemeral proofs: undof the system across various contexts. These examples 0406] (@ it lacks om complete separation betweenillustrate how the technical approach described in his men: 1 gens ond vemearon ar oton effectively addresses th challenges oulined in PAF foqy7] psn (2007, US. Pat. No. 7.246067): This datingpls [00030007] while maintaining complete PAVIEY eration system centrally stores sensitive information,and security, even during edge cases and security incidents. oe; yagi eneryption, directly links verification to idearity,DETAILED COMPARISON WITH PRIOR ART lacks temporal analysis, and provides no inceniivization) h ‘mechanisms. Our invention liminates central stooge of10390] “The following analysis compares the invention sensitive data, uses zero-knowledge proofs, implementswith cited references to demonstrate novelty and non-obvi-  pseudonymous verification, and incorporates behavioral sci-ousness under 35 US.C. §§ 102 and 103: ence-based incentives.10391] Tee (2022, US. Pat. No. 11.238.454): This bio- [0408] Bartley (2014, US. Pat. No. 8.719.056) Whilemeric payment system permanently stores templates on offering health behavior rewards, Bartley lacks privacydevices and lacks privacy protections for sensitive contexts. preserving mechanisms, stores personal data, directly linksUnlike Lee, our invention implements zero-retention poli- identity to activities, and offers no data minimization. Ourcies for personal identifiers, separates health status from invention maintains cryplographic separation between iden-US 2025/0267003 Al Aug. 21,202511tty and verification, implements zero-retention policies. and REFERENCES CITEDcombines sophisticated streak detection with mathematically guaranteed privacy. _—[yr—10409) Out vention dress significant chnical gaps eee IIon No Tw 3 Lesthrough the integration of five components absent from US F4Ne LINAS hn mdeexisting systems: US a No foams dmeim Nga) US Fa Na 1030008 Ty 09 Wicks10410] (2) A zero-knowledge proof system for health (1 Xo isis Oca 215 darverification that processes test documents while reveal 13 ba No 870056 + Moy 204 Dey ctl.ing no protected health information US Fa Na $3637 Novmbera0i2 FlrUS bane Tater yor Awtmeral10411] (6) A behavioral incentive engine that analyzes ————————temporal testing pattems to encourage adherence whilemaintaining privacy: Non-Patent Literature10412] (6) A lightweight, mobile-firs architecture With [0419] Ferser, C. B. and Skinner, B. F. “Schedules ofon-device pre-processing and minimal data transmis- Reinforcement.” Appleton-Century-Crofts, 1957.sion 10420] Kissel. R.. Regenscheid, A., Scholl, M., and[0413] (d) Differential privacy implementation that Sine. X. or Special P ublication $0088 Revision. Bostres te data wility while mathematical delves for Media Sanitization * National Insitearameeing individual privacy and of Standards and Technology, Gaithersburg, MD, NIST>: Special Publication’ S00-88 Revision 1. December10414] (€) An cabanced OAuth frmesvork using pseud- 2014,onymous identifiers that separate verification status [0421] Centers for Disease Control and Preventionfrom personal identity: (CDC). “Tout Access to Sexual Health Services.”710415] This system vod several technical fple- BDFmentatons no found in prior art specialized AVM inte- ET Bei Bogration for verifiable proofs from dynamic documents; em- PO, oisporal analysis with unlinkability preservation: document (043% NU 1 Gus on pv 202preprocessing wilh security feature detetion: psewdony- [oa] 60 (EERE ob TIE BRmous sharing via cryptographic blinding and modular a Suvi Remon 2053.reward anchiteeture that incentivizes compliance While pre- 10425) "ECPM G. “Flectronic Portfolio of ternationalserving privacy. Credentials (EPIC) Verification Services.” 2023[0416] Tiven when considered in combination, the cited 1. A computer-implemented method for incentivizingreferences fil 0 teach or suggest our integrated appmach. health verification through o temporaly-sensitive digitalJohnson lacks temporal analysis capabilities; Barley lacks reward system, comprisingprivacy protections: Zabar wilh Felsher would sill require (2) generating, by a computing device, cryplographictrusted intermediaries; and our document analysis pipeline verification proofs for health verification events:represents an advancement nol suggested by any combina. (b) recording, by the computing device, the cryptographiction. The references collectively teach away from our verification proofs with timestamps in  socure digitalapproach—Witchey prioritizes transparency over privacy. ledger implemented as  privacy-preserving distributedFelsher relies on intermediaries, and Barley encourages database with immutable transaction records:broad data sharing. The technological disparities between (¢) analyzing. by the computing device, temporal patternsthese references would require substantial inventive effort to between sequential verification events by calculatingbridge, as evidenced by the absence of similar solutions in intervals between verifications;the intervening years. (d) awarding, by the computing device, digital assetssing parameterized algoritm thatO17) The rn inion pois il sion 02 PRI drtfor health verification that addresses the imitations of oct pbivegor tionexisting approaches. By implementing verification protocols gets verification sreaks by identifying consecutivethat confirm regular esting without revealing resus, apply- \erification events within defined time periods.ing cryplogmphic techniques that prevent OMation (i) awards defined milestone bonuses a specific streakbetween identity and health status, integrating behavioral hreholds; ascience principles to support testing adherence, and enabling roo ;ion by athorcd rots while paving (7) S0t gduted sency mnipis tt crevidual privacy. this invention offers a balanced approach to fare anateal verification. The technical (v) implements steak protection that maintains continuity10418] implementation described in paragraphs [0056] despite minor deviations {rom optimal verification[0140], combined with this comparative analysis, estab schedules: andTishes the invention's novelty and non-obviousness, as no (¢) validating, by the computing device, digital assetexisting systems teach or suggest al cluimed elements in transactions through cryptographic verification usingtheir specific configuration digital signatures and zero-knowledge proofs that pre-US 2025/0267003 Al Aug. 21,202512vent manipulation while maintaining privacy of the (a) receiving. by a computing device, health documentaunderlying health verification data tion through a secure transmission channel with client-2. The method of claim 1, wherein awarding digital assets side encryption and secure key management;comprises (b) processing. by the computing device, the health docu-(o) assigning differentiated base asset values where pro- ‘mentation through a multi-stage document analysisVider-verified events receive a higher value than user- pipeline thatverified events; ©) performs client-side preliminary extraction 10(b) awarding progressively increasing milestone bonuses reduce data transmission;at defined consecutive verification intervals; and (ii) applies server-side optical character recognition(c) applying graduated frequency multipliers that increase optimized for medical documents;‘as intervals between verifications decrease. (i) employs natural language processing models3. The method of claim 1, wherein implementing streak trained on medical documentation: andprotection comprises: (@v) wilizes entity classification algorithms to distin-(a) preserving sreak continuity when verification resumes guish verification. metadata from sensitive. healihwithin a defined grace period of the recommended information; .interval: (©) extracting, by the computing device, only verification(b) applying a configurable secondary grace period; and metadata comprising:(©) calculating recovery acceleration factors for resumed @) relevant ps prt oo oman denifier: andrico afer gaps using n apie temporal algo- (1) sing facility or provider denier; anrithm that weighs historical verification patterns, (iii) document type classification:4A method for maintaining covploatanbie separtion (filtering. by the computing device, al sensitive healthbetwoun usr Heniy and hal vedio sim, com Imation using classification algordms with igh“ “ : accuracy: andPy implementing, by a computing device, a two-tier (9) Securely deleting, by the computing devi, the orii-an deny er that sores usr quhentieaion 44 apis pipeline comprise:(iy a veifeation tir hat sore only prendomymous © tent side preprocessing tha identifies document (ypestores and extracts only necessary fields;identi, nd version status enoypted with ap) serverside processing th Weis et das, pr-second on key: vider information. and authenticity markers: and(0) generating by the computing device, a psevdomymots  ¢) client-side name matching to ensure document own-identifier by: ) ership before data transmission.sees we , he function to th 8. The method of claim 6, wherein filiering sensitiveii) applying a one-way hashing function to the User peal information comprisesidentity data with u server-side secret key and 0+ () applying trained machine leaming models o identifyes — sensitive data regions: and(i) string only the resuling pendonymous ientfier (sy ematically reacting personal identifiers and testin he veston er Wil 20 soci ine oe belore ramos!mite expiration: . 9A system for privacy-preserving health verification(9 nloring, by the computing devi, COSINE yi ial env, compingSeperton tel (a) a least one processor: and(0) strict separation between User IDs linked to actual 51 15 90° BOSOn 0hv . ry storing instructions that, when executed byidentity and Profile IDs usd as peudonymous dnt eat Teast ont rteosson cust the spate(@) sociation of veifcaion cords exclusively with © Sao verifeaion metadata from heal doceSendonymons proflelDs the than deny deta. faton sag mul age document oly pipelineets that performs client-side preliminary extraction,. A method for securely sharing health verification status applies. server-side opical characler recogiion.with third pares, comprising ilo natural language processing models, and(@) implementing, by a computing device, an OAuth utilizes entity classification algorithms:20-based authorization flow with user-controlled con- (ii) implement a two-tier database architecture com-ent iprising an identity tier and a verification tier with(6) generating. by the computing device, cryptographi- Cyptographic separator:cally signed, time-limited verification tokens that con- (ii) award digital assets using a parameterized algo-tain no personally identifiable information; rithm that assigns differentiated base asset values,(©) enabling. by the computing device, verification shar- detects verification streaks, awards milestoneing through multiple channels including. APL-bascd bonuses, applies graduated frequency multipliers.verification. QR code generation. and wallet pass inte- and implements streak protection;uration; and (iv) enable secure verification sharing through OAuth(@ maintaining cryptographic separation between User 20-based authorization flow, cryplographicallyIDs and Profile IDs to ensure verification data cannot Signed tokens, and multiple sharing channels:be linked back to individual users. (v) integrate the extraction, cryptographic separation.6. A method for privacy-preserving extraction of verifi- incentivization, and sharing subsystems while maincation metadata from health documentation, comprising: taining functional separation: andUS 2025/0267003 Al Aug. 21,202513(vi) implement error handling protocols that maintainsystem integrity during failures.10. A non-transitory computer-readable medium storing.instructions that, when executed by at least ane processor,cause the processor to perform a method comprising:(@) extracting verification metadata from health documen-tation using a multi-stage document analysis pipelinethat fillers sensitive information while preserving veri-fication metadata;(b) creating a cryptographic separation between useridentity and verification status by generating a pseud-onymous identifier through one-way hashing of useridentity data with a server-side secret key and crypto-graphic salt; and(©) awarding digital assets based on temporal patternsbetween verification events using a parameterized algo-rithm that implements verification streaks and gradu-ated reward multipliers with logarithmic scaling tomaximize long-term engagement.