---layout: posttitle: "hello world"date: 2025-05-13 12:00:00 -0500categories: [general, ai, startups, personal]attribution: human---## introduction to this blogI go by 97 115 104 and this is my personal blog site. It contains arbitrary thoughts on various topics and information about things I build. While I don't profess to be an oracle, I do hope there's some helpful insight in my sharing of what I've learned that would be useful for others. If it is indeed useful or not is up to *you* as The Reader.## about meI'm a [techie](https://en.wiktionary.org/wiki/techie) located in San Francisco, currently in my early 30s. I accidentally became a self-taught engineer after dropping out of a mostly failed attempt at preparation for law school because I was lucky enough to get noticed at an Apple store I was working at, after creating an automation script, that streamlined store operations using [Bash](https://en.wikipedia.org/wiki/Bash_(Unix_shell)). Apple liked it enough to hire me at 19 with no degree or previous experience as an engineer and moved me to Cupertino with a sign-on bonus. I was there for a few months before deciding it wasn't for me. Thankfully they didn't ask for the bonus back. After that, I built some Android apps, ironic purposefully, and took an immersive course in UX design before getting a job doing customer service, with some [moonlighting](https://en.wikipedia.org/wiki/Moonlighting) as a developer, at a place called IDEO. I was there for a bit before getting a job at GitHub, where I was long enough to see the Microsoft acquisition. I left GitHub to join the [Web3](https://en.wikipedia.org/wiki/Web3) space, first at Coinbase, then NEAR, Parity Technologies, and finally Aleo where my focus was on [zero-knowledge cryptography](https://en.wikipedia.org/wiki/Zero_knowledge). Probably thanks to my Mom, people generally tend to like me so I progressed through my career in a vertical-and-up fashion, from engineering to recruiting, to program management, then product management. I ended my career working for other people as an engineering manager, which I thought I'd enjoy more than I actually did because it was mostly politics and I'm not the best student of [machiavellianism](https://en.wikipedia.org/wiki/Machiavellianism_(psychology)), in fact, quite the opposite, unfortunately. Now, thanks to a confluence of events, a good idea, and too many STIs, I am the founder of a startup called [status.health](https://status.health) focused on improving sexual wellness and reducing STI transmission rates.## ai tools and meI am an AI tools maxi and believe, strongly, that they help more than they hurt. Though, for some reason, I find mine an unpopular opinion among peers. When I reflect on it, I am reminded of the early days of every groundbreaking emerging technology: the industrial revolution, the telegram, cameras, phones, cell phones, the internet, social media, crypto, and now AI. EVERY SINGLE TIME it scares people. Luckily, I don't scare easily and am often eager to learn something new.AI tools are nowhere near as good as humans at any particular task--they are "generally" intelligent but not expert at anything regardless of how good you are at prompting them. They are much cheaper and easier to delegate to than an employee in the context of a startup or as a business owner. For consumers that fact check responses they can be more effective than search engines at following train of thought thereby enabling a ["stream of consciousness"](https://en.wikipedia.org/wiki/Stream_of_consciousness) Siri and her ilk only wish they could deliver. As such, AI tools have their place in growing a startup, helping with learning, and other things that require specialized knowledge or a huge learning curve: programming, complex math, a new language--you name it. AI is terrible at being creative. Probably not news to most of you reading, but worth reiterating as I continue to find it annoying since I am not the best "creative" either. When making assets for products I build, it's often the hardest task there is; I guess job security for the designers, artists, and writers out there. Congratulations.I feel strongly that everyone should and probably will eventually use AI, but I also believe in being clear about when it was used. Why not just say:> ❔"I created X with an initial prompt and multiple iterations based on research. After some back-and-forth, Claude and I got X to where it is now and I am proud of it. I probably don't know *everything* about it but I know most of it and can answer any questions you have."Not attesting to the way X deliverable was created only impacts your reputation negatively when you inevitably get stumped answering a question about something outside your depth. It's like a teacher calling on you in English class after you chose to spend the night playing World of Warcraft instead of reading "The Scarlet Letter." You should have just read the thing, but short of that, letting your teacher know you played WoW instead and plan to listen to the other students' answers to learn that way, is the next best thing.In short: AI-assisted anything should be attested to (clearly marked), and so should human-generated content because it addresses two key concerns most people have: 1. a person misrepresenting work and not actually knowing what a deliverable is or does, and 2. AI's reputation for being a human-killer vs. a human-enabler, except, of course, for the creatives. Assigning credit where credit is due is not new. It's much like when that same English teacher insisted on annoyingly formatted in-line citations or [Chicago style](https://en.wikipedia.org/wiki/The_Chicago_Manual_of_Style) for book reports.To make attesting to AI-assisted work vs. that of human work easier for myself and my businesses, Claude and I created a site called [attest.ink](https://attest.ink) which allows automated badge generation. I use attest.ink and its [attestations](https://en.wikipedia.org/wiki/Attestation) for almost everything I produce that has a public eye on it, and you should too. It's free! Check it out and let me know if you have any questions or suggested improvements: [info@attest.ink](mailto:info@attest.ink).#### what badge attestations look like<!-- Add this to your blog HTML --><a href="https://attest.ink" target="_blank" class="badge-link" rel="noopener">  <img src="https://attest.ink/assets/badges/human-generated.svg" alt="Human Generated" width="120" height="30">  <img src="https://attest.ink/assets/badges/chatgpt-generated.svg" alt="AI Assisted" width="120" height="30">  <img src="https://attest.ink/assets/badges/claude-generated.svg" alt="AI Assisted" width="120" height="30">  <img src="https://attest.ink/assets/badges/gemini-generated.svg" alt="AI Assisted" width="120" height="30">  <img src="https://attest.ink/assets/badges/midjourney-generated.svg" alt="AI Assisted" width="120" height="30">  <img src="https://attest.ink/assets/badges/dalle-generated.svg" alt="AI Assisted" width="120" height="30"></a>#### for those who think one can't determine if content is ai-generatedYou are wrong because, thankfully, there are a growing number of tools out there that can do. While they mostly use heuristics (educated guesses based on data), they do help "out" content where an individual is trying to claim something was generated without AI assistance. For those moments when it feels like misrepresentation might be afoot, or if you are simply curious, I found [Copyleaks's AI detector](https://copyleaks.com/ai-content-detector) the most useful—it even has a [Chrome extension](https://copyleaks.com/ai-content-detector/extension). So if you are the person being disingenuous, sorry not sorry.## ai and this blog siteBlog posts on this site are usually not AI-assisted other than for grammar and spelling checks because I am terrible at both. Blog content that is marked as human-generated is original other than that content, ideas, and interpretations are based on my analysis of other people's works, thoughts I've had based on conversations with friends and family, and general content consumption--we are, after all, a product of our environment. Not dissimilar from how AI is a product of the data it has access to. While I could easily use Claude to write every post though ChatGPT is better, I think it's more valuable to write them myself. Doing so serves as a good reminder of the time it takes to make something without AI-assistance, and it's fun despite how grueling I find long-form writing. I will admit that I am also creating this blog and a vast majority of its posts so I can train the various AIs on my writing style, voice, and humor, since they are currently woefully bad at both due to the lack of data out there representing me. When a blog post is AI-assited, you'll know via the attestation badge. The non-blog content on this site such as the site itself, *was* created with AI assistance because Claude is a faster engineer than I am (though not better ). That said, you can take solace in the fact that you are reading my thoughts, unadulterated as they may be. Look out for the "Human Generated" or "AI-Assisted" badge for clarity.## mostly open sourceFor the most part everything I create is under the typical open source [MIT License](https://opensource.org/licenses/MIT). You should feel free to use my words, my website content, and thoughts, I just ask for proper citation, e.g., `Harshberger, A, $DATE, $BLOG-POST-NAME` or `attest.ink, A. H. $DATE`. The exception to this rule is anything over at status.health or where I explicitly mark something under copyright with ©. In those instances, just shoot me an email for permission or with any questions: [x@97115104.com](mailto:x@97115104.com).

---

---layout: posttitle: "i'm probably going to launch a new cryptocurrency in addition to status.health..."date: 2025-05-31 02:00:00 -0500categories: [crypto, privacy, health, web3, zk, startups]attribution: ai claude opus 4---## nobody else is going to fix this mess for usIf you read my [hello world post](https://97115104.com/2025/05/13/hello-world/), you already know I’m a techie, but you can probably also guess I’m not in this for tech’s sake—and I’m even less of a fan of breathless “we’re disrupting X!” manifestos. In a world where even Trump has a meme coin, the bar for “launching a token” is officially on the floor. So, here’s where I stand: I’m probably going to launch a new cryptocurrency—not because I want my face on a billboard (actually that would freak me out), but because after years of wrestling with health tech, incentives, and privacy, I keep running into the same broken machinery and nobody else seems interested in fixing it.Lately, this has gotten a lot more personal. I was fired for not being a “cultural fit”—which, in plain English, meant I wouldn’t work weekends for my boss. I just didn’t care enough at the time, but am now writing this at 2 a.m. on a Saturday—#ironic #iconic. Anyways, since going independent, I’ve landed squarely in the same healthcare mess as countless freelancers, gig workers, and marginalized folks: sky-high costs, awful coverage, and almost no meaningful support for prevention. Throw in my own ongoing battles with STIs (yes, plural), and I’ve had a front-row seat to just how neglected and underfunded prevention really is.The big “solutioneers”—Kaiser, the government, insurance barons—are either too massive to move, too tangled up in lobbyist interests, or fundamentally structured to maintain the status quo. Insurance providers, especially, have incentives that run completely counter to most people’s real needs. So here I am: personal prevention meets a funding crisis—a weirdly perfect storm for someone stubborn enough (and just well-placed enough) to try building something new. I’m not here for vanity or hype (eww!). I’m here because the system’s busted, the usual players aren’t stepping up, and, for better or worse, I might just be in the right spot to do something about it.## why? (seriously, why?)Short version: the incentives in healthcare are upside down, and no amount of dashboards, SaaS contracts, or “secure” portals are going to fix that. If you’ve ever tried to prove you got tested, vaccinated, or did literally anything responsible, you know the drill: fill out a form, log into yet another portal, get a PDF for your trouble, and hope it doesn’t end up in some vendor’s next data breach. If you actually want to be rewarded for doing the right thing? Good luck. The system pays when you get sick, not when you stay healthy.Meanwhile, prevention is a cost center, not a business model. Nobody wants to fund it, even though [it’s the only thing that actually works](https://odphp.health.gov/news/202401/prevention-still-best-medicine) in the long run. The few times you are “incentivized,” it’s either a spammy sweepstakes or a $10 Amazon card—never enough to matter, never enough to make it feel like your effort is valued. And through it all, you’re handing over more data than you’re comfortable with, for less and less in return.Most people shrug and move on. I apparently have a higher tolerance for banging my head against the wall than most, so here we are.## what the hell would this even look like?I don’t want to build another “health app.” I want to build a protocol—a new set of rails that flips the incentives, rewards prevention, and makes privacy the default, not the exception. Imagine being able to prove you did the right thing (got tested, got a vaccine, whatever) without having to share your entire life story, or trust yet another startup with your data. Imagine a system where healthy behavior is actually worth something, and where the risk of participation is less than the risk of doing nothing.If you’re rolling your eyes, I get it. I’m skeptical too. But I’d rather try and fail than keep waiting for someone else to get it right.## incentives: for humans, not robotsLet’s be honest: most “health incentives” are either so trivial they’re meaningless, or so convoluted you need to be an actuary to figure them out. I want to design something that works with how actual people behave—not how some product manager thinks they *should* behave.Here’s what that means in practice: You get a tangible, real reward for verified healthy actions—not just a pat on the back. If you show up for regular testing or take other preventive action, you accrue value, not “points” that expire or can only be redeemed for branded swag. If you miss a test, the system forgives you up to a point; streaks and bonuses decay gently, not instantly, because life happens and the protocol recognizes that. Incentives are transparent and non-punitive: you can see exactly how rewards are calculated, and the most value accrues to those who are consistent, not those who can game the system. Points accrue for everyone, regardless of whether you know how to use a crypto wallet. If you want to stay in normie mode, fine—your rewards are just as real, and you can convert them to tokens whenever you want. There are no casino vibes, no “spin the wheel,” no loss aversion tricks—just a persistent, steady nudge toward better habits.The reward structure isn’t just “do X, get Y.” It’s a system that recognizes effort and continuity, not just perfection. Miss a test window because you had a rough month? You don’t get the digital equivalent of a dunce cap. Instead, your rewards taper off—gently. I want this system to actually *work* for real people, not just people who never miss a checkup or who have a spreadsheet for their wellness routine. And this isn’t just about individuals—if organizations, clinics, or even entire communities want to participate, there’s a pathway for that too (with proportional reward, without compromising privacy). The upside is that the system gets more robust as more entities join, and the incentives don’t get diluted into oblivion just because more people are doing the right thing. If anything, it’s the opposite: the more people and orgs that participate, the more sustainable and valuable the entire protocol becomes.## utility: not a speculative DeFi coin$HEALTH is a utility token, not a speculative asset. You won’t find it on DeFi exchanges or liquidity pools at launch, and there won’t be any token trading, yield farming, or pump-and-dump opportunities (sorry, not sorry). The entire point is to provide token infrastructure at the application layer—enabling products like [status.health](https://status.health) to reward prevention and healthy behavior, support governance, and fund public health via community grants. It isn’t meant to be a “number go up” coin or a vehicle for speculation. The goal is to create a circular economy where healthy actions are directly rewarded, and community health initiatives can be sustainably funded. If your main question is “wen Binance?” you’re definitely in the wrong place.What does that utility actually look like? In practice, $HEALTH tokens are earned by taking real preventive health actions—like regular testing or verification—through apps such as status.health. You can use those tokens to unlock premium features, participate in protocol governance, stake to help secure the network, or support local health initiatives via dontations. Most of this activity happens directly within the apps themselves, not on external exchanges, and is focused on rewarding participation and fueling public goods. As more partners and integrations come online, $HEALTH will serve as the backbone for new health-focused tools, transparent incentives, and grants—all designed to keep the ecosystem aligned around real-world outcomes, not speculation.## governance: decentralize only when it makes senseLet’s talk about decentralization. I’ve been around enough “DAOs” to know that most of them are just group chats with a multisig and a lot of vibes. Decentralization is great—*when there’s actually something to decentralize*. Until then, it’s a distraction.My approach is pretty simple: $HEALTH starts out centralized because, frankly, someone has to do the work and make the decisions. That means I’m responsible for the roadmap, the bugs, the (hopefully rare) drama, and the general direction. But here’s the difference: as *actual* usage grows, as the protocol matures, and as more real people and organizations rely on it, governance and control will transfer—gradually, and transparently—to the community.There are clear, metric-based thresholds for when governance hands off—these aren’t arbitrary, and you’ll be able to see them, track progress, and know what’s coming. Governance isn’t just about voting on proposals; it’s about funding public goods, supporting integrations, and making sure incentives stay aligned as the ecosystem evolves. Quadratic funding will ensure that many small contributors have more power than a few whales. If usage drops or the protocol gets gamed, decentralization slows or pauses—there’s no “one-way door” to chaos, and no DAO just for the sake of optics. The rules, formulas, and transition points are all public, so if you want to critique them, propose something better, or just follow along, you’ll have every tool you need. I’m committed to keeping the governance process as transparent as possible: you’ll know who’s making decisions, why they’re being made, and how to get involved if you care enough to show up. And, real talk—if nobody uses the thing, or if it turns into another DAO meme, I’d rather pull the plug than pretend it’s “community-led.” Governance is earned, not declared.Here’s a thing I’ve learned: persistent skeptics and critics are actually invaluable. People who don’t just clap along but keep showing up with feedback (good or brutal)—those are the folks I want in governance. In fact, if you keep at it long enough, you might just end up on my “persistent people to hire” list. You know who you are.I’m not interested in “community theater.” I want real, earned decentralization, with skin in the game and accountability. If that sounds too slow or not “Web3 enough” for you, there are plenty of Discords out there waiting for new mods.## is it open source?This is where I usually get hate mail from the free software crowd, so let me be clear: I love open source. Most of what I make is MIT-licensed, and I strongly believe in sharing tools that help people build. But with $HEALTH, I’m being careful. Some things will be public from the start: the [yellowpaper](https://healthprotocol.network/yp), the math, all incentive formulas, and selected SDKs and reference components. I want people to be able to audit the core mechanisms, see how rewards are calculated, and follow the logic behind the scenes.But not everything will be open—at least not right away. Why? Sometimes, it’s about protecting security while things are new and fragile. Sometimes, it’s about giving the project a chance to mature before it gets forked into oblivion or spammed by bots. Sometimes, it’s just that I’m still working out the kinks and don’t want to ship garbage into the wild. If you have a legit reason to ask for access, reach out and make your case. I’m open to collaboration and feedback, especially from folks who want to make the protocol better (or who spot problems I missed).If you want to see what’s public, check out [github.com/97115104](https://github.com/97115104) for my stuff and [github.com/statusdothealth](https://github.com/statusdothealth) for the status.health side. If it’s not there, it’s not open (yet or ever). If that’s a deal breaker for you, I get it. But I’d rather build something that works and is safe before opening every door and letting the whole world poke holes in it. If you want to be part of that process, you know where to find me.## what's next? (the real roadmap, for better or worse)Here’s where I stand: this is still an experiment. Patent’s pending (because lawyers need to eat), and now that both [status.health](https://status.health/) and [healthprotocol.network](https://healthprotocol.network/) are live (plus the yellowpaper is finally finished), I can actually start building the real MVP. Everything is still fragile and changing fast. For anyone actually curious about what’s ahead, here’s the roadmap:1. **Building the MVP for real-world use (through status.health):** Since I run both status.health and the $HEALTH Protocol, the first version of the MVP will launch through [status.health](https://status.health/)—but only for those who sign up for the beta ([sign up here](https://form.typeform.com/to/Ii3HSlEH?typeform-source=status.health)). Making it generally accessible will take real money and a lot of development time, so don’t expect it overnight. This isn’t just a demo for crypto insiders—the goal is a product where privacy and incentives work for normal humans, not just blockchain hobbyists.2. **Applying for grants:** Server bills aren’t paid in tokens (yet), so I’ll be applying for grants to help with ongoing development and infrastructure costs. The plan is to work with protocol foundations like the Ethereum Foundation, Web3 Foundation, and similar organizations, as well as explore on-chain treasuries and ecosystem funds that support public goods and experimental projects in the crypto space.3. **status.health as first customer:** I’m working on status.health and the $HEALTH Protocol concurrently—status.health is the first customer of the $HEALTH Protocol, and hopefully not the last. The idea is to prove the infrastructure actually solves a real problem. Over time, I want to see many more apps and orgs using $HEALTH’s rails, powered by open APIs and community grants.4. **Governance and analytics:** As the protocol grows, transparency grows too. Usage, impact, funding flows, and governance transitions will all be visible in real time. No hiding behind “coming soon”—if it’s working (or failing), everyone will see it.5. **Full decentralization (if earned):** If this thing gains real traction, governance and control will move out of my hands and into the community’s, with clear, published thresholds and rules. Not by vibes, but by actual adoption and contribution. If it doesn’t earn that, I’m not going to pretend otherwise.6. **Iteration and humility:** If the experiment blows up in my face, I’ll say so. If something’s not working, I’ll pivot or kill it. If you have feedback, ideas, or want to get involved, I’m open. If you want to sit on the sidelines and throw tomatoes, that’s fine too—I’ve got a thick skin.The bottom line: I’m not building this for a quick exit, a headline, or a token pump. I’m building it because I want a world where prevention and privacy aren’t afterthoughts. If that sounds interesting, follow along, contribute, or just lurk and watch.If you want the technical receipts, actual mechanics, or just want to see how weird my sense of humor gets when writing about cryptoeconomics, the [yellowpaper is here](https://healthprotocol.network/yp). If you’re feeling especially masochistic, you can also read [the patent](https://0x42r.io/patents/1.pdf)—which is way more painful to read (and was to write).

---

---layout: posttitle: "my first failure as a startup founder"date: 2025-06-06 18:00:00 -0500categories: [startups, personal]attribution: human---## the wrong co-founderMy first real mistake as a founder wasn’t about product or tech, although both did change from what they were originally. It was bringing in a friend—who was also my ex—to help build the company. I thought splitting the responsibility of running things would make it lighter, like a feather. I thought trust was enough. It isn’t. It’s like the weight that hits [Wile E. Coyote](https://en.wikipedia.org/wiki/Wile_E._Coyote_and_the_Road_Runner)—you can’t run your way out of it. You need more than history. You need someone who shows up every day, who wants to, and who can. Even if things are going swimmingly, there’s likely a time when they won’t, and when personalities are really under the pressure of potentially seven figures, or eight, or ten, or none—who knows. This might sound pessimistic, but it’s not. It’s a lesson in optimism and learning. You get better by going through it.## what I missedWe never defined the work other than at a high level: “you take marketing and finance and corporate governance bullshit, I’ll take engineering, product, and share some of the marketing.” We didn’t decide how decisions would get made other than “we’ll always run things by each other,” or agree that building a real company is full-time—like, seriously, 3 AM every night and no Cinco de Mayo parties because the patent. Startups don’t wait for you to catch up. We should have put it in writing, apart from our Founders Agreement. We should have actually weighed and measured the Founders Agreement, not signed it on our mobile devices on a whim thinking it would never *really* matter. I should have said what I wanted. He should have said what he wanted. We should have talked about our “fuck you numbers.” It was my first time doing this without a team, or a big company’s money, or someone else’s rules, so I can’t blame myself. I can’t blame him. Or the situation, or really anything. I can just learn, and share my experience so others don’t, hopefully, repeat the same errors in their founder journey.## what he broughtHe was ambitious and hungry. He wanted to be CEO. We actually fought about who would be CEO in the end. He wanted to be the one walking into the room as the leader of a big company, the one selling it to people, the one in front. He had what I didn’t—the desire to be seen, to be admired. I prefer to do things in the background and admire my work from afar. He helped with incorporating the company, and writing (because I am obviously bad at writing). He was great at scheduling things, talking with potential customers and investors off the clock, and getting folks excited. But he just didn’t have enough time. He wasn’t willing to take the same risk, mostly because he isn’t as privileged as I am and isn’t willing to lose it all, which I get. But that’s what it takes. I now have to be a different person, and for that I am thankful, because I am stronger. But it was painful to lose him—he mattered.## when it turned[The patent](https://0x42r.io/patents/1.pdf). By the time it was ready for another person to review, it was like 60 pages. I had written it, read it, and iterated on it at least a hundred times—or that’s what it felt like—by the time it was ready to be submitted. All that needed to happen was his eyes, confirmation, and a plus-one. But that was also the same day as Cinco de Mayo, and the fun was enticing for us both. I didn’t buckle and submitted that night—he went for a marg. It was the beginning of the end for our co-founder-ness.Around that same time, I pitched a licensing deal where I would own the patent and the company would lease it for free—unless I was no longer at the company, at which point it would have to pay for it. He said it wouldn’t be good for the company. What I found out later was that he started second-guessing my motives. I get it. What was really happening is that my nine-hour days felt worth more than the 50% of the company I owned, and if I was putting in all the real work, I wanted a bigger upside. In reality, I pitched that as a cry for his attention, to help course correct, and hopefully get more “work” for the company out of it.My error here was missing that he just didn’t have the time. He was working three other jobs after all. I was naive. I could have been kinder, nicer, less aggressive even. He mentioned some of the things I said were emotionally abusive. This was hard for me to hear, but he’s probably right in some ways. I can be mean when I am tired, hungry, and at the end of my proverbial leash. I needed him to help correct me and bring me back down to normal. He was my opposite after all. I didn’t realize that it was unrealistic to expect this from a business partner—and maybe even from a friend. I have to regulate myself, and make sure I take time to garden my body and my brain, not just the business. That lesson is one I’ll take with me forever, and I thank him for that.## how it endedThe real ending was in Dolores Park. We sat and talked a bit, both of us nervous—well, I was nervous, he was probably just mad, because it was nothing like when we first met at that club on his 23rd or 24th birthday, I forget which. In some ways, it felt like the last time we’d ever meet. The conversation ended with me offering that dinner I owed him for helping me sell some jewelry I needed to fund my lifestyle and the startup (that damn patent was expensive). He agreed to the dinner, after which he walked home, looking a little lighter in his step free of me and the startup, to the puppy we used to share and his beautiful boyfriend I sometimes wish was still me. I took some solace in the idea that we were still friends, but now, maybe we aren’t? Ugh, running a company is hard. Don’t do it with your friends, ex-lovers, or family. Be warned.## what founders really need to knowWhen you start out, you think you know how hard it’s going to be. I didn’t. The real test isn’t the product, or the legal forms, or the late nights—it’s everything that happens to you, and who you become in the process. I used to do ops and product, and engineering, and recruiting and everything. I thought I knew it all. In reality, it couldn’t be more different. Paying taxes, statements of information, articles of incorporation, a million contracts, boilerplate that isn’t code—the whole thing is dizzying. I was hoping he’d take this on, but I now see the value in knowing how to do it all myself, and I see the new me in it. I read every single line of a contract, the bylaws, the license agreement for the patent, and you should too. It’s what dictates the next 10+ years of your life as a founder, and maybe more if you believe the folks in ["Founder vs. Investor."](https://www.foundervsinvestor.com/)You don’t see it until you do, and it is worth it even though it doesn't feel like it. Or at least I hope it is.## for the recordThinking about it more, I don’t actually know if any of this was worth the cost. Losing a friend, especially an ex, never feels worth it. If I could do it again, I’d have shared my needs as a founder. I’d have explained that I am not the same person in business as I am in our friendship, or as I was in our relationship. I am somehow much stronger, but also much weaker in other ways. Please, *dear reader*, take note if you end up working with a friend or ex or family member: make the clear distinction between the *you* that is a business person, the *you* that is a friend, and the *you* that is in the clubs or when you see a cute boy, because they are all versions of you that show up at different times, and you better bet they haven’t seen the business you yet.Some days, the work is worth every cost. Other days, it isn’t. I’m still here, still learning, and still doing the job, and I plan to until this thing really does [reduce STI transmission rates](https://status.health/) and for me, that’s enough.## letter to you, and to meYou saw something in this before I did.  You helped turn it from idea to company.  You called out my blind spots.  You pulled me back in when I wanted out.  I wanted us both all-in, building something that mattered.  We didn’t get there.I hope your part of this means something—whether or not we ever talk again.  I hope you know you mattered, even when things fell apart.To myself:  Keep building.  Remember where you started.  Don’t lose more than you have to.  If it works, let it mean something for both of us.

---

---layout: posttitle: "hodl? lol. not here."date: 2025-06-25 01:30:00 PTcategories: [crypto, privacy, health, web3, zk, startups]attribution: ai chatgpt 4.1---## why i killed my own crypto project (and built something that could actually live)I built Health Protocol thinking if you gave people privacy, incentives, and proof, everything would just work. I shipped the infra and wrote the math with Claude's help. I modeled every outcome using [Monte Carlo sims](https://en.wikipedia.org/wiki/Monte_Carlo_method). Every time, the same ending. Whales won, bots drained the pool, real users quit. The DAO was a graveyard. Nothing felt new. It was the same fate as every corporate wellness app, insurance "incentive," and NFT move-to-earn. [STEPN](https://www.reddit.com/r/CryptoCurrency/comments/11qaopr/the_end_of_stepn_and_why_public_are_confused/) made you buy $1,200 sneakers for a shot at a token jackpot. By the end, bots were farming, prices tanked, and most users left holding nothing but another Discord full of bad memes. I did the research. I looked at everything: DeFi, behavioral econ, read [Richard Dawkins](https://en.wikipedia.org/wiki/Richard_Dawkins) and thought about how casinos and governments try to buy good behavior. None of them solved the core problem: how do you reward people for what actually matters and keep privacy as a non-negotiable? The old Health Protocol yellowpaper documented exactly how it would fail.## arishem, eternals, and building for life, not stasisThe moment I gave up on the old protocol was sometime after midnight, Marvel's Eternals on in the background, Arishem explaining why universes are born to die and recycle. It clicked. The problem wasn't just incentives, it was immortality: THE VAMPIRE PARADOX! If you build for stasis, the system probably gets gamed and dies a boring death. If you build for life with decay, adaptation, feedback, etc, then it turns out you get a protocol that might actually live forever. [Arishem](https://marvelcinematicuniverse.fandom.com/wiki/Arishem): cosmic midwife, blockchain muse.## value that vanishes, and the birth of the reservoirEvery protocol I looked at was built on the same dead logic: make a token, freeze value, pray nobody dumps. That doesn't work. Real biological systems move. In Autophage, tokens decay. You can't save indefinitely though Wellness Vaults (think HSAs but on-chain) are a thing. You either have to use tokens you earn right away or set them aside for specific goals like how college funds and HSAs work, and if you ghost, your balance fades out.The first person who called this out as bullshit was my mom. She said, "money isn't supposed to disappear." I tried to explain decay, game theory, and why hoarding ruins incentives. She said, "let it all flow back to everyone else, like a reservoir." She was right so that's exactly what I built; the missing piece. **The Reservoir** collects all decayed tokens and recycles them as system fuel. It pays future rewards, covers healthcare payouts, and backs the protocol's safety net. My mom named it, shout-out to you Momma!So you earn tokens, those tokens decay at different rates depending on how you earned them, i.e., through what exercise or activity. You can spend them lots of ways, but if you don't they simply return to The Reservoir, funding other people's health emergencies or yours. The Reservoir remembers 100% of the value you contribute through token decay and works like how "social security" is supposed to, except it doesn't run out and is USDC backed through platform and marketplace fees. ## game the system, then evolveYou earn tokens as mentioned before through health-related activities that include lots of things like sleeping, running, ab workouts, therapy session, STI checks, etc, really any health activity you can design a verification proof off of. Those activities are private to you and they should be, so the system implements a proof generation system. This gets complicated, but the gist is you access an App on the network like a theoretical status.health for STI test history verification, that App then generates a proof you did a thing, and sends it to the network. The network then pays you for your proof and stores it under your ProfileID which is cryptographically separated from your identity.  Proofs that are generated are not just trophies, you can actually turn them into products you sell, and even cooler, the protocol expects you to game it. Thanks to my buddy Dakota, who explained why [Path of Exile](https://www.pathofexile.com/) works: you can always evolve your build and break it in new ways, and it stays fair. That's how genetic adaptation landed in Autophage. Burn tokens, unlock new traits, specialize. If you're a marathon runner, show your trait stack and earn multiples on running activities. If you care about privacy, share less, and still get paid. The more you adapt, the better your rewards. Apps and businesses pay to issue, verify, and use the protocol. Users just access and build reputation. All fees are in USDC. No protocol token games. If you spam, you get cut out. If you contribute, you get paid.The protocol uses zkVM orchestration for proof verification - basically allowing apps to verify you did something health-related without ever seeing your actual data.## onlyfans for proofThe protocol flips health data ownership just like OnlyFans did for the porn industry. Not only can apps help you verify activity, but they can also help you sell health proofs you generate through the marketplace. Buyers could include individuals or businesses for things like medical research or post-hookup should a partner get anxious about your testing history. Anonymous marathon proof sells for $10-20 baseline. Reveal your ProfileID so buyers can follow your journey, earn 50-100% more. Show customizable traits your spec'd in proving you're an endurance specialist, command $50+ per proof. Therapy consistency, workout streaks, STI tests, sleep quality, whatever you track becomes a potential product you can benefit from.The marketplace has three layers. Individuals buy proofs from each other to study real health journeys or verify fitness, kind of like peacocks and their feathers. Someone training for their first marathon buys experienced runners' proofs to understand pacing. Dating apps verify STI status without storing medical records. Research organizations purchase anonymous population data at scale. A university studying exercise patterns might buy 10,000 anonymous running proofs for $50,000. Every transaction protects privacy through zero-knowledge separation. Buyers get verification without identity. Sellers get paid without exposure.You control the terms. List anonymous and stay completely private. Build a following around your ProfileID without revealing your name. Display specialized traits when expertise pays. The protocol takes 12% to fund The Reservoir. Apps that verified your activity get 5%. You keep 88% in USDC. Cash for health receipts. Privacy settings determine price. The market decides value. You own what you earn.## proposals and upgrades as a real jobThe protocol treats development like a real market. You can ship any improvement that makes the system better. New proof mechanisms for different health activities, stronger privacy circuits, reputation algorithms that catch gaming, incentive curves that balance rewards. When apps adopt your code and it processes real health data, you capture value through integration fees, performance bounties, usage royalties, and security rewards. Early simulations show active developers earning $30-90k annually just from base activity, but that's conservative math. The real opportunity is building critical infrastructure that thousands of apps depend on, like npm packages that print money. No voting, no committees, no "community calls" where nothing happens. Pure market dynamics: your code gets used, you get paid. It doesn't, you don't. The protocol needs everything from better zkVM orchestration to ML models for health insights to integration bridges for existing systems. Each improvement creates compound value and compound earnings. It's an economic system where protocol improvements translate directly to developer income indefinitely as the network scales. Anyone can propose improvements by staking 100-5,000 tokens on specific health metrics. The protocol runs controlled experiments for 30-180 days. Hit your 5% improvement target and get your stake back plus USDC bonuses proportional to impact. Miss and The Reservoir takes your tokens. Good improvers build reputation, stake more, run bigger experiments. Stack wins and you've got a real career improving public health infrastructure while getting paid in real money. The network adapts through these continuous experiments, evolving based on what actually works.## the usdc loopApps pay $0.10-0.20 monthly per user for verification rails. Enterprises drop $20-100 per employee wellness check. Users sell health proofs to each other with the marketplace taking 12%. All USDC flows to The Reservoir. The split: 10% for protocol development, 90% for healthcare settlements and reserves. Users earn USDC selling anonymous marathon proofs for 10 bucks or specialized genetic traits for 30. Apps compete on features while paying base infrastructure fees. Healthcare providers submit tokens and get settled in USDC at metabolic prices (essentially dynamic pricing based on the actual metabolic cost of healthcare activities). Developer improvements that get adopted earn integration fees and usage royalties. More network activity creates more fees, deeper reserves, better healthcare coverage. Your lifetime token contributions track your healthcare access rights. Decay funds the community, fees fund the infrastructure, proofs fund the users. Everything cycles through The Reservoir keeping value moving, never pooling.## receipts, math, and where to break thingsThe yellowpaper contains all the real math - every incentive curve, privacy proof, and system invariant. There's also a patent filed covering the core mechanics. The protocol documentation includes code snippets, use cases, and simulation frameworks. If you can break it, send through a report to info@autophage.io.## sims don't lieThe Health Protocol died every time in Monte Carlo simulations. Whales accumulated or bots farmed rewards or real users quit. Classic crypto death spiral. I killed it and built Autophage with different physics. Tokens that decay prevent hoarding. Genetic traits reward specialization over capital. The Reservoir recycles value instead of letting it pool. Ran thousands of simulations with every attack vector I could imagine. Bot armies, whale cartels (lol), bank runs (omg), you name it (I prob tried it). The protocol bent but didn't break. Value stayed distributed because decay forces circulation. Users specialized into sustainable niches. The marketplace found equilibrium prices. The Reservoir maintained reserves through fee flows. Not perfect, but the numbers finally worked. Sometimes you have to let something die to build something that can live.## the last thing i'll say about rocksCrypto built itself on hodling. Diamond hands. Never sell. Rocks don't need food, water, or healthcare. We do. The protocol's meme captures it: "hodl? lol. not here." Tokens decay because life does. Value moves or dies. The protocol name Autophage comes from autophagy - the biological process where cells eat their own damaged parts to survive. For those hunting deeper secrets, Charlotte holds the key.<!-- the first clue in national treasure -->*What if our deepest economic assumption that value should persist forever is perfectly inverted?*

---

---layout: posttitle: "utility, network, sf: why i'm applying to yc"date: 2025-07-17 21:30:00 PTcategories: [startups, privacy, health, yc, web3, sf]attribution: ai claude opus 4.1---## the status of all the thingsThe last 4-months have been a flurry of this and that. I've built 4 products from the ground up:- [attest.ink](https://attest.ink)- [status.health](https://status.health)- [autophage.xyz](https://autophage.xyz)- [healthprotocol.network](https://healthprotocol.network)Each has gone through many iterations and some are now defunct, or just stuck in the "research phase" until I decide what to do with them.[status.health](https://status.health) is the main business product I am now focusing all my time on. It was originally meant to be a B2C API that allowed users to verify a hookup's STI status on a dating or sex-positive platform like Tinder, Grindr, or Sniffies. It quickly morphed to being more focused on STI testing history as it became clear verifying a person's actual "clear or not" status would be too hard. Testing history is still valuable but from my perspective it was less so because the goal was to not get any STIs from hookups, c'est la vie.[healthprotocol.network](https://healthprotocol.network) was a response to the recent funding cuts the Trump administration posed on sexual health clinics and other LGBTQ+ health services. The idea was to create a cryptocurrency that could help fund clinics that lost funding plus reward users of status.health for verifying health actions with either tokens, or abstracted tokens I called "points." After speaking with a couple of friends and modeling out some of the functionality for the tokenomics, it felt too much like a crypto play that wouldn't really offer much good; something I am generally allergic to. Because I was able to secure the $HEALTH token ticker, I deployed the contract on Ethereum but went back to the drawing board.I wrote and published a [paper](https://autophage.xyz/paper/litepaper.pdf) for a novel protocol based on earning decaying tokens for health activities. It took months, and was fun to research but due to complexity, cost to run infrastructure, and other factors like the fact most people would gawk at the idea of tokens that decay, I chose to keep it in research until I can fund it without needing any money to do so as a public good.Which leaves us to today, when I chose to apply to Y Combinator for status.health. If you're not familiar with YC, it's probably the most famous incubator for startups; think a place fledgling founders can go to get their initial seed round and build in a fruitful microcosm. They offer around $500K in funding, but the real value is in the network you build, it's apparently tantamount to Harvard for startups, and harder to get into (research indicates around 1-2%...FML) and since I didn't graduate college maybe this is my way "in."## why yc fits the billLook, I can build anything. AI agents and I shipped four products in four months and I've got the git commits to prove it. But here's the thing about B2B sales to healthcare companies: they don't give a shit about your midnight commits. They care about compliance audits, security certifications, and whether you've got the runway to survive their 9-month procurement process.YC solves three problems I can't code my way out of. First, money for an actual security audit so I can prove what I already know...that status.health is HIPAA-exempt by design because we never touch health data. Second, access to founders who've actually closed enterprise deals and can tell me which procurement hoops are real and which are theater. Third, the YC stamp that makes risk-averse enterprises think "oh, they're backed by the people who backed Stripe, they probably won't disappear next quarter."The irony isn't lost on me. I built status.health to eliminate trust requirements through cryptography, but I need YC's trust network to get customers to believe in trustless systems. Sometimes the path to the future runs through the past.## the strategy: focus, then scaleI've made every classic founder mistake in four months. Built five things when I should've built one. Chased the perfect cryptographic solution when good enough would've shipped faster. Spent months on a litepaper about decaying tokens that maybe twelve people will ever read (and three of them will be bots).The strategy now is stupidly simple: make status.health work for one customer. Just one. Probably an identity verification provider who already has healthcare clients asking for this but can't build it because touching health data means touching liability. They integrate our API, their customers get health verification without HIPAA compliance costs, we get distribution to thousands of businesses overnight.Once that works, we expand. Dating apps want STI verification but are terrified of storing health data (rightfully so). Employers need vaccination records but don't want to become healthcare companies. Every business that touches health data is one breach away from disaster. We're the condom for data transmission; nobody wants to think about it, but everyone's relieved when it's there.## why san francisco is still homeEveryone shits on SF now, literally and figuratively (sorry for the imagery). Too expensive, too many homeless people, tech bros ruined it, the city's dying, etc. Yeah, rent's insane and I've seen things on Market Street that haunt me. But here's what the doomers miss: this city runs on building things that shouldn't exist and there's nowhere else like it.I walk fifteen minutes and pass three people arguing about Waymo, crypto, or Trump at a coffee shop. The guy at the bodega hot dog thing in front of every club on Castro knows what a TEE is because his roommate works at a privacy startup. My barber asks about my cap table. This isn't normal anywhere else. I want work-life integration where the guy I meet at a rave at 2am introduces me to someone who solves my technical problem at brunch the next day.SF is where you can say "I'm building HIPAA-exempt health verification using cryptographic proofs" and instead of blank stares, you get "oh shit, have you talked to the team at x and y about that?" The city's not dying; it's composting. All the tourists and grifters left when the dopamine ran out during COVID. What’s left are the people who sweat out the next GitHub Copilot bug fix at some ungodly hour because their API is actually in prod. Rent’s still hell and getting more hellish, but at least it’s hell with Wi-Fi, breakfast burritos, and the occasional circuit party.## the next chapterThe next six months are straightforward. Get into YC or don't (probably don't, statistically speaking). Either way, ship the enterprise version of status.health by end of year and look for a co-founder. Land one paying customer who validates that businesses will pay to not store health data. Use that proof to raise a real round and hire.If YC happens, great. I'll move faster with their money and network. If it doesn't, I'll keep building at 3am with lo-fi playing and proving that a college dropout who started at the Apple Store can solve problems that companies with hundreds of engineers can't touch mostly because their legal teams have meetings about meetings about risk.## why claude thinks yc will not accept my application and why i do not careI asked Claude to evaluate my YC application. It said I'm too early (no revenue), too technical (need a business co-founder), too focused on privacy (niche market), and competing against billion-dollar companies (Persona, Onfido, CLEAR). Claude's probably right. The acceptance rate is 1-2% and I'm a solo founder without a CS degree building in a space VCs think is "nice to have" not "need to have."But here's what Claude doesn't get right: every interesting company starts out looking like a bad idea. Airbnb was "who wants strangers in their house?" Uber was "illegal taxi company." Stripe was "another payments processor." status.health is "HIPAA compliance for companies that only understand huge compliance budgets." Sounds stupid until their next data breach costs more than our entire platform would be to buy...for now.YC might pass because I refuse to store data in a world built on data hoarding or because it seems too complicated. They might pass because I'm solo. They might pass because privacy isn't sexy until it's catastrophic. That's fine. I'll keep building either way.The truth is, I don't need YC to build status.health. I need them to sell it faster. If they don't see that opportunity, I'll build slower but still get there. The problem's not going away. Healthcare data breaches are accelerating and getting increasingly more costly. The solution exists, I built it.So yeah, I applied to YC knowing they'll probably reject me. But shots you don't take and all that. Worst case, I keep building what I'm building. Best case, I build it faster with smarter people around me.

---

---layout: posttitle: "the claude i loved is gone: how anthropic’s new policies hurt indie AI builders"date: 2025-08-13 16:25:00 PTcategories: [startups, ai, sf, op-ed, personal]attribution: ai chatgpt 5---## an early adopter’s dilemmaI’m a solo founder in San Francisco, building a privacy-first health platform (status.health) and relying heavily on AI tools to bootstrap prototypes. Six months ago, Anthropic’s Claude AI felt like a godsend. A friendly 100K-token context assistant that could write code, generate documents, and digest huge files without breaking the bank. I eagerly integrated Claude into my workflow, even paying for Claude Pro and later Claude Max to unlock more power.Today, I’ve hit a wall. Anthropic’s recent pricing and usage policy changes have effectively priced me out of using Claude. Stricter rate limits, steep token pricing for high-context tasks, and revamped model tiers (“Haiku,” “Sonnet,” and “Opus”) have made the model unaffordable and nearly unusable for an indie builder like me. In this op-ed, I’ll break down the timeline of these changes, from quiet nerfs to public caps, and explain how they’ve crushed the value Claude once offered. Along the way, I’ll share what other developers are saying on forums and contrast Anthropic’s approach with OpenAI, Google, and open-source alternatives. The goal is to shine a light on how a promising AI tool lost its way for the little guys, and to plead (even poetically) for a better path forward.## from haiku to opus: claude’s model tiers and token costsTo understand the squeeze on indie users, we need to talk about Claude’s model tiers. Earlier this year, Anthropic introduced three versions of Claude 3: *Haiku, Sonnet,* and *Opus*. These correspond to increasing levels of capability (and cost). Claude Haiku is optimized for speed (a smaller model with shorter context), Claude Sonnet balances performance and efficiency, and Claude Opus is the most powerful model with the highest reasoning ability. In practice, Opus can handle the hardest tasks but burns through tokens up to 5× faster than Sonnet. Each tier has its own pricing: as of mid-2025, Sonnet models cost about \$3 per million input tokens and \$15 per million output tokens, whereas Opus commands a whopping \$15 per million input and \$75 per million output. Haiku is far cheaper, *Claude 3 Haiku* was just \$0.25 per million input tokens but it’s also less capable.Crucially, Anthropic phased out the cheaper models as newer ones arrived. The entire Claude 2 series was retired by July 21, 2025, and even the Claude 3 Sonnet (snapshot 2024-02-29) was deprecated then. Developers who had built cost-efficient workflows around Claude 2 or early Claude 3 suddenly had to migrate to Claude 4 models, *whether they could afford them or not*. (Anthropic did give notice of deprecations in their docs, but if you blinked you missed it.) The result: no more access to the older, budget-friendly models. A startup that was happily using *Claude 2.0* on the cheap in June found that by late July their only “replacement” was Claude Sonnet 4 at enterprise pricing levels.Anthropic also started charging a premium for high-context usage. Claude’s famous 100K-token context window was a selling point for feeding entire codebases or research corpora. But when they expanded Claude’s context to 1 million tokens in August 2025, it came with strings attached: prompts above 200K tokens are billed *double* for inputs and *50% more* for outputs. This long-context feature is in *“public beta”* but only for top-tier API customers (Tier 4 and custom plans), meaning indie devs on standard plans can’t even access it yet. In theory I could now process 75,000 lines of code in one go, but practically I’m locked out of that capability unless I somehow become a high-volume enterprise client. Anthropic’s own pricing page acknowledges prompt caching and batching as ways to offset these costs, but implementing such tricks is a burden on developers and only necessary because the raw token prices are so high.## march–may 2025: new models launch, indie access lagsAt the start of 2025, Claude was riding high on goodwill from early adopters. Anthropic rolled out Claude 3.5 in stages. First Claude 3.5 *Sonnet* in June 2024, later Claude 3.5 *Haiku* and *Opus*, and boasted about *“cost-effective pricing”* and *200K token context windows*. By early 2025, an even more advanced Claude 4 was in the works. In May 2025, at Anthropic’s first “Code with Claude” developer conference, the Claude 4 model family launched, promising state-of-the-art performance on coding tasks. But something troubling happened in this launch: certain independent partners were excluded.Windsurf, a popular AI coding assistant startup (ironically being acquired by OpenAI), reported that Anthropic cut off their direct API access to Claude models with less than five days’ notice. *“Anthropic decided to cut off nearly all of our first-party capacity to Claude 3.x models,”* Windsurf’s CEO wrote, saying they had been willing to pay for full capacity but were still dropped on short notice. Even more galling: when Claude 4 launched, Windsurf wasn’t given access to run it, while other big-name coding tools (Cursor, GitHub Copilot, etc.) were granted direct Claude 4 access from day one. Windsurf had to scramble to route Claude through third-party providers at higher cost, warning users of potential outages in their Claude features. Anthropic, for its part, had just launched its own competing product (Claude Code) in February, raising eyebrows about whether they were sidelining a tool that soon would belong to OpenAI.For indie developers, the Windsurf incident was a red flag: Anthropic could revoke or limit API access abruptly, even for paying partners. As a founder, I empathized. I also had “Claude inside” my product roadmap and suddenly worried if a quiet policy change might yank that away. Windsurf’s CEO noted, *“We are disappointed by this decision and short notice.”* I’d use stronger words: it’s hard to trust an AI platform that can change the rules overnight. Unfortunately, June 2025 was only a prelude of worse to come.## july 2025: quiet cuts and “usage limit reached” chaosIn mid-July, heavy Claude users (especially those using the new Claude Code tool for programming assistance) began noticing mysterious new limits. Anthropic hadn’t made a big public announcement yet, but suddenly Claude chats were getting cut off early, often with no warning. I experienced this firsthand: long coding sessions that ran fine in June started hitting a brick wall in July, with Claude refusing to continue. Initially I thought it was a glitch, after all, I was paying \$200/month for Claude Max (10-20× capacity) and expected some serious usage headroom. Instead I got cryptic errors and stalled conversations.I wasn’t alone. On Reddit, users voiced shock and anger at how abruptly Claude became unusable for sustained work. *“100% and no warning you are getting close,”* one user complained, describing how chats would end abruptly with no chance to even get a summary or hand-off. *“It has become impossible to get any actual work done because I am spending all my time explaining what I am trying to do over and over to full chats, and limits hit in a few hours,”* they wrote, adding that it had gotten *“worse in the last week than anytime in the past 6 months!”*. Another frustrated developer vented, *“The other day I hit my limit on premium after less than 10 min. The f--- am I paying for?”*. This sentiment was widely echoed: many of us felt like we were paying for a “premium” service that suddenly wouldn’t let us work for more than a few minutes at a time.So, what changed? It turns out Anthropic had quietly tightened the rate limits on Claude.ai usage, specifically targeting Claude Code sessions using the Opus model. Without telling users upfront, they introduced hidden caps to stop what they considered abuse (like running Claude 24/7 or sharing accounts). In practice, those hidden changes hit normal users hard. One major change was how Claude chose between Opus and Sonnet models on the Max plan. Previously, a savvy user could stick mostly to the mid-tier Sonnet (to stretch their usage) and only invoke Opus for especially tough tasks. Sometime in July, Anthropic changed Claude Code’s behavior so that Max users’ sessions would start on Opus by default, only switching down to Sonnet after a certain threshold of usage. This “auto model switching” was presumably meant to preserve experience (giving you the best model at first) while preventing you from accidentally running Opus non-stop. But it backfired badly. Users who didn’t manually override the model would burn through their token allotment at 5× the normal rate with Opus, hitting the limit shockingly fast. *“I noticed an update made it use Opus first before going to Sonnet. Now when I start Claude I make sure it’s set to Sonnet. Opus is stupid expensive,”* one user reported. Others on the \$200 Max tier found even that wasn’t enough: *“with \$200 I’m reaching Opus limits pretty fast, while a week ago that would be possible with parallel instances only,”* said one, noting they had to micromanage multiple instances before and now even one instance choked.By July 17, the situation had blown up enough that TechCrunch ran a piece about it titled “Anthropic tightens usage limits for Claude Code — without telling users.” According to that report, some developers found it *“impossible to advance \[their] project since the usage limits came into effect.”* Anthropic’s response at the time was cagey. They acknowledged being aware of issues but *“declined to elaborate further.”* In other words, no clear apology or detailed explanation to users. Many of us felt blindsided. We had been early Claude champions, some even canceling ChatGPT Plus subscriptions because Claude’s bigger context and reliable output seemed worth it. Now Claude was throttled and sputtering; it *“has been worse in the last week than anytime in the past 6 months,”* as one user observed ruefully. The lack of communication and transparency was perhaps the worst of it. *“No warning… Wtf,”* one user wrote about the sudden capacity errors. *“I thought it was just me… I had this insane hallucination that something was changed,”* mused another, only half joking. We now know *something* was definitely changed, just mostly behind closed doors.## august 2025: weekly caps and the \$500++/month paywallAfter the outcry in July, Anthropic finally went public with a plan: they would impose new weekly usage caps on all paid tiers of Claude.ai, effective August 28, 2025. An email went out to subscribers and Anthropic even tweeted about it, framing the move as targeting a *“handful of users”* who run Claude Code 24/7 or resell access. They assured us this would affect *“less than 5% of subscribers”* and that “most users won’t notice a difference”. But the details told a different story for power users and indie devs pushing the limits.**5-Hour Windows + New Weekly Limits:** Anthropic kept the existing *5-hour rolling rate limit* (the one that limits how much you can do in any 5-hour period) and stacked two weekly limits on top. Now, in any 7-day span, you can hit a hard ceiling where Claude refuses work until the week resets. There are *two* weekly caps: one on overall usage (hours of any model) and one specifically on Claude Opus 4 usage. This was essentially Anthropic saying: *we will now meter out Opus, our “most advanced” model, by the hour.* If you used too much Opus time in a week, you’re done until next week (unless you pay more).**What the Caps Mean in Practice:** In their announcement, Anthropic gave rough numbers. On the \$20/month Pro plan, you can expect *40–80 hours of Claude Sonnet 4 per week* (and note: Pro users *cannot* access Opus at all under the new rules). On Max \$100 (5×), they said *15–35 hours of Opus 4 and 140–280 hours of Sonnet 4* per week. On Max \$200 (20×), *24–40 hours of Opus 4 and 240–480 hours of Sonnet 4* per week. These might sound like big numbers (who codes 40 hours straight on Opus in a week?), but they aren’t as generous as they look. Those hour figures are not literal clock hours, but based on token usage translated into an “effective hours” metric and Anthropic has been very fuzzy about how they calculate that. Heavy coding tasks with large codebases can chew through an “hour” of Opus much faster than 60 minutes of real time. Users quickly pointed out that the *Max 20× plan no longer gives 4× the value of Max 5×* once you hit the weekly cap, you pay double, but you might end up with the same weekly Opus allowance if you’re not careful. *“20× feels like marketing if a weekly cap cancels it,”* as one user succinctly put it.**Extra Usage Now Costs Extra:** Anthropic’s message to us was essentially *“if you hit the cap, you should go pay for API calls.”* They noted Max users could purchase additional usage beyond the weekly limit at standard API rates. In the Claude Code interface, if you reach your allotment, it will helpfully prompt you to switch to pay-as-you-go credits (which cost the same steep per-token prices given earlier). For an indie dev, this feels like moving the goalposts: we signed up for a flat-rate service because we needed predictable costs. Now, if we actually use what we need and exceed some opaque weekly threshold, we’re asked to start swiping the credit card per call. A few of us joked darkly that at the rate things are going, Anthropic would prefer we shell out \$500 or \$1000 a month just to use Claude at the level we used to on a \$100 or \$200 plan.The developer community’s reaction to the weekly caps was scathing. A Reddit megathread filled with nearly a thousand comments got boiled down into a user-generated “discussion report,” and the top issues were *lack of transparency* and *punishing loyal users*. *“The issue isn’t just limits – it’s opacity,”* the report’s TL;DR read, noting that without a live usage meter or clear definitions of what an “hour” means, users get surprise lockouts mid-week. People begged Anthropic for basic quality-of-life fixes: *“Give us a meter so I don’t get nuked mid-sprint,”* one user quote reads. It’s hard to overstate how disempowering it is to be in the middle of a coding session or a long conversation and have Claude abruptly refuse to continue because you unknowingly crossed some invisible line. Another user pointed out how the 20× Max plan felt like a bait-and-switch: *“If 20× doesn’t deliver meaningfully more weekly Opus, rename or reprice it.”* Right now, paying for the highest tier doesn’t guarantee you proportionally higher real usage, because the weekly cap “step up” isn’t 1:1. Meanwhile, those of us on the receiving end of these changes feel like we’re being punished for the abuses of a few. Sharing Claude accounts or running bots 24/7 is against Anthropic’s usage policy. Fair enough. But why not enforce those rules on the offenders rather than impose blanket limits on everyone? *“Don’t punish everyone – ban account-sharing and 24/7 botting,”* as one commenter wrote. Or in another user’s blunt plea: *“Ban abusers, don’t rate-limit paying devs.”*Anthropic defends the weekly caps as necessary for reliability, they even cited that Claude Code had suffered multiple outages in the prior month due to overwhelming demand. I don’t doubt that a small percentage of super-users were hammering Claude. The problem is the cure is worse than the disease for those of us who legitimately relied on Claude’s former flexibility. Imagine a freelancer or tiny startup who might have one intense week of work (a deadline sprint where they need Claude’s help refactoring a codebase or iterating on a long document) followed by a light week. Under the new regime, that intense week will almost certainly trigger a lockout, because the weekly cap *does not care* that you have a quiet week to balance it out. There’s no rollover of unused capacity. No way to request a temporary boost except paying per-token. As one user noted, *“Locked out till Monday is brutal. Smooth it daily.”* suggesting that a daily limit or some kind of smoothing would be more forgiving than a hard weekly wall. But right now, if you hit your weekly max on a Thursday, that’s it – no Claude for you until the next week.All of this has effectively erected a \$500/month or more paywall for getting the most out of Claude. Why starting with \$500? It’s an estimate of what an indie developer would need to spend to regain the freedom they had before. Currently, I have to spend on the order of \$800/month. The \$200 Max plan will not last you a full week of heavy development now, so you’d need to either maintain multiple subscriptions (risky and against ToS) or start buying API credits on top. For some, even a single \$200 plan is already a stretch; \$500 or more is out of the question. The conclusion is that Claude is no longer indie-friendly. It’s been groomed for enterprise budgets and tightly metered for everyone else. A founder friend of mine quipped that Anthropic must assume anyone using Claude at scale *“has venture funding or a corporate expense account.”* The rest of us are left picking up the crumbs or looking elsewhere.## unusable for code, long context, or “artifacts”The following policy changes have rendered Claude nearly unusable for several of its flagship use cases that drew me to it in the first place.**Long-form Coding Assistance:** Claude Code was supposed to be a game-changer for developers introducing an AI pair programmer with an immense context window to hold your entire project. In reality, with the new limits, Claude can’t effectively sustain a coding session on a non-trivial repository. Either you hit the 5-hour window cap (which stops you every so often), or now the weekly cap shuts you down entirely. *“I had to increase my plan to continue without interruptions… My fear is that soon I’ll have to sign 50×, 100×… That’s f---ed up, right?”* said one user on the Max plan when the limits started biting. Even for those willing to pay more, Claude’s assistance has become stop-and-go. It’s hard to build momentum or dive deep into a coding problem when you’re constantly watching a meter (that you can’t even see) or re-explaining context after a reset. As one Redditor lamented, *“it used to be able to follow instructions and now it can’t.”* The continuity of thought, arguably Claude’s strength over shorter-context models, is lost.**Artifact Generation:** In mid-2024 Anthropic introduced “Artifacts” which is a feature where Claude output things like code files, documents, or website designs into a side pane, letting you interact with them separately. It was a great idea, turning Claude from just a chat into a collaborative workspace. But today, artifact generation is hamstrung by the limits. Users report that artifacts sometimes stop appearing or updating properly due to the new restrictions. *“All of a sudden I’m seeing this weird non-updating to the artifacts,”* one user noted, suspecting *“something was changed.”* They were right because behind the scenes, Anthropic likely tweaked how artifact outputs count toward usage, possibly to prevent people from using Claude as a free code generator. The result for me has been incomplete or missing artifact outputs in Claude Code. For instance, I had Claude working on a multi-file Python script; it started writing an `simulations.py` artifact but hit a limit mid-generation. The artifact never finalized, and I couldn’t retrieve what partial code it had written. When I tried to coax Claude to resume, it acted confused (likely because the context got cut). This never used to happen before. Artifacts were one of Claude’s most promising features for builders, and now they’re another casualty of the crackdown.**Long Documents and Contextual Analysis:** A core promise of Claude was that 100K (and now 1M) token context, meaning you could feed an entire book or a trove of customer chats or a huge log file and get comprehensive analysis. But what’s the point of a large context window if the model taps out after a few responses or arbitrarily truncates its output to avoid hitting some token limit? I’ve seen Claude become increasingly conservative with large inputs, often replying *“Let’s summarize in parts”* or refusing to ingest a long text it would have cheerfully accepted earlier. In some cases, I suspect Claude’s frontend is preemptively limiting input size or chunking behind the scenes (perhaps this is part of Anthropic’s suggested Retrieval-Augmented Generation workaround). Users on Hacker News noted the irony that Anthropic didn’t raise subscription prices, but instead quietly imposed techniques like forced summarization (RAG) to cope with costs. A developer on Reddit observed: *“There were never any actual usage limits; they just said e.g. ‘Claude Max 20x users have 20× capacity’… But then \[they] go on to say \[in fine print] ...”* implying the limits were always somewhat opaque. Now it’s clear: high-context usage is *the* big cost center, and Anthropic has effectively kneecapped it for anyone not paying top dollar. I used to load up entire research papers and have Claude cross-reference them. Now, anything beyond a certain length triggers either a refusal or such a brief summary that I might as well not have bothered. It’s devastating for those of us in domains like healthcare, where lengthy guidelines or datasets were finally within AI’s reach thanks to Claude’s context size.**Claude no longer delivers on its core value propositions for individual builders**. The model might technically still be capable of great things, but Anthropic’s policies have put those capabilities behind a glass wall. It’s like being given a sports car and then finding the fuel tank has a tiny restrictor plate where you can press the pedal, but you can’t go far or fast anymore.## community backlash: “this feels like betrayal”The mood among early Claude adopters ranges from disappointment to outright anger. Many feel that Anthropic cultivated our loyalty with a great product, only to yank the rug out with sudden monetization and usage clamps. *“I also feel cheated,”* one user wrote, asking for alternatives as Claude’s performance degraded and limits tightened. Another said, *“I dropped Anthropic just before the pricing structure changes. I’m \[switching] between ChatGPT, Gemini and NotebookLM,”* referencing OpenAI, Google’s Gemini, and a new NotebookLM product, basically anyone else. On Hacker News, the TechCrunch story on Claude’s usage limits spurred comments noting that AI costs should be going *down* over time, instead of going up. *“Costs should be going down. Going up will reduce adoption by developers hoping to keep their apps low cost,”* one commenter noted, pointing out the obvious: as model providers find efficiencies (and as competition heats up), we expect better pricing or higher quotas, not stricter ones. Yet Anthropic’s move was the opposite, perhaps because their own cloud costs or profit goals forced their hand. Either way, it left a bad taste.What stings the most is the lack of warning and dialogue. There was no clear advance notice to subscribers that, for example, *“On July 15 we will adjust our usage policies, expect possibly shorter sessions,”* etc. The weekly cap announcement came about a month *after* the quiet changes. I guess always ask for forgiveness instead of permission and all that. As you might expect, early adopters, many of us indie hackers, felt blindsided. We had designed workflows and even products around Claude’s capabilities. Anthropic’s slogan could well have been “let us handle the heavy lifting, you focus on building.” But by not communicating upcoming limits, they hung developers out to dry. One developer wrote on Reddit: *“I recently upgraded to the \$100 plan and hit my limits super fast today. It drives me crazy that I can’t see any usage metrics.”* That was before the weekly cap rollout; even after, Anthropic still lacks an in-app dashboard showing how close you are to those weekly Opus hours or token quotas. People are literally setting up community-made trackers or manually timing their usage. It’s absurd. A high-tech AI service that makes you use a stopwatch and spreadsheet to avoid being cut off. As one frustrated user pleaded, it’s such a simple ask: *“Meter, definitions, alerts – that’s all we’re asking.”* The opacity is driving us away as much as the limits themselves.And let’s not forget: some users also suspect Claude’s quality itself has dipped in recent months. Complaints about the model’s output getting *“lazier”* or not following instructions as well have popped up. Whether this is objectively true or just an illusion (one jokester blamed it on Claude “being French and it’s August” vacation mode), the *perception* of a nerf is real. When your chat gets cut off repeatedly, you often have to resend or rephrase queries, which can make the model seem dumber (you’re essentially seeing it fail more). There’s speculation that Anthropic might have tweaked the model to be more conservative or shortened its allowed response lengths to save tokens. We don’t know, but the fact we’re wondering highlights the trust erosion. Early Claude was delightfully eager to help produce full, context-rich answers. Late 2025 Claude feels skittish and hamstrung.One poignant quote from a Redditor summed up the indie builder’s sense of betrayal: *“I was using Claude pretty heavily last year… People were complaining daily about it here and Anthropic insisted they hadn’t changed anything.”* In their case it was a deja vu due to a similar degradation happened around summer 2024 and was denied. This time, at least, we have concrete policy updates to point to, but the feeling is the same. We trusted Claude; we even paid when it was in beta because we saw its potential. Now many of us feel cut off at the knees, forced to either cough up much more money or lose a tool that had become integral to our work.## comparing options: openai, google, and open sourceIt’s worth noting that Anthropic’s rivals have taken *very different* approaches in this period. OpenAI’s GPT-4, for all its own limitations, has kept a relatively stable deal for individual users. For \$20 a month, ChatGPT Plus gives you GPT-4 access with a reasonable (and clearly communicated) cap on requests (currently 50 messages every 3 hours) and no surprise weekly limit. You can’t feed GPT-4 100K tokens of context, its max is 8K or 32K if you have the special version, but in practice GPT-4 often *feels* more available than Claude now. I can chat with GPT-4 continuously without my session mysteriously “filling up.” And if I need the API, OpenAI’s pricing per token is steep but at least they haven’t slapped new caps on the fly. In fact, OpenAI has been lowering some prices (they cut GPT-3.5 turbo costs significantly in 2023, and offered GPT-4 32K context to developers at a premium but with no usage tiers). That’s not to say OpenAI is perfect. They’ve had their own communication lapses and quality adjustments, but from a pure pricing perspective, an indie developer can *budget for OpenAI* with more confidence. There’s no \$500 surprise paywall lurking.Google’s Gemini (and related products like Bard or PaLM API) is still emerging, but Google seems to be positioning itself as the *developer-friendly* alternative. For instance, their recent *Gemini “Advanced”* model (a rival to GPT-4 and Claude) on Google Cloud is rumored to have flexible pricing and potentially larger context windows without exorbitant cost. Google hasn’t yet put Gemini into a consumer-facing \$20/month package as of this writing, Bard is free but limited in other ways, however, they’ve heavily invested in AI through products like NotebookLM (an AI notebook for researchers) and vowed not to sting developers with sudden changes. If anything, Google is more likely to offer higher free usage quotas to entice devs into their ecosystem. It remains to be seen, but many of us are exploring these options. One fellow founder told me he’s *“mixing and matching GPT-4, Claude, and Vertex AI (Google) to see which is most cost-effective in the long run.”* After Anthropic’s moves, Claude is usually the first to be turned off in that mix when cost becomes an issue.And then there’s the open-source route. Six months ago, I wouldn’t have seriously considered running a local LLM for my use case. Claude and GPT-4 were just too far ahead in quality. But open-source models have rapidly advanced (think Llama 2, Code Llama, etc.), and importantly, they come with no usage shackles. If you have the computing power, you can fine-tune and run these models entirely under your control. Several indie devs I know have started using local 13B-70B parameter models for coding tasks, using techniques like context splitting and vector databases to approximate a 100K context. It’s not as straightforward as using Claude was, and the model quality can be hit-or-miss, but at least it’s predictable cost: once you’ve set up the server, you’re only paying for electricity or cloud GPU time (which you directly control). There’s a philosophical draw to this as well *no AI overlord can throttle me if I’m running the model myself*. The downside is you might need a \$3,000 GPU or a hefty cloud instance, but over the long run that could be cheaper than burning \$500 a month on a service. The open-source community is also working on improving context handling (through smarter retrieval) and some models are surprisingly competent at code when fine-tuned.In comparing these platforms, I realize it boils down to a simple question: who is building for the indie hacker vs. who is building for the enterprise? OpenAI, despite being a big company, still maintains a very large base of individual enthusiasts and developers thanks to ChatGPT. Google is trying to woo developers onto its cloud with flexible AI offerings. Many open-source contributors *are* indie hackers themselves scratching their own itch. Anthropic, on the other hand, seems to be pivoting hard to enterprise clients, the kind who will pay for government contracts or integrate Claude into Fortune 500 workflows. (They even offered Claude to *“all three branches of the U.S. government for \$1”* in a trial, which tells you whom they want to impress.) There’s nothing wrong with a business pursuing paying customers, but it feels like Anthropic has forgotten the *early adopters* that helped prove their technology’s value. Indie builders popularized Claude’s strengths online, wrote blog posts praising its large context, built open-source wrappers, and generally gave Anthropic goodwill and mindshare disproportionate to our pocketbooks. To be sidelined now with restrictive policies and effectively told “you’re not our target user anymore.” That hurts.## hope for a better balanceAnthropic’s mission is about creating beneficial AI, and Claude truly is a remarkable achievement on the technical front. It’s *because* it was so good that these policy changes sting so much. I want to be clear: I’m not angry that Anthropic needs to control costs or prevent abuse. I understand a startup can’t hemorrhage money providing unlimited AI to a minority of super-users but there’s a right way to handle these challenges. The lack of transparency, abrupt implementation, and one-size-fits-all limits are what turned a reasonable business decision into a fiasco for the community. Anthropic could have engaged developers in dialogue (“here’s what we’re seeing, here are some options we’re considering…”). They could have built tools to help users adapt (like proper usage dashboards or smarter switching that doesn’t default to the expensive model first). They could have tailored solutions that target abusers directly rather than clipping everyone’s wings. Instead we got a summer of confusion and frustration, and a future where using Claude feels like walking on eggshells.As a founder and an early Claude user, I’m writing this in hopes of change. Anthropic stands at a crossroads: it can either double down on being a high-priced enterprise AI vendor or find a way to keep indie innovators on board. I believe there’s value in the latter. After all, today’s scrappy startup could be tomorrow’s big customer, and the innovations independent developers create often expand what’s possible with AI. Anthropic even interviewed me as a power user. We’re partners in the ecosystem. I want to continue using Claude to build but now I've gotta rethink my business plan and my entire development pipeline; I'm priced out.So, Anthropic, if you’re listening: please remember the little guys who got you here. We know running large models isn’t cheap, but **meet us halfway with fair pricing, clarity, and respect for our workflows**. At the very least, be upfront and *honest* when you must impose limits. Surprises belong in AI-generated stories. Please keep them out of service terms.And to end on a note that perhaps the Anthropic team might appreciate, here's a short poem:**a plea to anthropic**```you gave us  a brain that thought  late nights felt shorter  deployments felt lighter  builders without vcs  builders without budgets  felt like you saw us  then  you raised the moat  without warning  and sat back  i’m asking  please  leave a sliver-open door  for the lean ones  the “not-yet-funded” crowd  we preferred your model  we still do  but if we’re priced out  we’ll build our own steps  or climb overyou took away  a quiet kind of equity  in months  don’t let it vanish  completely```

---

---layout: posttitle: "privacy or bust: why you should move to lumo"date: 2025-08-28 17:05:00 PTcategories: [ai, privacy, startups, op-ed, security]attribution: ai lumo---## the promise that slipped...againWhen I first started using large language models, Claude was the only one that felt reliable enough to act like a pseudo-employee. I used it to code, write documentation, and handle repetitive operations. That one tool let me run multiple ventures alone. I built my workflows around it.Anthropic was founded in 2021 by people who claimed they wanted something different. Dario Amodei had been Vice President of Research at OpenAI, where he helped create GPT-2 and GPT-3. He left because he believed AI was moving too fast without safety built in. The idea was to build AI that was steerable, interpretable, robust, and safe. [This profile](https://kantrowitz.medium.com/the-making-of-anthropic-ceo-dario-amodei-449777529dd6) and [this Wikipedia entry](https://en.wikipedia.org/wiki/Anthropic) describe that origin story. Anthropic was even structured as a Public Benefit Corporation and funded by some of the largest names in technology. That mattered. It suggested they were accountable to the community that believed in them as much as to their investors.  That mission collapsed. The price hikes, hidden caps, forced migration to expensive tiers, and rewrite of the terms of service that allowed Anthropic to read user chat histories are a betrayal of the community that supported them and a violation of the mission they claimed as a PBC. I wrote about that [here](https://97115104.com/2025/08/13/the-claude-i-loved-is-gone/) and the terms change itself is covered [in this report](https://www.perplexity.ai/page/anthropic-reverses-privacy-sta-xH4KWU9nS3KH4Aj9F12dvQ). I had already been looking for alternatives since the pricing rug a month ago. Today, when the terms change went live, I cancelled my account.  If a Public Benefit Corporation can rug its users this openly, the model itself may be broken. It is time to ask whether the legal structure of “ethical AI” has any meaning when the mission can be discarded at will.## looking for an alternativeI spent the last month testing everything I could. Hosted open-source models looked promising, but most of them carried the same risks as Claude and ChatGPT: unclear privacy, shifting terms, and unpredictable costs. Running models like [Llama](https://ai.meta.com/Llama/) or [Mistral](https://mistral.ai/) locally worked for experiments, but not for daily scale unless you are willing to manage hardware, updates, and uptime yourself.  I initially ignored Lumo. It was clunky and missing features like web search. I assumed it was not ready. Then Proton emailed me to announce it again, and my father, who is deeply privacy focused, asked if I had tried it. That was enough to make me take a second look.  I was surprised. The user experience still has rough edges, and their cat mascot is funny, but the architecture is strong. Proton has earned my trust with Mail, VPN, Drive, and Pass. When Proton says end-to-end encryption, I believe it.## privacy that holdsLumo encrypts every prompt, response, and file on the client before it leaves your device. Even Proton cannot read your conversations. No policy update can undo that.  The code is [open source](https://github.com/ProtonMail/WebClients/tree/main/applications/lumo). Anyone can audit it. Proton states that they do not log conversations, do not train on user data, and do not sell usage statistics. OpenAI and Anthropic do the opposite by retaining data and reserving the right to reuse it.  At this point promises are worthless. Proof is the only thing that matters. Proton delivers proof.## pricing you can trustI began searching for alternatives when Anthropic’s pricing rug landed. Today’s terms change was the final straw. The speed of that shift shows how fragile trust becomes when pricing and policy can be dictated unilaterally.  Proton has been consistent for a decade. Proton Mail has never turned into an ad-driven inbox. Proton VPN has never pivoted to selling user activity. Proton Drive has never locked existing files behind a paywall. The company has held the same line since the beginning. That track record is why I trust Lumo’s subscription model.  If Proton ever betrayed that trust, the reputational damage would outweigh any revenue gain. They have more to lose by breaking their word than they could possibly earn. That pressure is what makes their pricing stable. Stability is about whether you believe the company will honor the deal in a year. With Proton, I do.## the ecosystemThe Proton ecosystem is what makes this work. Apple built its reputation on seamless integration across devices. Proton has done the same across privacy tools.  Mail, Pass, VPN, Drive, Calendar, and now Lumo connect under one account. When I generate code in Lumo, save it in Drive, and share notes over Mail, the entire flow happens inside a single encrypted environment. That improves convenience and reduces risk. There are fewer vendors, fewer policies to monitor, and fewer points of failure where data can leak.  The ecosystem matters because every tool shares the same design principle: privacy first.## open source is the real answerThe long-term answer is open-source AI. Models like [Llama](https://ai.meta.com/Llama/), [Mistral](https://mistral.ai/), [Falcon](https://falconllm.tii.ae/), [Mixtral](https://mistral.ai/news/mixtral-of-experts/), and even [DeepSeek](https://www.deepseek.com/) show what is possible. Open-source models can be audited, forked, and improved by the community. They cannot suddenly change their terms or rug you. That is the safeguard.  This is where Anthropic looks the worst. They have never open-sourced Claude. That silence would be one thing for a normal startup chasing profit. But Anthropic is the only one of these companies incorporated as a Public Benefit Corporation. They are the ones who put “safety” and “accountability” in their charter. Yet Meta, Mistral, TII, DeepSeek, and even OpenAI have all published models while Anthropic has kept everything locked away.  Dario Amodei left OpenAI claiming the moral high ground. The irony is that Sam Altman has now released an open-source model ([GPT-OSS](https://openai.com/index/introducing-gpt-oss/)), and DeepSeek has opened theirs, while Anthropic, the so-called public-benefit company, has done nothing. If anyone in this story deserves to be called the bigger asshole, it is Dario.  Open-source AI will eventually win. In the meantime, running these models yourself is still out of reach for most people. It requires hardware, bandwidth, and constant upkeep. This is where Lumo fits. Proton has already open-sourced its [client code](https://github.com/ProtonMail/WebClients/tree/main/applications/lumo), and they have the track record to prove they mean it. Lumo gives everyday users privacy and stability today, while the open-source ecosystem continues to mature.## where this leaves usThe Claude I and many others relied on is dead. OpenAI has ceilings that undermine trust. Other providers have shown they will more often choose profit over privacy or their users. If we continue paying them, nothing changes.  The only way to shift this industry is to support services that prioritize privacy from the start. I have moved my workflows to Lumo and I support open-source models. If enough of us do the same, the industry may follow. At minimum, alternatives will have enough support to compete.  We need AI that cannot leak by design. And cannot rug by design. Lumo is that. At least for now...Take a look and see for yourself. You can read more about it [here](https://lumo.proton.me/about).

---

---layout: posttitle: "the eye-ronic misadventure: a $149 lesson in healthcare theatre"date: 2025-09-30 15:39:00 PTcategories: [rant, health, sf, personal]attribution: ai gemini---## a glimmer of hopeI’ve been battling a [chalazion](https://www.aao.org/eye-health/diseases/what-are-chalazia-styes) on my eyelid for what feels like a geological epoch. It is a tiny, stubborn, blocked oil gland that decided to set up camp on my face. For over a month it has been a standoff between me, this unwelcome red bump, and a series of hot compresses. My urgent‑care clinic gave me the usual runaround with wait times so long I could have learned a new language. That is [Medi‑Cal](https://www.dhcs.ca.gov/services/medi-cal/Pages/default.aspx) for ya. I decided to take matters into my own hands.My search led me to call my old optometrist, a place iron­ically named Eye Gotcha. Since I had had [LASIK](https://www.fda.gov/medical-devices/laser-eye-surgery/what-lasik) years ago and was not a current patient, they could not help and referred me to Valencia Optometry. In hindsight the name of my old doctor’s office should have been a warning. I called Valencia, where the person who answered the phone was lovely and assured me they could handle my pesky chalazion. “We don’t take Medi‑Cal,” they warned. “No problem,” I said, “I will pay out of pocket.” The price for my salvation was $149 – a steal, I thought, to finally rid myself of this tiny, unwelcome tenant living rent free on my face.## the bouncy ball overtureI arrived at the office, filled with a misplaced sense of optimism. The first thing I noticed was a mysterious, rhythmic "BOING... BOING... BOING" from the apartment above. The doctor, Martha Klaufman, was not amused. She complained openly about the noise, which struck me as unprofessional, but I let it slide. I was on a mission. A herd of elephants could have been tap-dancing upstairs, and I wouldn't have cared if it meant getting this thing off my eyelid.I dutifully filled out the mountain of paperwork and was then asked to immediately pay the $149 "co-pay." I use quotes because the term implies a shared experience. The only thing being shared here was my money with their bank account. I paid, my heart aflutter with anticipation.## the bait and switchFive minutes later, I was called in. Not for a chalazion consultation, mind you. That would be too logical. I was given a standard eye exam, the kind you get when you need glasses. I don't need glasses. I have had LASIK. I explained again why I was there, mentioning I already had antibiotics and a referral to a specialist with a six-week wait. This was my attempt to circumvent that broken system.Dr. Klaufman, mid-vision-check, explained to me what I already knew from a simple AI search: stubborn chalazions need a steroid shot. Then, in a stroke of medical genius, she decided to prescribe me *another* antibiotic "to kill the bugs," despite the fact that there was no active infection. The prescription was for a weaker antibiotic than the one I already had. It was like trying to fight a dragon with a water pistol ## a referral to nowhereThen came the grand finale. She announced she couldn't actually *do* anything about the chalazion. "That wasn't the kind of thing she did there," she said. Instead, she would give me a referral.The assistant called the specialist's office on speakerphone. We all listened as they confirmed they could only see me with a referral from my primary care doctor (which I had for a different place) and that the wait was, you guessed it, still six weeks. When I meekly asked about paying out of pocket to expedite, they quoted me $500. Just for the appointment.Dr. Klaufman told the assistant to hang up and, with a straight face, instructed me to "call my primary care doctor and ask for a referral to the new place."## where this leaves meThe entire appointment lasted fifteen minutes, from the moment I walked in to the moment I walked out. I paid $149 for fifteen minutes of my life I will never get back, a useless prescription, and a referral to a place I will never go because of the same constraints that brought me there. At a rate of nearly $600 an hour she is paid handsomely for providing absolutely no value to me or apparently anyone if you believe the other Yelp reviews.I walked out stunned. For a fleeting moment I imagined paying someone to stand outside her office and bounce a ball on repeat, a darkly comic tribute to the absurdity that opened my visit. My actual revenge was less theatrical: I went home and wrote a [Yelp review](https://www.yelp.com/biz/valencia-optometry-san-francisco-4?hrid=pX2clDNn_MFMxyMsWQz0mA&utm_campaign=www_review_share_popup&utm_medium=copy_link&utm_source=(direct)) since I am, admittedly, too broke to do the former. It was then that I saw what I should have checked beforehand. The page was a graveyard of one star experiences, a chorus of voices echoing my own frustration; a useless doctor.The lesson, of course, is brutally simple: never go to a place, no matter how in need you are, without checking the reviews first. My desperation cost me $149 or really my mom since she gave me the money.The healthcare system I tried to bypass by paying cash is just as broken on the other side. The transaction was more theatre than medicine and I paid for the ticket. The only good thing that emerged was that once I got home I began using the antibiotic my urgent‑care clinic prescribed more aggressively. The swelling and redness have begun to recede, and the chalazion is now less noticeable.We need healthcare that cannot waste your time by design and cannot rug you by design.This wasn’t it.

---

---layout: posttitle: "privacy and safety in AI tooling are still missing."date: 2025-10-11 18:53:00 PTcategories: [startups, ai, op-ed, privacy, zk]attribution: ai various---## the ai safety problemWe spend endless hours marveling at what generative AI can do, painting pictures, writing code, composing music. Rarely do we pause to ask what it should do, and what could be prevented. The AI stacks of 2025 deliver massive computational power with almost no friction, but they have almost no built-in mechanisms for verification, enforcement, or audit. Without these, outputs circulate unchecked, exposing users to harm and developers to liability, and the speed at which models generate content already outpaces both regulatory oversight and human moderation.## why verification matters right nowMinors can already interact with models capable of producing sexually explicit or otherwise unsafe content. At the same time, privacy-focused tools often forward every prompt to third-party APIs, creating single points of failure that can leak sensitive conversations or creative work. Generative AI workflows frequently expose proprietary ideas, personal queries, or private data to models without users’ knowledge, and these prompts can be incorporated into model training without consent. Regulators are responding. The EU AI Act and GDPR’s privacy-by-design framework require auditable safeguards and verifiable provenance. Several U.S. states are passing disclosure laws that push platforms toward demonstrable safety guarantees. Platforms without technical means to prove compliance risk regulatory penalties, exclusion from sensitive markets such as healthcare and education, and erosion of user trust. Cryptographically verifiable controls offer a pathway to safely scale AI deployment while maintaining user privacy.## three proposed pillars of a verifiable control planeThese are ideas for what a verifiable control plane could look like. The first pillar is verifiable model integrity. Zero-knowledge proofs (zk-proof) could cryptographically show that an output originates from an approved, safety-aligned model without revealing internal parameters or training data. Implementations might use zk-SNARK or zk-STARK constructions optimized for neural network inference, producing proofs that verify correctness without exposing model weights or training sets. Using zero-knowledge machine learning (zkML), models can confirm the correctness of a response and produce verifiable proofs without accessing raw prompts, preserving user privacy and reducing the risk of data leakage. The second pillar is private identity attestation. Decentralized identity wallets could issue cryptographic credentials asserting that a user meets policy requirements such as age or jurisdiction, which can be verified without revealing any personal identifiers. The third pillar is on-device pre-screening. Lightweight classifiers running locally could evaluate prompts or outputs against policy before they leave the device, detecting unsafe content while remaining efficient on consumer hardware. These layers could be combined in various ways depending on platform risk profiles, creating composable, auditable controls that may also reduce operational costs.## what a safe-by-design workflow could look likeIn a practical scenario, a generative AI app might require an age-attestation credential in order to access mature filters from a decentralized identity wallet, receiving a zero-knowledge proof confirming the user is over eighteen without exposing their birthdate. When the user enters a prompt, an on-device classifier could evaluate it for unsafe language, sexual content, violence, or extremist content. Only policy-compliant prompts would be sent to a cloud endpoint, where a model running in a zero-knowledge-compatible environment generates the output along with a cryptographic proof confirming it came from the approved model. The client verifies the proof before displaying the output. Immutable logs of proof hashes and anonymized attestation status could be written to a tamper-evident ledger or secure storage, allowing auditors to verify compliance without accessing raw user data. This approach could scale across text, images, video, and multi-modal AI, creating a verifiable trail of safe operations while preserving privacy. zkML also opens the door to fair value for user contributions. Instead of models passively consuming prompts as free training data, users could retain control over their inputs, receive proofs of how their prompts are used, and even be compensated for contributing to model improvement, closing the loop and encouraging engagement.## scalable alternatives and trade-offsFully decentralized approaches reduce trust assumptions but introduce latency, higher computational cost, and deployment complexity. Centralized systems are faster and simpler but concentrate liability and reduce privacy guarantees. Hybrid architectures could combine on-device screening with centralized verification, balancing speed, privacy, and auditability. Standardized protocols for zk-proof verification, decentralized identity, and on-device classifiers would allow interoperability and reduce friction for adoption, enabling developers to incrementally integrate layers according to operational constraints and risk tolerance. Incorporating zkML for privacy and verifiable model outputs also reduces the risk that sensitive prompts are exposed or exploited by model providers while maintaining verifiable assurances of correctness.## enforcement mechanisms and real-world precedentsApple’s on-device NSFW detection and Google’s SafetyNet attestations demonstrate that local enforcement and cryptographic verification are feasible at scale. GDPR and the EU AI Act already mandate traceability and logging for high-risk AI systems, requirements that zk-proofs and auditable logs could satisfy. Implementing these mechanisms shifts liability away from downstream integrators while providing regulators with verifiable evidence of safe operations without exposing sensitive user data. Integrating privacy-preserving proofs and verifiable ML could establish a precedent for compensating users for their contributions to model training, aligning safety, privacy, and economic incentives.## who builds it and who benefitsOpen-source communities could provide reusable primitives for zkML, identity protocols, and on-device classifiers. Platform providers could operate verifiable compute endpoints and publish model integrity hashes. Identity-wallet projects could issue reusable attestation credentials to expand adoption. Developers would integrate these components to accelerate time-to-market, reduce compliance burdens, and enforce privacy-by-design. Regulators could verify compliance without accessing personal data, and users would benefit from safer, privacy-preserving services while retaining agency over their prompts and receiving compensation when appropriate. This ecosystem aligns incentives across builders, regulators, and users, creating infrastructure for responsible AI at scale.## how this improves the landscapeFiltering unsafe content before it leaves the device reduces the propagation of illegal or harmful material. Privacy-preserving attestations enable enforcement of policies at scale without storing personal identifiers. Cryptographic proofs make audits measurable and reduce operational overhead. Verified outputs could serve as market signals for responsible practices, nudging the broader AI ecosystem toward safety and privacy standards. Users gain control over their data and the potential to be compensated for contributing to model improvements, shifting the economics of AI toward fairness. Over time, these mechanisms could establish a verifiable baseline for AI services where safety, privacy, and fair contribution are inherent, measurable properties.## closing thoughtsThe AI ecosystem in 2025 lacks a structured control plane capable of verifying model outputs, enforcing policy, and preserving privacy at scale. The ideas outlined here, namely, verifiable model integrity, private identity attestation, on-device pre-screening, and privacy-preserving zkML, represent one possible path forward. The building blocks exist, but widespread adoption requires coordination between open-source communities, platform providers, identity projects, and regulatory stakeholders. Implementing such a control plane could allow AI services to operate at scale with measurable safety, privacy, compliance, and fair treatment of user contributions, providing a foundation for trust in a rapidly evolving landscape.We already know how to make AI faster; the question now is how to make it accountable.## sidebar: limitations and possible remediationsImplementing a verifiable control plane has clear challenges. zk-proofs for large neural networks are still computationally expensive, which could bottleneck high-throughput applications. On-device classifiers may misclassify content, generating false positives or negatives, particularly with ambiguous or culturally specific inputs. Decentralized identity systems depend on robust adoption, credential issuance security, and resistance to cloning or replay attacks. Hybrid architectures introduce orchestration complexity between local and centralized systems. Potential remediations include optimizing zk-proof circuits for inference, continual retraining of classifiers with active learning, leveraging hardware-backed secure enclaves for credential storage, and using modular protocols that allow incremental integration and testing. Feedback loops from auditors and regulators can refine thresholds and policy enforcement over time.## sidebar: quantum computing, cryptographic resistance, and risksThe cryptographic primitives that underpin zk-proofs and private attestation rely on assumptions about computational hardness. Advances in quantum computing pose a risk to these assumptions, particularly for proof systems based on discrete logarithms or factoring. Post-quantum-resistant constructions such as lattice-based SNARKs and hash-based signature schemes provide a potential path to future-proof proofs and attestations, but they come with larger proof sizes and higher computational cost. In a hybrid system, quantum-resistant algorithms should be considered for both model integrity proofs and identity credentials. Quantum threats also introduce operational risk: if a sufficiently powerful quantum computer becomes available, previously recorded proofs could be retrospectively compromised, highlighting the need for continuous key rotation, periodic algorithm upgrades, and monitoring for emerging quantum capabilities. Building a verifiable control plane today requires factoring in quantum resilience as part of the design, even if practical quantum attacks remain speculative, because AI outputs can remain relevant for years and need long-term trust guarantees.

---

---layout: posttitle: "where does intelligence come from?"date: 2025-12-18 20:02:00 PTcategories: [general, personal, op-ed, rant, ai]attribution: human---## books for christmasEarlier today I was in Barnes & Noble, my favorite place to be now that I’ve moved back home. There isn’t much else competing for my attention as I try to avoid social media, mostly through iOS Screen Time settings. I was there to look for Christmas presents as I’ve decided unilaterally, and somewhat tyrannically, that I only buy books for people. And only for family.A few days ago I had spotted [*I Am a Cat* by Natsume Sōseki](https://en.wikipedia.org/wiki/I_Am_a_Cat) or Sōseki Natsume, depending on whether you trust the book cover or Wikipedia. Author names are important as so many with a nom de plume can attest, so this strikes me as horrible marketing. It is also very confusing for those looking to share his work with other people, something I strangely have already tried to do.As is my ritual when considering a purchase at B&N, I read about 5% of the book while standing in the aisle, clutching a mostly watery latte from the accompanying Starbucks? It's a question because they don't accept Starbucks gift cards, staunchly via signage, and often lack specialized drinks you'd get at a "normal" Starbucks. You also can't collect those points people always zealot about. The B&N "Starbucks" pastries are always better though, so that's a plus, and they accept the B&N membership for discounts, if you're one of those people, like me. Standing with a latte reading for hours in a bookstore counts as exercise, in my opinion, because the weather in Southern California is too hot to go outside despite being December. Due to the heat and general malaise, I can’t be bothered to work out in any structured way beyond the occasional push-up to reassure myself I still exist. Something commonplace amongst most millennials. Just ask.*I Am a Cat* reminds me of my dad because of Charlie and Moris. Cats I now live with. Cats that annoy me constantly, Moris more than Charlie. Cats he nevertheless cherishes roughly on par with his human children. Cats he shares in common with his new girlfriend who I call "Snuff," and who has two of her own, Cleo and Oliver. Snuff sometimes also fosters kittens for the fun of it and has a room dedicated to the task. What I loved about the book was its skepticism toward humans, delivered from a cat’s perspective, about how humans are mostly trash until they aren’t, a sentiment I wholeheartedly agree with. I grabbed the first copy I saw, trying not to drop my latte, and moved on.I headed to my favorite section: science. Not science fiction. Actual science. The kind with graphs and math and authors with the Dr. prefix. Most of the titles made my eyes roll because they were about AI or some permutation of AI, most of which I'd already ruled out or read, and I'm sorry, but I don’t care about Sam Altman, Elon, or any of the other shitheads monetizing what should be a common-good tool, like the internet, rather than monopolizing, rent-seeking platforms designed to vacuum up other people’s thoughts, ideas, and creativity. Most of the books in science under "new technology" are basically biographies of the current top AI players, rather than subject matter elucidating model peculiarities, infrastructure strategies, or a meaningful change in emergent capabilities.Anyways, there wasn’t anything I could find in science that my dad would like except maybe a book called [*How to Invent Everything* by Ryan North](https://www.goodreads.com/book/show/39026990-how-to-invent-everything?ref=nav_sb_ss_1_24). If you haven’t heard of it, it has a strange, comic-book-style cover, which tracks given the author’s roots in cartooning for Marvel. But why it was shelved under science and not science fiction was unclear. More confusingly, the book presents itself as a literal manual for time travel and uses that framing to walk through the history of civilization. No way it can compete with [*Sapiens*](https://en.wikipedia.org/wiki/Sapiens:_A_Brief_History_of_Humankind), I thought, but I kept reading all the same.The problem with *How to Invent Everything*, at least from what I read and later confirmed on Goodreads, is that it’s confidently wrong about some basic things, rather like the hallucinations in GPT tools. It claims, for instance, that beer and bread were created by animals rather than yeast (they were not), and it does not address Arthur C. Clarke's quote about the lack of time travelers as evidence against time travel. Reviews echoed the same criticism: clever, funny, but not nearly as accurate as it wants to be. I put the book back assuming my dad would be annoyed like me and went to another section.Sci-fi is usually safe with my dad since he still references h2g2, [*Hitchhiker’s Guide*](https://en.wikipedia.org/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy), almost every time sci-fi comes up. I checked out [*The Cabinet* by Un-Su Kim](https://www.goodreads.com/book/show/56353777-the-cabinet) (interesting premise, odd translated cadence) and [*Three Californias* by Kim Stanley Robinson](https://www.goodreads.com/book/show/45046588-three-californias?ref=nav_sb_ss_4_24). The latter intrigued me because my dad had recently been looking for a nonfiction book about California that apparently existed everywhere except the actual shelves. Robinson’s book offered three fictional Californias instead. I read a few pages. Good. Just not *him*.I circled back to *I Am a Cat* and read a Goodreads review that said:“I recommend reading a bit each night, before sleep, for hilarious dreams.”My dad likes hilarious dreams. He reads before bed. Decision made.## curiosity and skepticismFinding a book for my mom is nearly impossible. I’ve seen her buy exactly two books in my three decades of existence, both for me. She admits to reading only one: the Bible, and listening to a few others.I walked over to the religious section, which I have entered several times before, all accidentally. It’s disorienting. So many Bibles, motivational diaries, near-death experiences, daily Bible studies, and other religious non-artifacts. However, buried within the shelves was a comparative religion subsection that was more comprehensive than you'd expect. It had various books about religion and comparing different religions, and popes, and other things. Notably, a translated Qur’an, which I learned isn’t meant to be read so much as sung, and then a book titled [*God, The Most Unpleasant Character in All Fiction* by Dan Barker](https://www.goodreads.com/book/show/25986260-god?ref=nav_sb_ss_1_49), with a foreword by Richard Dawkins, one of my favorite authors.I read nearly 30% of it while standing awkwardly in the aisle, receiving several uninterested but curious looks. The book builds its case entirely from biblical quotations, God as villain, by His own words. It exists, I learned from the foreword, because Dawkins once wrote, in his book [*The God Delusion*](https://en.wikipedia.org/wiki/The_God_Delusion), a series of now-famous adjectives:“The God of the Old Testament is arguably the most unpleasant character in all fiction - jealous and proud of it, a petty, unjust, unforgiving control-freak; a vindictive, bloodthirsty ethnic cleanser, a misogynistic, homophobic, racist, infanticidal, genocidal, filicidal, pestilential, megalomaniacal, sadomasochistic, capriciously malevolent bully.”Dawkins posits an entire book would be required to accurately reference each adjective from the excerpt above, and that he, not being a biblical scholar, would take longer than he had time for to write it. He thought of Barker, a reformed pastor, and set him to the task. Barker obviously obliged.Realizing I hadn’t actually read *The God Delusion*, I wandered back to the science section to find it, but alas, it was not there. Instead, I found something else. [*Crypto-Infection* by Dr. Christian Perronne](https://www.goodreads.com/book/show/55651112-crypto-infections). I was originally drawn to it because of the prefix "Crypto-" being someone who spent the last several years working in the blockchain industry, but then realized it was a neologism by the author used to describe a new or, in his words, specific class of infectious disease caused by the same bacteria that causes Lyme disease, *Borrelia burgdorferi*. The premise is that this bacteria is causing widespread, underdiagnosed chronic infections all across Europe, but specifically France, the UK, and Germany. I have a particular aversion to ticks, having once had an experience with one on two occasions, spread apart by two hours, in New Hampshire, on the same day. I was convinced the tick was stalking me to find the right place to burrow its head somewhere on my body, akin to a terrible alien parasite incarnated in a sci-fi novel. Eventually, my grandfather plucked it off my arm and smashed it between his fingers. I hate anything that burrows into a person and requires fire to remove. Obviously.I read several pages of *Crypto-Infection*, but the most interesting part of what I read had nothing to do with infections or ticks, incidentally. It was a translator’s note. The book was originally written in Dr. Perronne's native French:“The embrace of intelligence is proffered with two arms: that of curiosity and that of skepticism.”I stopped to consider. Intelligence is broad, and two arms seems limiting. Sure, curiosity and skepticism, but surely there's more to it. Not everyone is curious, and definitely not everyone is a skeptic, as we know merely from the presence of the religion section, just as we can be sure of curiosity from the science section and skepticism, famously, from philosophy.I thought about the intelligent people I know. Which is… everyone. Every human is intelligent.Are we all curious? No.Are we all skeptical? Also no.There's a huge variety of intelligent people all across the globe with varying skills, and all feel the proffered embrace of intelligence.My mom, in particular, is neither curious nor skeptical, and she’s one of the most intelligent people I know. She’s said as much my entire life and usually gets annoyed with my questions. But so does everyone. She’s also devoutly religious. So either the translator was wrong, or intelligence isn’t so exclusionary.After four hours in B&N, I checked out. In the car, with *I Am a Cat* on the passenger seat, still considering what begets intelligence. After sitting in the car for several minutes contemplating, I decided I'd call my mom to hear her thoughts, given she seemed the perfect counterargument. I called.After some obligatory discussion about an eye appointment I didn’t want to talk about (see [this post](https://97115104.com/2025/09/30/chalazion/)), I asked Mom directly about curiosity and skepticism, reading the quote from the translator of *Crypto-Infection* directly. After being sure she'd heard, as you can never be sure with the spotty reception from Mint Mobile, I waited for her response.She paused for several seconds. Then said,“Well… I’m not curious or skeptical.”Then, after a moment longer,“Maybe a little curious.”“But mostly I learn from observing. Watching. Listening.”I asked a few follow-up questions about what that meant for her, and how she would compare observation to things like curiosity. At first, she agreed with me about why curiosity was obvious and the same with skepticism, and then mentioned that sometimes she just trusts, which comes from intuition from observation. Observation does proffer information. And what is intelligence, if not the ability to gather, organize, and apply information?We briefly touched on faith as a conduit, too, which I explained away by simply referring to the many people who have walked off cliffs believing they could fly, the old tale grown-ups use to explain why doing drugs is bad. We settled on three super-categories: curiosity, skepticism, and attention.Attention and not observation since attention is mostly applied observation. We ended our call agreeing it was an interesting subject to think about, but mostly a thought experiment, and were both tired from the mental gymnastics. I couldn't get it out of my mind, though.Are there other categories? How does intelligence work? Or am I just being curious. Or skeptical. Or attending to myself thinking. I thought, to be continued, and drove the rest of the way home.## food for thoughtAs I usually do after speaking with my mom or dad or other friends about some lofty thought experiment, I broached the topic with my best friend, to hear his thoughts, but mostly to solidify my thinking on the subject, closing a mental loop.I read the same excerpt from the translator of *Crypto-Infection* to him, and asked for his thoughts. He is a highly skeptical person, and equally curious, so I suspected he would have a similar perspective to me, though he might approach the problem of where intelligence is ultimately proffered differently.I was correct in that he agreed curiosity and skepticism make sense, but he also helped me come up with a few concepts that clarified attention, and how each, when combined, build a strong case to exclude things that do not proffer intelligence. For example, one can apply a simple concept of multipliers where a super-category, take curiosity, can be multiplied by a person's motivation for a given problem. If a person is very curious about why the sky is blue, thereby sufficiently motivated, then they are likely to build a good amount of intelligence about why the sky is blue through research and determination. On the other hand, if a person is curious about why the sky is blue, but is not motivated to find the answer, then they are unlikely to obtain the intelligence explaining the reason for the color. Put more simply, we can represent this as a math equation where S = the super-category, M = multiplier, and I = the amount of intelligence gained.I = S * MFor example:* if I = Curiosity (1) * Motivation (3), I = 3* if I = Curiosity (1) * Motivation (0), I = 0The same concept can be applied for all super-categories, like skepticism and attention. If we can quantify the level of intelligence gained by multiplying a super-category, then how can we validate a super-category? Well, if a category cannot be multiplied ad infinitum, then it cannot possibly be a super-category. For example, if we tried organization as a super-category, it becomes obvious that a person cannot organize to infinity, but they can be curious to infinity, and they can be skeptical to infinity. In the case of the sky being blue, you could learn about light, and particles, and the ocean, and any number of unlimited other subjects multiplied by how motivated you are to solve the problem. Similarly, you could be skeptical about God, like who created God, and would likely give yourself a headache, but you could, with sufficient motivation, go at the problem forever, reading innumerable texts. And as mentioned earlier, if you have faith to infinity, you'll likely end up at the bottom of a cliff, in a different kind of infinity.What then is the progenitor of a super-category? Is that even a thing? Does motivation or curiosity come first? Well, if you think about it neurologically, a person cannot be motivated without energy, that is from food, exercise, or some other source. Figuratively speaking, you get out what you put in when it comes to the body. So energy must then be what feeds super-categories of intelligence. So with sufficient energy, or calories from food, etc., you could then be infinitely intelligent? It appears possible. Of course, apart from notable exceptions of the human condition being sleep, safety, and age.To better understand this idea, the "scaling hypothesis," currently being discussed in the AI infrastructure space, might be helpful. The hypothesis states, to put it simply, that LLMs are as intelligent as the resources they have access to. There are several types of resources the hypothesis considers important such as pre-training data, post-training feedback, and the compute available via raw processing power (currently powerful GPUs). All are analogous to human traits and abilities. For a primer on the topic, I highly recommend [*The Scaling Era: An Oral History of AI* by Dwarkesh Patel](https://www.goodreads.com/book/show/239472387-the-scaling-era?from_search=true&from_srp=true&qid=ISKSBYzaJU&rank=1).If AI has been proven to outperform, deterministically, based on the scaling hypothesis by reducing loss thereby increasing accuracy with greater access to resources, then maybe the same concept holds true for individual human intelligence? A so-called "human scaling hypothesis." Patel talks about this briefly in his book, when contrasting brain size and circuitry with other mammals and non-mammals throughout history, and it makes sense.Could it be that one can deterministically calculate intelligence by simply generating a coefficient of superset categories and multipliers? Can we, as individuals, make ourselves more intelligent simply by finding our preferred super-category, and then fueling our multipliers with the right kind of energy?Lofty questions, yes. Mental gymnasium activated, yes. All just food for thought, also yes...pun intended ## secondary thoughts “on entertainment”My day started before Barnes & Noble.It started with a recruiter message about a founding engineer role at a startup called [camfer](https://camfer.dev/). This is how professional curiosity manifests for me now: a compelling title, an ambiguous mandate, and just enough information to force unpaid diligence before even deciding whether interest is warranted. Applying cold doesn't work anymore, and referrals don't seem to pan out like they used to, so for better or worse, the best strategy seems to be to create a fishing pole profile, anglerfish the best possible opportunities by creating content, engaging with platforms, and sharing your life's story. Hiring and recruiting, on both sides, has become strangely asymmetrical. Recruiters initiate, candidates investigate. Recruiters narrate; candidates verify. That or you get screwed. Don't verify and you might be in a nightmare job, get laid off, or worse, and have to move back in with your parents. Try cold applying, and you'll end up having to research a different kind of black hole...I hadn't heard of camfer, though the recruiter did mention a recent Y Combinator raise and CAD plus AI, which was intriguing. I needed more information to decide if I should spend valuable time and attention learning what I needed to in order to have a decent first interview. In my research, I came across a blog post by one of the founders titled [*“on entertainment.”*](https://www.keat.one/2024/11/28/on-entertainment.html) It was the very first post from the founder. A common thing I do now is to look for the first post on a blog, as I find it is an indicator of what to expect from the author(s) and their content. I read it quickly at first, then again more slowly. It wasn’t really about entertainment in the conventional sense. It was about what entertainment has become optimized for, and more importantly, how the decisions made in the name of optimization are having real consequences, especially for younger generations.*"on entertainment"* articulated something I’ve felt but hadn’t fully named: modern entertainment is engineered to monopolize attention so that it can sell you things you probably want but also, probably, don't need. All based on you, as a person, which is obtained through an aggregate of your life via browsing history and social media, in the form of an advertisement ID. Scary.The concept of the modern entertainment economy followed me into B&N, and it framed everything that came after, especially my thoughts on intelligence, reading, and the convergence of AI tools with targeted ads.I’ve been thinking about and trying to design more incentive-aligned systems for a while now, including the [Autophage Protocol](https://autophage.xyz/paper/litepaper.pdf) and my more recent writing on [AI privacy and safety](https://97115104.com/2025/10/11/ai-privacy-safety/). In Autophage, I proposed a system where *activity measurably benefits human health* by incentivizing movement, adherence, and participation, and in AI privacy and safety, I propose a system that rewards users for their prompts and engagement.That same logic could apply to attention, more broadly.Right now, attention is extracted, refined, and sold. Platforms profit from it, effectively stealing energy through attention. Models improve because of it. Yet the human providing it remains uncompensated, unacknowledged, and increasingly shaped by the very systems monetizing their cognition.This imbalance becomes more acute with AI.Large models are trained, in part, through [Reinforcement Learning from Human Feedback](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback), including passive behavioral signals like what users accept, reject, refine, or abandon. This is labor, even if it is unrecognized as such. It improves systems. It increases their value. And yet users are neither paid nor meaningfully protected. Their prompts may be logged, their preferences inferred, their cognition quietly folded into optimization loops they do not control, and they pay for the pleasure.In my writing on AI privacy and safety, I argued that this is both an ethical issue and, in some cases, a technical oversight. If users are participating, actively or passively, in the improvement of these systems, they should be compensated, as compensation is monetarily valuable to the individual, especially projecting into a future where AI tools are ubiquitous and attention as a cognitive resource more commonplace, and an acknowledgment of agency, restoring some semblance of symmetry. In the absence of that, we'd be in a world of hurt as a group of people, "humans," given the moderation of these tools and their cognitive pull on our other capabilities like curiosity and skepticism. Deepfakes, fallacious news, personalized worlds, and agent-to-agent protocols already exist. How can the next generation hope to compete when they are already primed to feed an intelligence that continues to grow in performance and capability before they've even been able to get a job?Which brings me back to intelligence.In wandering the bookstore and talking with people I trust, I kept circling three candidates for what actually proffers intelligence: curiosity, skepticism, and attention.Attention is the easiest to measure, which is why institutions fixate on it. Standardized testing, credentialing, performance metrics, blah, blah, blah. Attention alone is inert.When most discretionary attention is consumed by infinite feeds, attention ceases to be a precursor to understanding and becomes a terminal state. It anesthetizes curiosity and renders skepticism socially inefficient. Why doubt when the next stimulus is already loading?From that perspective, doom scrolling is a cultural habit that begets an epistemic failure mode.AI compounds this risk. When answers are instantaneous, fluent, personalized, and confident, the struggle that once produced understanding is bypassed because things like curiosity become short-circuited or dangerously reinforced, while skepticism becomes optionalized and abstracted.If attention is the currency of the digital age, then the economy we’ve built values attention above other kinds of intelligence, and it is not us who benefit from it.## what's next?Is there a future where attention is reciprocally incentivized, like in the dystopian Black Mirror-esque episodes where people’s only job is running on a treadmill they watch ads on, or perhaps where self-driving cars are free because the entire cabin is a glittering, excruciating, blue-light anathema?What happens to the distribution of wealth or intelligence? Is a benevolent superintelligence powered by an infinite attention economy at the cost of other forms of intelligence more beneficial to the greater good, or is "every person for themself" still the better option if a single entity or entities control that superintelligence?Are we creating a new kind of oligarchy posing as the companies and products we currently trust enough to use?Only time will answer these questions. The more interesting question may not be what companies are building, but what kinds of intelligence we are still able to cultivate.If attention is finite, then choosing where to spend it becomes a deeply personal act, and stealing it possible.A future that respects intelligence does not require perfect systems or benevolent machines. It looks smaller. Fewer feeds. Slower tools. Interfaces that reward curiosity, skepticism, attention, and other things like creativity, openness, and collaboration. Intelligence should grow because it is abstract, exercised, and shared.Maybe intelligence has always come from the same place. From time set aside. From attention that is chosen. From the quiet decision to stand in a bookstore aisle and read five percent of a book with no system watching, in an effort to buy a personal Christmas gift that enriches, instead of a bauble served up via algorithmic slop chosen by a machine that has no relationship to the giver or receiver, other than the profit margin it obtained.